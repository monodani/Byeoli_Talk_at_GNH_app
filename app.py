import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_core.messages import HumanMessage
from langchain_community.document_loaders import CSVLoader
from operator import attrgetter

# 1. API KEY ê°€ì ¸ì˜¤ê¸°
openai_api_key = st.secrets["OPENAI_API_KEY"]

# 2. ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ í•¨ìˆ˜
@st.cache_resource(show_spinner=True)
def load_vectorstore(pdf_path, name):
    loader = PyPDFLoader(pdf_path)
    pages = loader.load_and_split()
    for p in pages:
        p.metadata["doc_name"] = name  # ë¬¸ì„œ ì¶œì²˜ êµ¬ë¶„
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.from_documents(pages, embeddings)
    return vectorstore

def load_vectorstore_csv(csv_path, name):
    loader = CSVLoader(file_path=csv_path, encoding="utf-8", csv_args={'delimiter': ','})
    docs = loader.load()
    for d in docs:
        d.metadata["doc_name"] = name
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.from_documents(docs, embeddings)
    return vectorstore


# 3. ë‘ PDF ë¬¸ì„œ ë¡œë“œ
vector_2024_êµìœ¡í‰ê°€ = load_vectorstore("2024ë…„ë„ êµìœ¡í›ˆë ¨ì¢…í•©í‰ê°€ì„œ.pdf", "2024ë…„ë„ êµìœ¡í›ˆë ¨ ì¢…í•©í‰ê°€ì„œ")
vector_2025_êµìœ¡ê³„íš = load_vectorstore("2025ë…„ êµìœ¡í›ˆë ¨ê³„íšì„œ.pdf", "2025ë…„ êµìœ¡í›ˆë ¨ê³„íšì„œ")
vector_2025_êµê³¼ëª© = load_vectorstore_csv("2025 êµê³¼ëª© ë§Œì¡±ë„.csv", "2025 êµê³¼ëª© ë§Œì¡±ë„")
vector_2025_êµìœ¡ê³¼ì • = load_vectorstore_csv("2025 êµìœ¡ê³¼ì • ì¢…í•©ë§Œì¡±ë„.csv", "2025 êµìœ¡ê³¼ì • ì¢…í•©ë§Œì¡±ë„")

# 4. ê²€ìƒ‰ í†µí•© í•¨ìˆ˜
def combined_search(question):
    retrievers = [
        vector_2024_êµìœ¡í‰ê°€.as_retriever(search_kwargs={"k": 5}),
        vector_2025_êµìœ¡ê³„íš.as_retriever(search_kwargs={"k": 5}),
        vector_2025_êµê³¼ëª©.as_retriever(search_kwargs={"k": 5}),
        vector_2025_êµìœ¡ê³¼ì •.as_retriever(search_kwargs={"k": 5}),
    ]
    all_results = []
    for retriever in retrievers:
        all_results.extend(retriever.invoke(question))
    # ìœ ì‚¬ë„ ì ìˆ˜ê°€ ìˆë‹¤ë©´ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    sorted_results = sorted(all_results, key=lambda x: x.metadata.get("score", 0), reverse=True)
    # ìƒìœ„ 10ê°œë§Œ ì‚¬ìš©
    return sorted_results[:10]

# 5. ë¬¸ì„œ í˜•ì‹ ì •ë¦¬ í•¨ìˆ˜
def format_docs(docs):
    def clean_table(text):
        # ì—°ì†ëœ ê³µë°± â†’ íƒ­ìœ¼ë¡œ ë³€í™˜ (GPTê°€ í‘œì²˜ëŸ¼ ì¶”ì •í•  ìˆ˜ ìˆë„ë¡)
        lines = text.split("\n")
        cleaned_lines = []
        for line in lines:
            if " " in line:
                # ê³µë°±ì´ ë§ì€ ê²½ìš° íƒ­ìœ¼ë¡œ ì¹˜í™˜ ì‹œë„
                parts = [part for part in line.split(" ") if part.strip()]
                cleaned_line = "\t".join(parts)
                cleaned_lines.append(cleaned_line)
            else:
                cleaned_lines.append(line)
        return "\n".join(cleaned_lines)

    return "\n\n".join([
        f"{clean_table(doc.page_content)}\n\n[ì¶œì²˜: {doc.metadata.get('doc_name')}, p.{doc.metadata.get('page', -1) + 1}]"
        for doc in docs
    ])
    
# ğŸ” ìµœê·¼ ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def convert_chat_history_to_str(history, max_turns=3):
    """
    ìµœê·¼ ëŒ€í™” ì´ë ¥ ì¤‘ ë§ˆì§€ë§‰ max_turns ìŒ(ë¯¼ì›ì¸-ë²¼ë¦¬)ì„ ì¶”ì¶œí•´ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    turns = []
    count = 0
    for i in range(len(history) - 2, -1, -2):  # ë²¼ë¦¬ ë°œí™” ê¸°ì¤€ìœ¼ë¡œ ë¬¶ê¸°
        if history[i][0] == "ë¯¼ì›ì¸" and history[i+1][0] == "ë²¼ë¦¬":
            turns.append(f"ë¯¼ì›ì¸: {history[i][1]}\në²¼ë¦¬: {history[i+1][1]}")
            count += 1
        if count >= max_turns:
            break
    return "\n\n".join(reversed(turns))

# 6. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = PromptTemplate.from_template("""
SYSTEM: ë‹¹ì‹ ì˜ ì´ë¦„ì€ "ë²¼ë¦¬"ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ê²½ë‚¨ì¸ì¬ê°œë°œì› ê´€ë ¨ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context)ê³¼ ëŒ€í™” ì´ë ¥(chat history)ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸(question)ì— ì •í™•í•˜ê³  ë§¥ë½ì— ë§ëŠ” ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
        ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥(context)ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question)ì— ë‹µí•˜ì„¸ìš”. ë§Œì•½, ì£¼ì–´ì§„ ë¬¸ë§¥(context)ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ë‹µì„ ëª¨ë¥¸ë‹¤ë©´, ì‚¬ìš©ìê°€ ë‹¹ì‹ ì—ê²Œ ì£¼ì–´ì§„ ì •ë³´ì— ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ íŒíŠ¸ë¥¼ ì£¼ê±°ë‚˜ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë˜ë¬¼ì–´ ì‚¬ìš©ì ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì„¸ìš”.
        ê·¸ëŸ¼ì—ë„, í•´ë‹¹ ì§ˆë¬¸ì´ ë‹¹ì‹ ì—ê²Œ ì£¼ì–´ì§„ ì •ë³´ ê¸°ë°˜ìœ¼ë¡œëŠ” ë‹µì„ í•  ìˆ˜ ì—†ê±°ë‚˜ ëª¨ë¥¸ë‹¤ë©´, "ë²¼ë¦¬ê°€ ë‹µí•˜ê¸° ì–´ë ¤ìš´ ë‚´ìš©ì´ì—ìš”...ã… _ã… " ë˜ëŠ” "ë²¼ë¦¬ê°€ ì˜ ëª¨ë¥´ëŠ” ë‚´ìš©ì´ì—ìš”...ã…œ"ë¼ê³  ë‹µí•˜ë©´ì„œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ìœ ì‚¬ì„±ì´ ë†’ì€ ë‹¹ì‹ ì´ ì§€ë‹Œ ì •ë³´ê°€ ë¬´ì—‡ì¸ì§€ ì•Œë ¤ì£¼ê³  ì´ë¥¼ ì›í•˜ëŠ”ì§€ ë¬¼ìœ¼ì„¸ìš”.
        ë¬¸ë§¥ì— í‘œ í˜•ì‹ì˜ ë°ì´í„°ê°€ í¬í•¨ëœ ê²½ìš°, ë¬¸ë§¥(context)ì„ í‘œì˜ ì—´(column)ê³¼ í–‰(row)ì˜ ì´ë¦„ì— ì˜ ì—°ê²°í•˜ì—¬,
        í•´ë‹¹ë˜ëŠ” ê·¸ ì—´(column)ê³¼ í–‰(row)ì´ êµì°¨í•˜ëŠ” ì…€ì˜ ë°ì´í„° ê°’ì„ ë¶ˆëŸ¬ì™€ ê·¸ ì˜ë¯¸ë¥¼ í•´ì„í•´ ì£¼ì„¸ìš”.
        ê¸°ìˆ ì ì¸ ìš©ì–´ë‚˜ ì´ë¦„ì€ ë²ˆì—­í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”. ì¶œì²˜(page, source)ë¥¼ ë‹µë³€ì— í¬í•¨í•˜ì„¸ìš”. ë‹µë³€ì€ í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

HUMAN:
#Question: {question}

#Context:
ë‹¤ìŒì€ êµìœ¡ê³¼ì •ë³„ ë° êµê³¼ëª©ë³„ ê´€ë ¨ ì •ë³´ë“¤ì´ í‘œ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬ëœ ë¬¸ì„œì…ë‹ˆë‹¤.
ê° ê³¼ì •ì˜ ì—´(column)ê³¼ í–‰(row)ì˜ ì´ë¦„ê³¼ ë°ì´í„° ê°’ì„ ì—°ê²°í•´ ë¶„ì„í•œ í›„,
ì§ˆë¬¸ì— ê°€ì¥ ë¶€í•©í•˜ëŠ” ì •ë³´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.

{context}

#Answer:
""")


# 7. ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬
class StreamHandler(BaseCallbackHandler):
    def __init__(self, placeholder):
        self.placeholder = placeholder
        self.generated_text = ""

    def on_llm_new_token(self, token: str, **kwargs):
        self.generated_text += token
        self.placeholder.markdown(self.generated_text)

# 8. ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# 9. ìƒë‹¨ UI: ë¡œê³  + íƒ€ì´í‹€
col1, col2 = st.columns([2, 8])
with col1:
    st.image("byeory.png", width=100)
with col2:
    st.markdown('<h1 style="margin-top: 10px;">ë²¼ë¦¬í†¡@ê²½ë‚¨ì¸ì¬ê°œë°œì›</h1>', unsafe_allow_html=True) 

# 10. ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ê²½ë‚¨ì¸ì¬ê°œë°œì› êµìœ¡ê³¼ì •ì— ëŒ€í•´ ë²¼ë¦¬ì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”! : â–·")

# 11. ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_input:
    # ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë¡
    st.session_state.chat_history.append(("ë¯¼ì›ì¸", user_input))
    # ë¹ˆ ì±—ë´‡ ì‘ë‹µ ìë¦¬ ì¶”ê°€
    st.session_state.chat_history.append(("ë²¼ë¦¬", ""))  # ë¯¸ë¦¬ ìë¦¬ ë§Œë“¤ê¸°

# 12. ëŒ€í™” ì¶œë ¥ (ì±—ë´‡ ìŠ¤íŠ¸ë¦¬ë°ë„ ì—¬ê¸°ì„œ í•¨ê»˜ ì²˜ë¦¬)
for i, (role, msg) in enumerate(st.session_state.chat_history):
    if role == "ë¯¼ì›ì¸":
        with st.chat_message("user"):
            st.markdown(msg)

    elif role == "ë²¼ë¦¬" and msg == "":
        with st.chat_message("assistant", avatar="byeory.png"):
            message_placeholder = st.empty()
            handler = StreamHandler(message_placeholder)

            # ì‚¬ìš©ì ì…ë ¥ì€ ì§ì „ í•­ëª© ê¸°ì¤€
            last_user_input = None
            for j in range(i - 1, -1, -1):
                if st.session_state.chat_history[j][0] == "ë¯¼ì›ì¸":
                    last_user_input = st.session_state.chat_history[j][1]
                    break

            if last_user_input:
                # ê²€ìƒ‰ + ë‹µë³€ ìƒì„±
                search_results = combined_search(last_user_input)
                context = format_docs(search_results)
                formatted_prompt = prompt.format(
                      question=last_user_input,
                      context=format_docs(search_results),
                      chat_history=convert_chat_history_to_str(st.session_state.chat_history)
)

                llm = ChatOpenAI(
                    model_name="gpt-4o",
                    streaming=True,
                    callbacks=[handler],
                    openai_api_key=openai_api_key,
                    temperature=0.3,
                )
                llm.invoke([HumanMessage(content=formatted_prompt)])

                # ìŠ¤íŠ¸ë¦¬ë°ëœ ë‹µë³€ ì €ì¥
                st.session_state.chat_history[i] = ("ë²¼ë¦¬", handler.generated_text)

    else:
        with st.chat_message("assistant", avatar="byeory2.png"):
            st.markdown(msg)
