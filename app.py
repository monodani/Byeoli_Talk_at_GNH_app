import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA

# 1. API KEY ê°€ì ¸ì˜¤ê¸°
openai_api_key = st.secrets["OPENAI_API_KEY"]

# 2. PDFì—ì„œ ë°ì´í„° ì¶”ì¶œ ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (ì•± ì‹œì‘ ì‹œ 1íšŒë§Œ)
@st.cache_resource(show_spinner=True)
def load_vectorstore(pdf_path):
    loader = PyPDFLoader(pdf_path)
    pages = loader.load_and_split()
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.from_documents(pages, embeddings)
    return vectorstore

vectorstore = load_vectorstore("í†µê³„ê°€ì´ë“œ.pdf")

# 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = PromptTemplate.from_template("""
SYSTEM: ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥(context) ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question) ì— ë‹µí•˜ì„¸ìš”. ë§Œì•½, ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ 'ì£¼ì–´ì§„ ì •ë³´ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤' ë¼ê³  ë‹µí•˜ì„¸ìš”.
ê¸°ìˆ ì ì¸ ìš©ì–´ë‚˜ ì´ë¦„ì€ ë²ˆì—­í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”. ì¶œì²˜(page, source)ë¥¼ ë‹µë³€ì— í¬í•¨í•˜ì„¸ìš”. ë‹µë³€ì€ í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

HUMAN:
#Question: {question}

#Context: {context}

#Answer:
""")

# 4. ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬
class StreamHandler(BaseCallbackHandler):
    def __init__(self):
        self.text_area = st.empty()
        self.full_text = ""

    def on_llm_new_token(self, token: str, **kwargs):
        self.full_text += token
        self.text_area.markdown(self.full_text)

# 5. ëŒ€í™” ë‚´ì—­ ì´ˆê¸°í™”
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

st.title("ğŸ“Š í†µê³„ê°€ì´ë“œ ê¸°ë°˜ ë¯¼ì› ì±—ë´‡")

# 6. ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("í†µê³„ê°€ì´ë“œ.pdfì—ì„œ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•˜ì„¸ìš”:")

def format_docs(docs):
    return "\n".join([
        f"<document><content>{doc.page_content}</content><source>{doc.metadata.get('source','í†µê³„ê°€ì´ë“œ.pdf')}</source><page>{doc.metadata.get('page',-1)+1 if 'page' in doc.metadata else '?'}></page></document>"
        for doc in docs
    ])

# 7. ì…ë ¥ì‹œ ì²˜ë¦¬
if user_input:
    st.session_state.chat_history.append(("ë¯¼ì›ì¸", user_input))

    # (1) PDFì—ì„œ context ì¶”ì¶œ
    retriever = vectorstore.as_retriever(search_kwargs={"k":5})
    search_results = retriever.invoke(user_input)
    context = format_docs(search_results)
    
    # (2) í”„ë¡¬í”„íŠ¸ ì ìš©
    formatted_prompt = prompt.format(question=user_input, context=context)
    
    # (3) ë‹µë³€ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
    handler = StreamHandler()
    llm = ChatOpenAI(
        streaming=True,
        callbacks=[handler],
        openai_api_key=openai_api_key,
        model_name="gpt-4o-mini",
        temperature=0.4,
    )
    response = llm.predict(formatted_prompt)
    st.session_state.chat_history.append(("ì±—ë´‡", response))

# 8. ëŒ€í™” ë‚´ì—­ ì¶œë ¥ (ìµœì‹  ëŒ€í™”ê°€ ì•„ë˜ì— ë³´ì´ë„ë¡, ìŠ¤í¬ë¡¤ ìë™ í•˜ë‹¨)
# 1) ì „ì²´ ëŒ€í™” ë‚´ì—­ì„ forë¬¸ìœ¼ë¡œ ìœ„ì—ì„œ ì•„ë˜ë¡œ ì¶œë ¥
for idx, (role, msg) in enumerate(st.session_state.chat_history):
    # st.chat_messageëŠ” Streamlit 1.25 ì´ìƒì—ì„œ ì§€ì›
    with st.chat_message(role):
        st.markdown(msg)
        # ë‹µë³€ì´ ìµœì‹ (ë§ˆì§€ë§‰)ì¸ ê²½ìš°, ì—¬ê¸°ì— ìë™ ìŠ¤í¬ë¡¤ anchor ì‚½ì…
        if idx == len(st.session_state.chat_history) - 1:
            st.markdown('<div id="bottom"></div>', unsafe_allow_html=True)

# 2) ë‹µë³€ ìƒì„±/ê°±ì‹  ì‹œ ìë™ìœ¼ë¡œ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤
st.markdown(
    """
    <script>
    var elem = document.getElementById('bottom');
    if (elem) elem.scrollIntoView({behavior: "smooth", block: "end"});
    </script>
    """,
    unsafe_allow_html=True
)
