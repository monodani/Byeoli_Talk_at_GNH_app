import streamlit as st

from langchain.chat_models import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings

# 1. API KEY ê°€ì ¸ì˜¤ê¸°
openai_api_key = st.secrets["OPENAI_API_KEY"]

# 2. PDFì—ì„œ ë°ì´í„° ì¶”ì¶œ ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (ì•± ì‹œì‘ ì‹œ 1íšŒë§Œ)
@st.cache_resource(show_spinner=True)
def load_vectorstore(pdf_path):
    loader = PyPDFLoader(pdf_path)
    pages = loader.load_and_split()
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.from_documents(pages, embeddings)
    return vectorstore

vectorstore = load_vectorstore("í†µê³„ê°€ì´ë“œ.pdf")

# 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = PromptTemplate.from_template("""
SYSTEM: ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥(context) ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question) ì— ë‹µí•˜ì„¸ìš”. ë§Œì•½, ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ 'ì£¼ì–´ì§„ ì •ë³´ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤' ë¼ê³  ë‹µí•˜ì„¸ìš”.
ê¸°ìˆ ì ì¸ ìš©ì–´ë‚˜ ì´ë¦„ì€ ë²ˆì—­í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”. ì¶œì²˜(page, source)ë¥¼ ë‹µë³€ì— í¬í•¨í•˜ì„¸ìš”. ë‹µë³€ì€ í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

HUMAN:
#Question: {question}

#Context: {context}

#Answer:
""")

# 4. ëŒ€í™” ë‚´ì—­(ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸)
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

st.title("ğŸ“Š í†µê³„ê°€ì´ë“œ ê¸°ë°˜ ë¯¼ì› ì±—ë´‡")

def format_docs(docs):
    return "\n".join([
        f"<document><content>{doc.page_content}</content><source>{doc.metadata.get('source','í†µê³„ê°€ì´ë“œ.pdf')}</source><page>{doc.metadata.get('page',-1)+1 if 'page' in doc.metadata else '?'}></page></document>"
        for doc in docs
    ])

# 5. ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ (ë§¨ ì•„ë˜ì—ì„œë§Œ ì¶œë ¥)
class StreamHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs):
        st.session_state.chat_history[-1]["message"] += token
        rerun_chat_display()

# 6. ëŒ€í™” ë‚´ì—­ ë Œë”ë§ (í•­ìƒ ìµœì‹  ë©”ì‹œì§€ê°€ ì•„ë˜)
def rerun_chat_display():
    st.empty()
    for idx, chat in enumerate(st.session_state.chat_history):
        with st.chat_message(chat["role"]):
            st.markdown(chat["message"])
            if idx == len(st.session_state.chat_history) - 1:
                st.markdown('<div id="bottom"></div>', unsafe_allow_html=True)
    st.markdown(
        """
        <script>
        var elem = document.getElementById('bottom');
        if (elem) elem.scrollIntoView({behavior: "smooth", block: "end"});
        </script>
        """,
        unsafe_allow_html=True
    )

# 7. ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
user_input = st.chat_input("í†µê³„ê°€ì´ë“œì—ì„œ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•˜ì„¸ìš”:")

if user_input:
    # 1) ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì¶”ê°€
    st.session_state.chat_history.append({"role": "ë¯¼ì›ì¸", "message": user_input})
    rerun_chat_display()

    # 2) context ì¶”ì¶œ
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    search_results = retriever.invoke(user_input)
    context = format_docs(search_results)
    
    # 3) í”„ë¡¬í”„íŠ¸ ì ìš©
    formatted_prompt = prompt.format(question=user_input, context=context)
    
    # 4) ì±—ë´‡ ë‹µë³€ ìë¦¬(ë¹ˆ ë¬¸ìì—´) ì¶”ê°€
    st.session_state.chat_history.append({"role": "ì±—ë´‡", "message": ""})

    # 5) ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ ì‹¤ì‹œê°„ ê°±ì‹ 
    handler = StreamHandler()
    llm = ChatOpenAI(
        streaming=True,
        callbacks=[handler],
        openai_api_key=openai_api_key,
        model_name="gpt-4o-mini",
        temperature=0.4,
    )
    response = llm.predict(formatted_prompt)
    st.session_state.chat_history[-1]["message"] = response
    rerun_chat_display()

else:
    rerun_chat_display()
