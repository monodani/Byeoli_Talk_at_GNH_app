#!/usr/bin/env python3
"""
ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› RAG ì±—ë´‡ - BaseLoader ê¸°ë³¸ í´ë˜ìŠ¤

ëª¨ë“  ë„ë©”ì¸ë³„ ë¡œë”ê°€ ìƒì†ë°›ëŠ” ê¸°ë³¸ í´ë˜ìŠ¤:
- ê³µí†µ ì¸í„°í˜ì´ìŠ¤ ì •ì˜
- í•´ì‹œ ê¸°ë°˜ ì¦ë¶„ ë¹Œë“œ
- ë²¡í„°ìŠ¤í† ì–´ ìë™ ìƒì„±
- ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…

ì£¼ìš” ë©”ì„œë“œ:
- process_domain_data(): ê° ë¡œë”ì—ì„œ êµ¬í˜„
- build_vectorstore(): FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
- validate_schema(): ìŠ¤í‚¤ë§ˆ ê²€ì¦ (ì„ íƒì )
"""

import logging
import hashlib
import time
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from utils.textifier import TextChunk

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒì )
try:
    from langchain_community.vectorstores import FAISS
    from langchain_community.embeddings import OpenAIEmbeddings
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class BaseLoader(ABC):
    """
    ëª¨ë“  ë„ë©”ì¸ ë¡œë”ì˜ ê¸°ë³¸ í´ë˜ìŠ¤
    
    ê³µí†µ ê¸°ëŠ¥:
    - ë„ë©”ì¸ë³„ ë°ì´í„° ì²˜ë¦¬
    - FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    - í•´ì‹œ ê¸°ë°˜ ì¦ë¶„ ë¹Œë“œ
    - ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
    """
    def __init__(self, domain=None, loader_id=None, source_dir=None, vectorstore_dir=None, target_dir=None, index_name=None, schema_dir=None, **kwargs):
    """
    BaseLoader ì´ˆê¸°í™”
    
    Args:
        domain: ë„ë©”ì¸ ì´ë¦„ (ì˜ˆ: "satisfaction") 
        loader_id: ë¡œë” ID (domainê³¼ ë™ì¼, í˜¸í™˜ì„±ìš©)
        source_dir: ì†ŒìŠ¤ ë°ì´í„° ë””ë ‰í„°ë¦¬
        vectorstore_dir: ë²¡í„°ìŠ¤í† ì–´ ì¶œë ¥ ë””ë ‰í„°ë¦¬  
        target_dir: ë²¡í„°ìŠ¤í† ì–´ ì¶œë ¥ ë””ë ‰í„°ë¦¬ (vectorstore_dirê³¼ ë™ì¼, í˜¸í™˜ì„±ìš©)
        index_name: ì¸ë±ìŠ¤ íŒŒì¼ëª…
        schema_dir: ìŠ¤í‚¤ë§ˆ ë””ë ‰í„°ë¦¬ (ì„ íƒì )
    """
    # í˜¸í™˜ì„± ì²˜ë¦¬
    self.domain = domain or loader_id
    self.source_dir = Path(source_dir or ".")
    
    # target_dirë„ ë°›ê¸° (vectorstore_dir ëŒ€ì‹ )
    if target_dir:
        self.vectorstore_dir = Path(target_dir)
    else:
        self.vectorstore_dir = Path(vectorstore_dir or ".")
    
    # index_name ê¸°ë³¸ê°’ ì„¤ì •  
    self.index_name = index_name or f"{self.domain}_index"
    
    # ë””ë ‰í„°ë¦¬ ìƒì„±
    self.vectorstore_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"âœ¨ {self.domain.upper()} BaseLoader ì´ˆê¸°í™” ì™„ë£Œ")
    
    @abstractmethod
    def process_domain_data(self) -> List[TextChunk]:
        """
        ë„ë©”ì¸ë³„ ë°ì´í„° ì²˜ë¦¬ (ê° ë¡œë”ì—ì„œ êµ¬í˜„)
        
        Returns:
            List[TextChunk]: ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ì²­í¬ë“¤
        """
        pass
    
    def get_supported_extensions(self) -> List[str]:
        """ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì (ì„ íƒì  ì˜¤ë²„ë¼ì´ë“œ)"""
        return ['.pdf', '.csv', '.txt', '.png', '.jpg']
    
    def validate_schema(self, file_path: Path, schema_path: Path) -> bool:
        """
        ìŠ¤í‚¤ë§ˆ ê²€ì¦ (ì„ íƒì )
        
        Args:
            file_path: ê²€ì¦í•  íŒŒì¼ ê²½ë¡œ
            schema_path: ìŠ¤í‚¤ë§ˆ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            bool: ê²€ì¦ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ê°„ë‹¨í•œ íŒŒì¼ ì¡´ì¬ ê²€ì¦
            return file_path.exists()
        except Exception as e:
            logger.warning(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return True  # ê²€ì¦ ì‹¤íŒ¨í•´ë„ ì²˜ë¦¬ ê³„ì†
    
    def calculate_source_hash(self) -> str:
        """ì†ŒìŠ¤ ë°ì´í„° í•´ì‹œ ê³„ì‚° (ì¦ë¶„ ë¹Œë“œìš©)"""
        try:
            hash_md5 = hashlib.md5()
            
            # ì†ŒìŠ¤ ë””ë ‰í„°ë¦¬ì˜ ëª¨ë“  íŒŒì¼ í•´ì‹œ ê³„ì‚°
            for file_path in self.source_dir.rglob('*'):
                if file_path.is_file() and file_path.suffix in self.get_supported_extensions():
                    hash_md5.update(str(file_path.stat().st_mtime).encode())
                    hash_md5.update(str(file_path.stat().st_size).encode())
            
            return hash_md5.hexdigest()[:16]
        except Exception as e:
            logger.warning(f"í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return str(int(time.time()))  # í´ë°±: íƒ€ì„ìŠ¤íƒ¬í”„
    
    def needs_rebuild(self) -> bool:
        """ì¬ë¹Œë“œ í•„ìš” ì—¬ë¶€ í™•ì¸"""
        try:
            # ë²¡í„°ìŠ¤í† ì–´ íŒŒì¼ ì¡´ì¬ í™•ì¸
            faiss_file = self.vectorstore_dir / f"{self.index_name}.faiss"
            pkl_file = self.vectorstore_dir / f"{self.index_name}.pkl"
            
            if not (faiss_file.exists() and pkl_file.exists()):
                logger.info(f"ğŸ”¨ {self.domain}: ë²¡í„°ìŠ¤í† ì–´ íŒŒì¼ì´ ì—†ì–´ì„œ ìƒˆë¡œ ë¹Œë“œ")
                return True
            
            # í•´ì‹œ íŒŒì¼ í™•ì¸
            hash_file = self.vectorstore_dir / ".source_hash"
            if not hash_file.exists():
                logger.info(f"ğŸ”¨ {self.domain}: í•´ì‹œ íŒŒì¼ì´ ì—†ì–´ì„œ ìƒˆë¡œ ë¹Œë“œ")
                return True
            
            # í•´ì‹œ ë¹„êµ
            current_hash = self.calculate_source_hash()
            with open(hash_file, 'r') as f:
                stored_hash = f.read().strip()
            
            if current_hash != stored_hash:
                logger.info(f"ğŸ”¨ {self.domain}: ì†ŒìŠ¤ ë°ì´í„° ë³€ê²½ìœ¼ë¡œ ì¬ë¹Œë“œ")
                return True
            
            logger.info(f"âœ… {self.domain}: ë²¡í„°ìŠ¤í† ì–´ê°€ ìµœì‹  ìƒíƒœ")
            return False
            
        except Exception as e:
            logger.warning(f"ì¬ë¹Œë“œ í™•ì¸ ì‹¤íŒ¨: {e}")
            return True  # í™•ì¸ ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ì¬ë¹Œë“œ
    
    def save_source_hash(self):
        """í˜„ì¬ ì†ŒìŠ¤ í•´ì‹œ ì €ì¥"""
        try:
            current_hash = self.calculate_source_hash()
            hash_file = self.vectorstore_dir / ".source_hash"
            with open(hash_file, 'w') as f:
                f.write(current_hash)
            logger.debug(f"ğŸ“ {self.domain}: ì†ŒìŠ¤ í•´ì‹œ ì €ì¥ë¨")
        except Exception as e:
            logger.warning(f"í•´ì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def build_vectorstore(self, force_rebuild: bool = False) -> bool:
        """
        ë²¡í„°ìŠ¤í† ì–´ ë¹Œë“œ (ì¦ë¶„ ë¹Œë“œ ì§€ì›)
        
        Args:
            force_rebuild: ê°•ì œ ì¬ë¹Œë“œ ì—¬ë¶€
            
        Returns:
            bool: ë¹Œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ì¬ë¹Œë“œ í•„ìš”ì„± í™•ì¸
            if not force_rebuild and not self.needs_rebuild():
                logger.info(f"â­ï¸ {self.domain}: ì´ë¯¸ ìµœì‹  ë²¡í„°ìŠ¤í† ì–´ ì¡´ì¬")
                return True
            
            logger.info(f"ğŸ”¨ {self.domain}: ë²¡í„°ìŠ¤í† ì–´ ë¹Œë“œ ì‹œì‘...")
            start_time = time.time()
            
            # 1. ë„ë©”ì¸ ë°ì´í„° ì²˜ë¦¬
            chunks = self.process_domain_data()
            
            if not chunks:
                logger.warning(f"âš ï¸ {self.domain}: ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            logger.info(f"ğŸ“„ {self.domain}: {len(chunks)}ê°œ ì²­í¬ ìƒì„±ë¨")
            
            # 2. FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            if not FAISS_AVAILABLE:
                logger.error(f"âŒ {self.domain}: FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                return False
            
            success = self._create_faiss_vectorstore(chunks)
            
            if success:
                # 3. í•´ì‹œ ì €ì¥
                self.save_source_hash()
                
                elapsed_time = time.time() - start_time
                logger.info(f"âœ… {self.domain}: ë²¡í„°ìŠ¤í† ì–´ ë¹Œë“œ ì™„ë£Œ ({elapsed_time:.2f}s)")
                return True
            else:
                logger.error(f"âŒ {self.domain}: ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            logger.error(f"âŒ {self.domain}: ë²¡í„°ìŠ¤í† ì–´ ë¹Œë“œ ì¤‘ ì˜ˆì™¸: {e}")
            return False
    
    def _create_faiss_vectorstore(self, chunks: List[TextChunk]) -> bool:
        """FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (ë‚´ë¶€ ë©”ì„œë“œ)"""
        try:
            # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            embeddings = OpenAIEmbeddings()
            
            # í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            texts = [chunk.text for chunk in chunks]
            metadatas = [chunk.metadata for chunk in chunks]
            
            # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            vectorstore = FAISS.from_texts(
                texts=texts,
                embedding=embeddings,
                metadatas=metadatas
            )
            
            # ì €ì¥
            vectorstore.save_local(
                folder_path=str(self.vectorstore_dir),
                index_name=self.index_name
            )
            
            # ìƒì„± í™•ì¸
            faiss_file = self.vectorstore_dir / f"{self.index_name}.faiss"
            pkl_file = self.vectorstore_dir / f"{self.index_name}.pkl"
            
            if faiss_file.exists() and pkl_file.exists():
                file_size = faiss_file.stat().st_size / (1024*1024)  # MB
                logger.info(f"ğŸ’¾ {self.domain}: ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ë¨ ({file_size:.1f}MB)")
                return True
            else:
                logger.error(f"âŒ {self.domain}: ë²¡í„°ìŠ¤í† ì–´ íŒŒì¼ ìƒì„± ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            logger.error(f"âŒ {self.domain}: FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def load_vectorstore(self) -> Optional[FAISS]:
        """ìƒì„±ëœ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ"""
        try:
            if not FAISS_AVAILABLE:
                logger.error("FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                return None
            
            embeddings = OpenAIEmbeddings()
            vectorstore = FAISS.load_local(
                folder_path=str(self.vectorstore_dir),
                embeddings=embeddings,
                index_name=self.index_name,
                allow_dangerous_deserialization=True
            )
            
            logger.info(f"ğŸ“š {self.domain}: ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì„±ê³µ")
            return vectorstore
            
        except Exception as e:
            logger.error(f"âŒ {self.domain}: ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def get_stats(self) -> Dict[str, Any]:
        """ë²¡í„°ìŠ¤í† ì–´ í†µê³„ ì •ë³´"""
        try:
            faiss_file = self.vectorstore_dir / f"{self.index_name}.faiss"
            pkl_file = self.vectorstore_dir / f"{self.index_name}.pkl"
            
            stats = {
                'domain': self.domain,
                'vectorstore_exists': faiss_file.exists() and pkl_file.exists(),
                'faiss_size_mb': faiss_file.stat().st_size / (1024*1024) if faiss_file.exists() else 0,
                'last_modified': datetime.fromtimestamp(faiss_file.stat().st_mtime).isoformat() if faiss_file.exists() else None,
                'source_dir': str(self.source_dir),
                'vectorstore_dir': str(self.vectorstore_dir)
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"í†µê³„ ì •ë³´ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'domain': self.domain, 'error': str(e)}


# ================================================================
# ê°œë°œ/í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# ================================================================

def test_base_loader():
    """BaseLoader ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    class TestLoader(BaseLoader):
        """í…ŒìŠ¤íŠ¸ìš© ë¡œë”"""
        
        def process_domain_data(self) -> List[TextChunk]:
            return [
                TextChunk(
                    text="í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
                    metadata={'test': True},
                    source_id="test.txt"
                )
            ]
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    loader = TestLoader(
        domain="test",
        source_dir=Path("test_data"),
        vectorstore_dir=Path("test_vectorstore"),
        index_name="test_index"
    )
    
    print("âœ… BaseLoader í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    test_base_loader()
