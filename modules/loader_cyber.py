#!/usr/bin/env python3
"""
ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› RAG ì±—ë´‡ - ì‚¬ì´ë²„ êµìœ¡ ë¡œë” (BaseLoader íŒ¨í„´ ì¤€ìˆ˜)

notice ë¡œë” íŒ¨í„´ì„ ë”°ë¼ ì™„ì „íˆ ìˆ˜ì •ë¨:
- process_domain_data(self) ì‹œê·¸ë‹ˆì²˜ë¡œ ë³€ê²½
- ì›ë³¸ CSV íŒŒì¼ ì§ì ‘ ì½ê¸° ë¡œì§ ì¶”ê°€
- BaseLoader í‘œì¤€ íŒ¨í„´ ì™„ì „ ì¤€ìˆ˜
- ê¸°ì¡´ í…œí”Œë¦¿ ì‹œìŠ¤í…œ ìœ ì§€
"""

import logging
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from modules.base_loader import BaseLoader
from utils.textifier import TextChunk
from utils.config import config

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class CyberLoader(BaseLoader):
    """
    ì‚¬ì´ë²„ êµìœ¡ ë¡œë” - BaseLoader í‘œì¤€ íŒ¨í„´ ì¤€ìˆ˜
    
    ì²˜ë¦¬ ëŒ€ìƒ:
    - data/cyber/mingan.csv (ë¯¼ê°„ìœ„íƒ ì‚¬ì´ë²„êµìœ¡)
    - data/cyber/nara.csv (ë‚˜ë¼ë°°ì›€í„° ì‚¬ì´ë²„êµìœ¡)
    
    íŠ¹ì§•:
    - notice ë¡œë”ì™€ ë™ì¼í•œ process_domain_data(self) ì‹œê·¸ë‹ˆì²˜
    - ì›ë³¸ CSV íŒŒì¼ ì§ì ‘ ì½ê¸°
    - ê¸°ì¡´ í…œí”Œë¦¿ ì‹œìŠ¤í…œ ì™„ë²½ ë³´ì¡´
    - í•´ì‹œ ê¸°ë°˜ ì¦ë¶„ ë¹Œë“œ ì§€ì›
    """
    
    # ê¸°ì¡´ í…œí”Œë¦¿ ë³´ì¡´ (ê²€ì¦ëœ ì½”ë© ë¡œì§)
    MINGAN_TEMPLATE = """'{êµìœ¡ê³¼ì •}' ê³¼ì •ì€, 2025ë…„ ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì›ì—ì„œ ìš´ì˜í•˜ê³  ìˆëŠ” ë¯¼ê°„ìœ„íƒ ì‚¬ì´ë²„êµìœ¡ ê³¼ì • ì¤‘ í•˜ë‚˜ë¡œ, {ê°œë°œì—°ë„}ë…„ {ê°œë°œì›”}ì›”ì— ë§Œë“¤ì–´ì§„ êµìœ¡ ì½˜í…ì¸ ë¡œ ë‚´ìš© ë¶„ë¥˜ìƒ {êµ¬ë¶„}>{ëŒ€ë¶„ë¥˜}>{ì¤‘ë¶„ë¥˜}>{ì†Œë¶„ë¥˜}>{ì„¸ë¶„ë¥˜}ì— í•´ë‹¹ë˜ê³ , í•™ìŠµì‹œê°„ì€ {í•™ìŠµì‹œê°„}ì‹œê°„ì´ë©°, í•™ìŠµì— ëŒ€í•œ êµìœ¡ ì¸ì •ì‹œê°„ì€ {ì¸ì •ì‹œê°„}ì‹œê°„ì…ë‹ˆë‹¤.
---
"""

    NARA_TEMPLATE = """'{êµìœ¡ê³¼ì •}' ê³¼ì •ì€, 2025ë…„ ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› ë‚˜ë¼ë°°ì›€í„°ì—ì„œ ìš´ì˜í•˜ëŠ” ê³µë™í™œìš© ë‚˜ë¼ì½˜í…ì¸ ë¥¼ í™œìš©í•œ êµìœ¡ê³¼ì •ìœ¼ë¡œ, ë‚´ìš© ë¶„ë¥˜ìƒ {ë¶„ë¥˜}ì— í•´ë‹¹ë˜ë©°, í•™ìŠµì‹œê°„ì€ {í•™ìŠµì°¨ì‹œ}ì´ê³  í•™ìŠµì— ëŒ€í•œ êµìœ¡ ì¸ì •ì‹œê°„ì€ {ì¸ì •ì‹œê°„}ì…ë‹ˆë‹¤. ì°¸ê³ ì‚¬í•­ìœ¼ë¡œ, ë³¸ ê³¼ì •ì€ êµìœ¡ ë§ë¯¸ì— ì§„í–‰ë˜ëŠ” ë³„ë„ì˜ í‰ê°€ê°€ {í‰ê°€ìœ ë¬´}.
---
"""
    
    def __init__(self):
        super().__init__(
            domain="cyber",
            source_dir=config.ROOT_DIR / "data" / "cyber",
            vectorstore_dir=config.ROOT_DIR / "vectorstores" / "vectorstore_cyber",
            index_name="cyber_index"
        )
        
        # ì²˜ë¦¬í•  íŒŒì¼ ì •ì˜
        self.mingan_file = self.source_dir / "mingan.csv"
        self.nara_file = self.source_dir / "nara.csv"
    
    def process_domain_data(self) -> List[TextChunk]:
        """
        BaseLoader ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„: ì‚¬ì´ë²„ êµìœ¡ ë°ì´í„° ì²˜ë¦¬
        
        âœ… notice ë¡œë”ì™€ ë™ì¼í•œ ì‹œê·¸ë‹ˆì²˜: process_domain_data(self)
        """
        all_chunks = []
        
        # 1. ë¯¼ê°„ìœ„íƒ ì‚¬ì´ë²„êµìœ¡ ì²˜ë¦¬
        mingan_chunks = self._process_mingan_csv()
        all_chunks.extend(mingan_chunks)
        
        # 2. ë‚˜ë¼ë°°ì›€í„° ì‚¬ì´ë²„êµìœ¡ ì²˜ë¦¬
        nara_chunks = self._process_nara_csv()
        all_chunks.extend(nara_chunks)
        
        logger.info(f"âœ… ì‚¬ì´ë²„ êµìœ¡ í†µí•© ì²˜ë¦¬ ì™„ë£Œ: ë¯¼ê°„ {len(mingan_chunks)}ê°œ + ë‚˜ë¼ {len(nara_chunks)}ê°œ = ì´ {len(all_chunks)}ê°œ ì²­í¬")
        
        return all_chunks
    
    def _process_mingan_csv(self) -> List[TextChunk]:
        """ë¯¼ê°„ìœ„íƒ ì‚¬ì´ë²„êµìœ¡ CSV ì§ì ‘ ì½ê¸° ë° ì²˜ë¦¬"""
        chunks = []
        
        if not self.mingan_file.exists():
            logger.warning(f"ë¯¼ê°„ìœ„íƒ ì‚¬ì´ë²„êµìœ¡ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.mingan_file}")
            return chunks
        
        try:
            logger.info(f"ğŸ¢ ë¯¼ê°„ìœ„íƒ ì‚¬ì´ë²„êµìœ¡ ì²˜ë¦¬ ì‹œì‘: {self.mingan_file}")
            
            # CSV ì§ì ‘ ì½ê¸°
            df = self._read_csv_with_encoding(self.mingan_file)
            
            if df is None:
                return chunks
            
            logger.info(f"ğŸ“„ ë¯¼ê°„ìœ„íƒ ë°ì´í„°: {len(df)}í–‰ ë¡œë“œë¨")
            
            # ê° í–‰ì„ ê¸°ì¡´ í…œí”Œë¦¿ìœ¼ë¡œ ë³€í™˜
            for idx, row in df.iterrows():
                try:
                    # ë°ì´í„° ê²€ì¦ ë° ì •ì œ
                    clean_data = self._validate_and_clean_mingan_data(row.to_dict(), f"mingan_row_{idx}")
                    if not clean_data:
                        continue
                    
                    # ê¸°ì¡´ í…œí”Œë¦¿ ì ìš©
                    try:
                        formatted_content = self.MINGAN_TEMPLATE.format(**clean_data)
                    except KeyError as e:
                        logger.error(f"ë¯¼ê°„ìœ„íƒ í…œí”Œë¦¿ ì ìš© ì‹¤íŒ¨ (í–‰ {idx}): ëˆ„ë½ í•„ë“œ {e}")
                        continue
                    
                    # ë©”íƒ€ë°ì´í„° ìƒì„±
                    metadata = {
                        'source_file': 'mingan.csv',
                        'source_id': f'cyber/mingan.csv#row_{idx}',
                        'cyber_type': 'mingan',
                        'education_course': clean_data.get('êµìœ¡ê³¼ì •', ''),
                        'development_year': str(clean_data.get('ê°œë°œì—°ë„', '')),
                        'development_month': str(clean_data.get('ê°œë°œì›”', '')),
                        'category_path': f"{clean_data.get('êµ¬ë¶„', '')}>{clean_data.get('ëŒ€ë¶„ë¥˜', '')}>{clean_data.get('ì¤‘ë¶„ë¥˜', '')}>{clean_data.get('ì†Œë¶„ë¥˜', '')}>{clean_data.get('ì„¸ë¶„ë¥˜', '')}",
                        'learning_hours': self._safe_convert_to_float(clean_data.get('í•™ìŠµì‹œê°„', '')),
                        'recognition_hours': self._safe_convert_to_float(clean_data.get('ì¸ì •ì‹œê°„', '')),
                        'cache_ttl': 2592000,  # 30ì¼ TTL
                        'processing_date': datetime.now().isoformat(),
                        'chunk_type': 'cyber_mingan'
                    }
                    
                    chunk = TextChunk(
                        text=formatted_content,
                        source_id=metadata['source_id'],
                        metadata=metadata
                    )
                    
                    chunks.append(chunk)
                    
                except Exception as e:
                    logger.error(f"ë¯¼ê°„ìœ„íƒ í–‰ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… ë¯¼ê°„ìœ„íƒ ì‚¬ì´ë²„êµìœ¡ ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
        except Exception as e:
            logger.error(f"âŒ ë¯¼ê°„ìœ„íƒ ì‚¬ì´ë²„êµìœ¡ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        return chunks
    
    def _process_nara_csv(self) -> List[TextChunk]:
        """ë‚˜ë¼ë°°ì›€í„° ì‚¬ì´ë²„êµìœ¡ CSV ì§ì ‘ ì½ê¸° ë° ì²˜ë¦¬"""
        chunks = []
        
        if not self.nara_file.exists():
            logger.warning(f"ë‚˜ë¼ë°°ì›€í„° ì‚¬ì´ë²„êµìœ¡ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.nara_file}")
            return chunks
        
        try:
            logger.info(f"ğŸ›ï¸ ë‚˜ë¼ë°°ì›€í„° ì‚¬ì´ë²„êµìœ¡ ì²˜ë¦¬ ì‹œì‘: {self.nara_file}")
            
            # CSV ì§ì ‘ ì½ê¸°
            df = self._read_csv_with_encoding(self.nara_file)
            
            if df is None:
                return chunks
            
            logger.info(f"ğŸ“„ ë‚˜ë¼ë°°ì›€í„° ë°ì´í„°: {len(df)}í–‰ ë¡œë“œë¨")
            
            # ê° í–‰ì„ ê¸°ì¡´ í…œí”Œë¦¿ìœ¼ë¡œ ë³€í™˜
            for idx, row in df.iterrows():
                try:
                    # ë°ì´í„° ê²€ì¦ ë° ì •ì œ
                    clean_data = self._validate_and_clean_nara_data(row.to_dict(), f"nara_row_{idx}")
                    if not clean_data:
                        continue
                    
                    # ê¸°ì¡´ í…œí”Œë¦¿ ì ìš©
                    try:
                        formatted_content = self.NARA_TEMPLATE.format(**clean_data)
                    except KeyError as e:
                        logger.error(f"ë‚˜ë¼ë°°ì›€í„° í…œí”Œë¦¿ ì ìš© ì‹¤íŒ¨ (í–‰ {idx}): ëˆ„ë½ í•„ë“œ {e}")
                        continue
                    
                    # ë©”íƒ€ë°ì´í„° ìƒì„±
                    metadata = {
                        'source_file': 'nara.csv',
                        'source_id': f'cyber/nara.csv#row_{idx}',
                        'cyber_type': 'nara',
                        'education_course': clean_data.get('êµìœ¡ê³¼ì •', ''),
                        'classification': clean_data.get('ë¶„ë¥˜', ''),
                        'learning_sessions': str(clean_data.get('í•™ìŠµì°¨ì‹œ', '')),
                        'recognition_hours': self._safe_convert_to_float(clean_data.get('ì¸ì •ì‹œê°„', '')),
                        'evaluation_required': clean_data.get('í‰ê°€ìœ ë¬´', ''),
                        'cache_ttl': 2592000,  # 30ì¼ TTL
                        'processing_date': datetime.now().isoformat(),
                        'chunk_type': 'cyber_nara'
                    }
                    
                    chunk = TextChunk(
                        text=formatted_content,
                        source_id=metadata['source_id'],
                        metadata=metadata
                    )
                    
                    chunks.append(chunk)
                    
                except Exception as e:
                    logger.error(f"ë‚˜ë¼ë°°ì›€í„° í–‰ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… ë‚˜ë¼ë°°ì›€í„° ì‚¬ì´ë²„êµìœ¡ ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
        except Exception as e:
            logger.error(f"âŒ ë‚˜ë¼ë°°ì›€í„° ì‚¬ì´ë²„êµìœ¡ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        return chunks
    
    def _read_csv_with_encoding(self, csv_file: Path) -> Optional[pd.DataFrame]:
        """ì¸ì½”ë”© ìë™ ê°ì§€ë¡œ CSV ì½ê¸°"""
        encodings = ['utf-8', 'cp949', 'euc-kr', 'utf-8-sig']
        
        for encoding in encodings:
            try:
                df = pd.read_csv(csv_file, encoding=encoding)
                logger.info(f"CSV íŒŒì¼ ë¡œë“œ ì„±ê³µ (ì¸ì½”ë”©: {encoding})")
                return df
                
            except UnicodeDecodeError:
                logger.debug(f"ì¸ì½”ë”© {encoding} ì‹¤íŒ¨, ë‹¤ìŒ ì‹œë„...")
                continue
            except Exception as e:
                logger.error(f"CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ (ì¸ì½”ë”©: {encoding}): {e}")
                continue
        
        logger.error(f"ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨: {csv_file}")
        return None
    
    def _validate_and_clean_mingan_data(self, row_data: Dict[str, Any], source_id: str) -> Optional[Dict[str, str]]:
        """ë¯¼ê°„ìœ„íƒ ë°ì´í„° ê²€ì¦ ë° ì •ì œ"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            required_fields = ['êµìœ¡ê³¼ì •', 'ê°œë°œì—°ë„', 'í•™ìŠµì‹œê°„', 'ì¸ì •ì‹œê°„']
            for field in required_fields:
                if field not in row_data or pd.isna(row_data[field]):
                    logger.warning(f"ë¯¼ê°„ìœ„íƒ í•„ìˆ˜ í•„ë“œ ëˆ„ë½ ({source_id}): {field}")
                    return None
            
            # ë°ì´í„° ì •ì œ
            clean_data = {}
            for key, value in row_data.items():
                if pd.isna(value):
                    clean_data[key] = ''
                else:
                    clean_data[key] = str(value).strip()
            
            return clean_data
            
        except Exception as e:
            logger.error(f"ë¯¼ê°„ìœ„íƒ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨ ({source_id}): {e}")
            return None
    
    def _validate_and_clean_nara_data(self, row_data: Dict[str, Any], source_id: str) -> Optional[Dict[str, str]]:
        """ë‚˜ë¼ë°°ì›€í„° ë°ì´í„° ê²€ì¦ ë° ì •ì œ"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            required_fields = ['êµìœ¡ê³¼ì •', 'ë¶„ë¥˜', 'í•™ìŠµì°¨ì‹œ', 'ì¸ì •ì‹œê°„']
            for field in required_fields:
                if field not in row_data or pd.isna(row_data[field]):
                    logger.warning(f"ë‚˜ë¼ë°°ì›€í„° í•„ìˆ˜ í•„ë“œ ëˆ„ë½ ({source_id}): {field}")
                    return None
            
            # ë°ì´í„° ì •ì œ
            clean_data = {}
            for key, value in row_data.items():
                if pd.isna(value):
                    clean_data[key] = ''
                else:
                    clean_data[key] = str(value).strip()
            
            return clean_data
            
        except Exception as e:
            logger.error(f"ë‚˜ë¼ë°°ì›€í„° ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨ ({source_id}): {e}")
            return None
    
    def _safe_convert_to_float(self, value: Any) -> float:
        """ì•ˆì „í•œ float ë³€í™˜"""
        try:
            if pd.isna(value) or value == '':
                return 0.0
            return float(str(value).strip())
        except (ValueError, TypeError):
            return 0.0


# ================================================================
# ê°œë°œ/í…ŒìŠ¤íŠ¸ìš© ì§„ì…ì 
# ================================================================

def main():
    """ê°œë°œ/í…ŒìŠ¤íŠ¸ìš© ì§„ì…ì """
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    loader = CyberLoader()
    
    try:
        # BaseLoaderì˜ í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
        success = loader.build_vectorstore()
        
        if success:
            logger.info("âœ… ì‚¬ì´ë²„ êµìœ¡ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ")
        else:
            logger.error("âŒ ì‚¬ì´ë²„ êµìœ¡ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì‹¤íŒ¨")
            
    except Exception as e:
        logger.error(f"âŒ ë¡œë” ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise


if __name__ == '__main__':
    main()
