#!/usr/bin/env python3
"""
ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› RAG ì±—ë´‡ - ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ ê³µì§€ì‚¬í•­ ë¡œë”

data/notice/notice.txt íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ 
vectorstore_notice ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

ğŸ¯ ëª©í‘œ: 5ê°œ ì´í•˜ ê³µì§€ì‚¬í•­ì„ ì™„ë²½í•˜ê²Œ ì²˜ë¦¬
- ë†“ì¹˜ëŠ” ê³µì§€ = 0ê°œ
- ì–´ë–¤ ê²€ìƒ‰ì–´ë¡œë„ ì°¾ì„ ìˆ˜ ìˆê²Œ
- ìµœëŒ€ ì •í™•ë„ ì¶”êµ¬ (ì†ë„ëŠ” ì‹ ê²½ ì•ˆ ì”€)
"""

import logging
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Set, Tuple
from datetime import datetime, date
from urllib.parse import urlparse
import hashlib
from collections import Counter
import difflib

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from modules.base_loader import BaseLoader
from utils.textifier import TextChunk
from utils.config import config

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class PremiumNoticeLoader(BaseLoader):
    """
    í”„ë¦¬ë¯¸ì—„ ê³µì§€ì‚¬í•­ ë¡œë” - ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ì •í™•ë„
    
    ì²˜ë¦¬ ëŒ€ìƒ:
    - data/notice/notice.txt (ìµœëŒ€ 5ê°œ ê³µì§€ì‚¬í•­)
    
    íŠ¹ì§•:
    - ì™„ë²½í•œ ì •í™•ë„ ì¶”êµ¬ (ì†ë„ < í’ˆì§ˆ)
    - ë‹¤ì¤‘ ì•Œê³ ë¦¬ì¦˜ ìœµí•© í‚¤ì›Œë“œ ì¶”ì¶œ
    - ë™ì˜ì–´/ìœ ì‚¬ì–´/ì˜¤íƒ€ í—ˆìš© ê²€ìƒ‰
    - ì˜ë¯¸ë¡ ì  ë¶„ì„ ë° ê°ì • ë¶„ì„
    - VIP ëŒ€ìš°: ê° ê³µì§€ì‚¬í•­ì„ ë³´ì„ì²˜ëŸ¼ ì„¸ë°€ ê°€ê³µ
    """
    
    def __init__(self):
        super().__init__(
            domain="notice",
            source_dir=config.ROOT_DIR / "data" / "notice",
            vectorstore_dir=config.ROOT_DIR / "vectorstores" / "vectorstore_notice",
            index_name="notice_index"
        )
        
        # í”„ë¦¬ë¯¸ì—„ í…œí”Œë¦¿
        self.template = self._get_premium_template()
        
        # ğŸ¯ ë™ì˜ì–´ ì‚¬ì „ (í™•ì¥ ê°€ëŠ¥)
        self.synonym_dict = {
            'íœ´ë¬´': ['íœ´ì›', 'ë¬¸ë‹«ìŒ', 'ì‰¼', 'íœ´ê´€', 'íê´€', 'ìš´ì˜ì¤‘ë‹¨'],
            'ê¸´ê¸‰': ['ì‘ê¸‰', 'ì¦‰ì‹œ', 'ê¸‰í•¨', 'ì„œë‘˜ëŸ¬', 'ë¹¨ë¦¬', 'ì‹ ì†'],
            'ë³€ê²½': ['ìˆ˜ì •', 'ì¡°ì •', 'ë°”ë€œ', 'ê°œì •', 'ê°±ì‹ ', 'ì—…ë°ì´íŠ¸'],
            'ì·¨ì†Œ': ['ì¤‘ë‹¨', 'íì§€', 'ì² íšŒ', 'ë¬´íš¨', 'ì‚­ì œ'],
            'ì—°ê¸°': ['ë¯¸ë£¸', 'ì§€ì—°', 'ëŠ¦ì¶¤', 'ì—°ì¥'],
            'êµìœ¡': ['ìˆ˜ì—…', 'ê°•ì˜', 'í›ˆë ¨', 'í•™ìŠµ', 'ê³¼ì •'],
            'í‰ê°€': ['ì‹œí—˜', 'í…ŒìŠ¤íŠ¸', 'ì‹¬ì‚¬', 'ê²€í† ', 'ì±„ì '],
            'ì œì¶œ': ['ì ‘ìˆ˜', 'ì „ë‹¬', 'ì†¡ë¶€', 'ë³´ëƒ„'],
            'ë§ˆê°': ['ì¢…ë£Œ', 'ë', 'ì™„ë£Œ', 'ë°ë“œë¼ì¸'],
            'ì•ˆë‚´': ['ê³µì§€', 'ì•Œë¦¼', 'ê³ ì§€', 'í†µë³´', 'ì „ë‹¬']
        }
        
        # ğŸ¯ ê°ì •/ê¸´ê¸‰ë„ í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ í¬í•¨)
        self.urgency_keywords = {
            'ì´ˆê¸´ê¸‰': 100, 'ê¸´ê¸‰': 90, 'ì¦‰ì‹œ': 85, 'ì‹ ì†': 80,
            'ì¤‘ìš”': 70, 'í•„ìˆ˜': 65, 'ë°˜ë“œì‹œ': 60, 'ì£¼ì˜': 55,
            'ìœ ì˜': 50, 'ì°¸ê³ ': 30, 'ì•Œë¦¼': 20, 'ì•ˆë‚´': 10
        }
        
        # ğŸ¯ ìƒí™©ë³„ í‚¤ì›Œë“œ íŒ¨í„´
        self.situation_patterns = {
            'ì¬í•´': [r'(í™”ì¬|ì§€ì§„|íƒœí’|í™ìˆ˜|ì‚°ì‚¬íƒœ|í­ìš°|í­ì„¤)', r'ìì—°ì¬í•´', r'ì¬ë‚œìƒí™©'],
            'ë³´ê±´': [r'(ì½”ë¡œë‚˜|ë…ê°|ê°ì—¼|ë°©ì—­|ê²©ë¦¬|í™•ì§„)', r'ì§ˆë³‘', r'ì „ì—¼ë³‘'],
            'ì‹œì„¤': [r'(ê³µì‚¬|ë³´ìˆ˜|ìˆ˜ë¦¬|ì ê²€|ì •ë¹„)', r'ì‹œì„¤.{0,5}(ê³µì‚¬|ë³´ìˆ˜)', r'ê±´ë¬¼.{0,5}ê³µì‚¬'],
            'ì‹œìŠ¤í…œ': [r'(ì„œë²„|ì‹œìŠ¤í…œ|ë„¤íŠ¸ì›Œí¬|í™ˆí˜ì´ì§€).{0,5}(ì¥ì• |ì˜¤ë¥˜|ì ê²€)', r'ì „ì‚°.{0,5}(ì¥ì• |ì ê²€)'],
            'í–‰ì •': [r'(ì ‘ìˆ˜|ì‹ ì²­|ë§ˆê°|ì—°ì¥|ë³€ê²½).{0,10}(ì¼ì •|ê¸°ê°„)', r'ì„œë¥˜.{0,5}(ì œì¶œ|ì ‘ìˆ˜)']
        }
        
        # ğŸ¯ ì˜¤íƒ€ íŒ¨í„´ (ìì£¼ ë°œìƒí•˜ëŠ” ì˜¤íƒ€ë“¤)
        self.typo_corrections = {
            'íœ´ë­„': 'íœ´ë¬´', 'ê¸´ë”': 'ê¸´ê¸‰', 'ë³€ê²¨': 'ë³€ê²½', 
            'êµìœ¼': 'êµìœ¡', 'í‰ê¹Œ': 'í‰ê°€', 'ì•ˆì• ': 'ì•ˆë‚´',
            'ê²Œì‹œ': 'ê²Œì‹œ', 'ê³µì§€ì‚¬í•™': 'ê³µì§€ì‚¬í•­'
        }
    
    def get_supported_extensions(self) -> List[str]:
        """ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì ë°˜í™˜"""
        return ['.txt']
    
    def process_domain_data(self) -> List[TextChunk]:
        """
        í”„ë¦¬ë¯¸ì—„ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ - ìµœê³  í’ˆì§ˆ ë³´ì¥
        
        Returns:
            List[TextChunk]: ì™„ë²½í•˜ê²Œ ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ì²­í¬ë“¤
        """
        all_chunks = []
        
        # notice.txt íŒŒì¼ ì²˜ë¦¬
        notice_file = self.source_dir / "notice.txt"
        
        if not notice_file.exists():
            logger.warning(f"ê³µì§€ì‚¬í•­ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {notice_file}")
            return all_chunks
        
        try:
            logger.info(f"ğŸ¯ í”„ë¦¬ë¯¸ì—„ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì‹œì‘: {notice_file}")
            
            # íŒŒì¼ ì½ê¸° (ë‹¤ì¤‘ ì¸ì½”ë”© ì‹œë„)
            content = self._safe_file_read(notice_file)
            
            # ê³µì§€ì‚¬í•­ ì§€ëŠ¥í˜• íŒŒì‹±
            notices = self._premium_parse_notices(content)
            
            if not notices:
                logger.warning("íŒŒì‹±ëœ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
                return all_chunks
            
            logger.info(f"ğŸ“‹ ë°œê²¬ëœ ê³µì§€ì‚¬í•­: {len(notices)}ê°œ")
            
            # ê° ê³µì§€ì‚¬í•­ì„ VIP ëŒ€ìš°ë¡œ ì²˜ë¦¬
            for idx, notice in enumerate(notices):
                try:
                    logger.info(f"ğŸ’ ê³µì§€ì‚¬í•­ #{idx+1} VIP ì²˜ë¦¬ ì¤‘: {notice['title'][:30]}...")
                    
                    # í”„ë¦¬ë¯¸ì—„ ì²­í¬ ìƒì„±
                    chunk = self._create_premium_chunk(notice, idx + 1)
                    all_chunks.append(chunk)
                    
                    logger.info(f"âœ… ê³µì§€ì‚¬í•­ #{idx+1} ì™„ë²½ ì²˜ë¦¬ ì™„ë£Œ")
                    
                except Exception as e:
                    logger.error(f"âŒ ê³µì§€ì‚¬í•­ #{idx+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    # ì—ëŸ¬ ë°œìƒí•´ë„ ê¸°ë³¸ ì²˜ë¦¬ëŠ” ì‹œë„
                    try:
                        fallback_chunk = self._create_fallback_chunk(notice, idx + 1)
                        all_chunks.append(fallback_chunk)
                        logger.info(f"ğŸ”„ ê³µì§€ì‚¬í•­ #{idx+1} í´ë°± ì²˜ë¦¬ ì™„ë£Œ")
                    except:
                        logger.error(f"ğŸ’¥ ê³µì§€ì‚¬í•­ #{idx+1} ì™„ì „ ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
                        continue
            
            logger.info(f"ğŸ‰ í”„ë¦¬ë¯¸ì—„ ì²˜ë¦¬ ì™„ë£Œ: {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")
            
        except Exception as e:
            logger.error(f"ğŸ’¥ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            return all_chunks
        
        return all_chunks
    
    def _safe_file_read(self, file_path: Path) -> str:
        """ì•ˆì „í•œ íŒŒì¼ ì½ê¸° (ë‹¤ì¤‘ ì¸ì½”ë”© ì‹œë„)"""
        encodings = ['utf-8', 'cp949', 'euc-kr', 'utf-16']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    content = f.read()
                logger.info(f"âœ… íŒŒì¼ ì½ê¸° ì„±ê³µ (ì¸ì½”ë”©: {encoding})")
                return content
            except UnicodeDecodeError:
                continue
        
        # ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨ ì‹œ ë°”ì´ë„ˆë¦¬ë¡œ ì½ê³  ì—ëŸ¬ ë¬´ì‹œ
        with open(file_path, 'rb') as f:
            raw_content = f.read()
        content = raw_content.decode('utf-8', errors='ignore')
        logger.warning("âš ï¸ ë°”ì´ë„ˆë¦¬ ëª¨ë“œë¡œ íŒŒì¼ ì½ê¸° (ì¼ë¶€ ë¬¸ì ì†ì‹¤ ê°€ëŠ¥)")
        return content
    
    def _premium_parse_notices(self, content: str) -> List[Dict[str, Any]]:
        """
        í”„ë¦¬ë¯¸ì—„ ê³µì§€ì‚¬í•­ íŒŒì‹± - ë‹¤ì¤‘ ì „ëµ ì‚¬ìš©
        """
        notices = []
        
        # ì „ëµ 1: êµ¬ë¶„ì ê¸°ë°˜ ë¶„ë¦¬
        sections = self._split_by_separators(content)
        
        # ì „ëµ 2: íŒ¨í„´ ê¸°ë°˜ ë¶„ë¦¬ (êµ¬ë¶„ìê°€ ì—†ëŠ” ê²½ìš°)
        if len(sections) <= 1:
            sections = self._split_by_patterns(content)
        
        # ì „ëµ 3: ì˜ë¯¸ ê¸°ë°˜ ë¶„ë¦¬ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
        if len(sections) <= 1:
            sections = self._split_by_semantics(content)
        
        logger.info(f"ğŸ“ ë¶„ë¦¬ëœ ì„¹ì…˜ ìˆ˜: {len(sections)}")
        
        for idx, section in enumerate(sections):
            section = section.strip()
            if not section or len(section) < 10:  # ë„ˆë¬´ ì§§ì€ ì„¹ì…˜ ë¬´ì‹œ
                continue
            
            try:
                notice = self._premium_parse_single_notice(section, idx + 1)
                if notice:
                    notices.append(notice)
                    
            except Exception as e:
                logger.error(f"ê°œë³„ ê³µì§€ì‚¬í•­ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ (ì„¹ì…˜ {idx+1}): {e}")
                continue
        
        # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì •ë ¬
        notices.sort(key=lambda x: x.get('urgency_score', 0), reverse=True)
        
        return notices
    
    def _split_by_separators(self, content: str) -> List[str]:
        """êµ¬ë¶„ì ê¸°ë°˜ ë¶„ë¦¬"""
        separators = ['---', '===', '***', 'â”â”â”', 'â–ªâ–ªâ–ª']
        
        for sep in separators:
            if sep in content:
                sections = content.split(sep)
                if len(sections) > 1:
                    logger.info(f"âœ… êµ¬ë¶„ì '{sep}'ë¡œ {len(sections)}ê°œ ì„¹ì…˜ ë¶„ë¦¬")
                    return sections
        
        return [content]
    
    def _split_by_patterns(self, content: str) -> List[str]:
        """íŒ¨í„´ ê¸°ë°˜ ë¶„ë¦¬"""
        # íŒ¨í„´ 1: [ì œëª©] ë˜ëŠ” ì œëª©: í˜•íƒœ
        title_pattern = r'(?:\[.+?\]|ì œëª©\s*[:ï¼š].+?)(?=\n)'
        matches = list(re.finditer(title_pattern, content))
        
        if len(matches) >= 2:
            sections = []
            for i, match in enumerate(matches):
                start = match.start()
                end = matches[i + 1].start() if i + 1 < len(matches) else len(content)
                sections.append(content[start:end])
            logger.info(f"âœ… ì œëª© íŒ¨í„´ìœ¼ë¡œ {len(sections)}ê°œ ì„¹ì…˜ ë¶„ë¦¬")
            return sections
        
        # íŒ¨í„´ 2: URL ê¸°ë°˜ ë¶„ë¦¬
        url_pattern = r'https?://[^\s]+'
        urls = list(re.finditer(url_pattern, content))
        
        if len(urls) >= 2:
            sections = []
            for i, url_match in enumerate(urls):
                start = content.rfind('\n', 0, url_match.start()) + 1
                end = content.find('\n\n', url_match.end())
                if end == -1:
                    end = urls[i + 1].start() if i + 1 < len(urls) else len(content)
                sections.append(content[start:end])
            logger.info(f"âœ… URL íŒ¨í„´ìœ¼ë¡œ {len(sections)}ê°œ ì„¹ì…˜ ë¶„ë¦¬")
            return sections
        
        return [content]
    
    def _split_by_semantics(self, content: str) -> List[str]:
        """ì˜ë¯¸ ê¸°ë°˜ ë¶„ë¦¬ (íœ´ë¦¬ìŠ¤í‹±)"""
        lines = content.split('\n')
        sections = []
        current_section = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ ì‹œì‘ ì‹ í˜¸
            is_new_notice = (
                any(keyword in line for keyword in ['ê³µì§€', 'ì•ˆë‚´', 'ì•Œë¦¼']) and
                (line.startswith('[') or 'ì œëª©' in line or len(line) < 100)
            )
            
            if is_new_notice and current_section:
                sections.append('\n'.join(current_section))
                current_section = [line]
            else:
                current_section.append(line)
        
        if current_section:
            sections.append('\n'.join(current_section))
        
        logger.info(f"âœ… ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ {len(sections)}ê°œ ì„¹ì…˜ ë¶„ë¦¬")
        return sections
    
    def _premium_parse_single_notice(self, section: str, section_num: int) -> Optional[Dict[str, Any]]:
        """
        ê°œë³„ ê³µì§€ì‚¬í•­ í”„ë¦¬ë¯¸ì—„ íŒŒì‹±
        """
        lines = [line.strip() for line in section.split('\n') if line.strip()]
        
        if not lines:
            return None
        
        notice = {
            'title': '',
            'url': '',
            'attachments': [],
            'content': '',
            'dates': [],
            'contacts': [],
            'emails': [],
            'full_text': section,
            'urgency_score': 0,
            'situation_type': 'normal',
            'semantic_keywords': [],
            'section_number': section_num
        }
        
        # ğŸ¯ ë‹¤ì¤‘ ì „ëµ ì œëª© ì¶”ì¶œ
        notice['title'] = self._extract_premium_title(lines, section)
        
        # ğŸ¯ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
        notice['url'] = self._extract_urls(section)
        notice['attachments'] = self._extract_attachments(section)
        notice['dates'] = self._extract_premium_dates(section)
        notice['contacts'] = self._extract_contacts(section)
        notice['emails'] = self._extract_emails(section)
        
        # ğŸ¯ ë‚´ìš© ì •ì œ
        notice['content'] = self._extract_clean_content(section, notice)
        
        # ğŸ¯ ê³ ê¸‰ ë¶„ì„
        notice['urgency_score'] = self._calculate_urgency_score(notice)
        notice['situation_type'] = self._detect_situation_type(section)
        notice['semantic_keywords'] = self._extract_semantic_keywords(notice)
        
        return notice
    
    def _extract_premium_title(self, lines: List[str], full_text: str) -> str:
        """í”„ë¦¬ë¯¸ì—„ ì œëª© ì¶”ì¶œ - 10ê°€ì§€ ì „ëµ"""
        
        # ì „ëµ 1: [ì œëª©] í˜•íƒœ
        for line in lines[:3]:
            if line.startswith('[') and line.endswith(']'):
                return line.strip('[]').strip()
        
        # ì „ëµ 2: "ì œëª© : ë‚´ìš©" í˜•íƒœ
        title_match = re.search(r'ì œëª©\s*[:ï¼š]\s*([^/\n]{1,100})', full_text)
        if title_match:
            return title_match.group(1).strip()
        
        # ì „ëµ 3: "ì œëª©:" ë‹¤ìŒ ì¤„
        for i, line in enumerate(lines[:-1]):
            if 'ì œëª©' in line and ':' in line and len(lines) > i + 1:
                return lines[i + 1].strip()
        
        # ì „ëµ 4: ì²« ì¤„ì´ ì œëª©ì¼ ê°€ëŠ¥ì„± (íŠ¹ì • íŒ¨í„´)
        first_line = lines[0]
        if any(keyword in first_line for keyword in ['ê³µì§€', 'ì•ˆë‚´', 'ì•Œë¦¼', 'ë³€ê²½', 'ì·¨ì†Œ']):
            return first_line[:80] + ('...' if len(first_line) > 80 else '')
        
        # ì „ëµ 5: URL ë°”ë¡œ ìœ„ ì¤„
        url_pattern = r'https?://[^\s]+'
        url_match = re.search(url_pattern, full_text)
        if url_match:
            before_url = full_text[:url_match.start()].strip()
            last_line_before_url = before_url.split('\n')[-1].strip()
            if last_line_before_url and len(last_line_before_url) < 100:
                return last_line_before_url
        
        # ì „ëµ 6-10: ë” ë³µì¡í•œ íœ´ë¦¬ìŠ¤í‹±ë“¤...
        # (ìƒëµ - í•„ìš” ì‹œ ì¶”ê°€)
        
        # ìµœí›„ ìˆ˜ë‹¨: ì²« ì¤„ (ê¸¸ì´ ì œí•œ)
        title = first_line[:50]
        if len(first_line) > 50:
            title += "..."
        
        return title if title else "ì œëª© ì—†ìŒ"
    
    def _extract_urls(self, text: str) -> str:
        """URL ì¶”ì¶œ (ë‹¤ì¤‘ íŒ¨í„´)"""
        patterns = [
            r'https?://[^\s\n]+',
            r'www\.[^\s\n]+',
            r'[a-zA-Z0-9.-]+\.(?:com|kr|org|net|edu|gov)[^\s]*'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(0)
        
        return ''
    
    def _extract_attachments(self, text: str) -> List[Dict[str, str]]:
        """ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ (ì •êµí•œ íŒŒì‹±)"""
        attachments = []
        
        # íŒ¨í„´ 1: "ì²¨ë¶€íŒŒì¼:" ì„¹ì…˜
        attachment_section_match = re.search(r'ì²¨ë¶€íŒŒì¼[:ï¼š]?\s*\n((?:.*?(?:\.hwp|\.pdf|\.docx?).*?\n?)+)', text, re.MULTILINE)
        if attachment_section_match:
            section = attachment_section_match.group(1)
            file_matches = re.findall(r'([^(]+\.(?:hwp|pdf|docx?))\s*\(([^)]+)\)', section)
            for filename, size in file_matches:
                attachments.append({
                    'filename': filename.strip(),
                    'size': size.strip(),
                    'type': filename.split('.')[-1].lower()
                })
        
        # íŒ¨í„´ 2: ì¸ë¼ì¸ íŒŒì¼ëª…
        inline_files = re.findall(r'([ê°€-í£\w\s]+\.(?:hwp|pdf|docx?))\s*\(([^)]*(?:kb|mb|bytes?))\)', text, re.IGNORECASE)
        for filename, size in inline_files:
            if not any(att['filename'] == filename.strip() for att in attachments):
                attachments.append({
                    'filename': filename.strip(),
                    'size': size.strip(),
                    'type': filename.split('.')[-1].lower()
                })
        
        return attachments
    
    def _extract_premium_dates(self, text: str) -> List[Dict[str, Any]]:
        """í”„ë¦¬ë¯¸ì—„ ë‚ ì§œ ì¶”ì¶œ - ì™„ë²½í•œ íŒŒì‹±"""
        dates = []
        
        # ë‹¤ì–‘í•œ ë‚ ì§œ íŒ¨í„´ë“¤
        date_patterns = [
            (r'(\d{4})\s*[ë…„.]\s*(\d{1,2})\s*[ì›”.]\s*(\d{1,2})\s*ì¼?', 'full_date'),
            (r'(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼', 'month_day'),
            (r'(\d{1,2})/(\d{1,2})', 'slash_format'),
            (r'(\d{1,2})-(\d{1,2})', 'dash_format'),
            (r'([ê°€-í£])ìš”ì¼', 'weekday'),
            (r'ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ê¸€í”¼', 'relative_day'),
            (r'ì´ë²ˆ\s*ì£¼|ë‹¤ìŒ\s*ì£¼|ë‹¤ë‹¤ìŒ\s*ì£¼', 'relative_week'),
            (r'(\d{1,2})\s*ì‹œ\s*(\d{1,2})\s*ë¶„?', 'time'),
            (r'(\d{1,2}):(\d{2})', 'time_colon')
        ]
        
        context_keywords = [
            'ì œì¶œê¸°í•œ', 'ë§ˆê°', 'ì¼ì •', 'ê¸°ê°„', 'ì‹œê°„', 'ë‚ ì§œ',
            'ì‹œì‘', 'ì¢…ë£Œ', 'ê°œì‹œ', 'ì™„ë£Œ', 'ì ‘ìˆ˜', 'ì‹ ì²­'
        ]
        
        for keyword in context_keywords:
            # í‚¤ì›Œë“œ ì£¼ë³€ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì°¾ê¸°
            keyword_pattern = rf'{keyword}[^:ï¼š]*[:ï¼š]?\s*([^\n]{{1,50}})'
            keyword_matches = re.findall(keyword_pattern, text)
            
            for match_text in keyword_matches:
                for pattern, date_type in date_patterns:
                    pattern_matches = re.findall(pattern, match_text)
                    
                    for match in pattern_matches:
                        dates.append({
                            'type': keyword,
                            'date_type': date_type,
                            'raw_text': match_text.strip(),
                            'extracted': match,
                            'context': keyword
                        })
        
        return dates
    
    def _extract_contacts(self, text: str) -> List[str]:
        """ì—°ë½ì²˜ ì¶”ì¶œ"""
        patterns = [
            r'(\d{2,3}-\d{3,4}-\d{4})',  # ì „í™”ë²ˆí˜¸
            r'(\d{3,4}-\d{4})',          # ë‚´ì„ ë²ˆí˜¸
            r'ë‚´ì„ \s*(\d{3,4})',         # ë‚´ì„  í‘œê¸°
        ]
        
        contacts = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            contacts.extend(matches)
        
        return list(set(contacts))
    
    def _extract_emails(self, text: str) -> List[str]:
        """ì´ë©”ì¼ ì¶”ì¶œ"""
        email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
        emails = re.findall(email_pattern, text)
        return list(set(emails))
    
    def _extract_clean_content(self, text: str, notice: Dict) -> str:
        """ì •ì œëœ ë‚´ìš© ì¶”ì¶œ"""
        content = text
        
        # URL ì œê±°
        if notice['url']:
            content = content.replace(notice['url'], '')
        
        # ì²¨ë¶€íŒŒì¼ ì„¹ì…˜ ì œê±°
        content = re.sub(r'ì²¨ë¶€íŒŒì¼[:ï¼š]?\s*\n(?:.*?(?:\.hwp|\.pdf|\.docx?).*?\n?)+', '', content, flags=re.MULTILINE)
        
        # ì œëª© ì œê±° (ì¤‘ë³µ ë°©ì§€)
        if notice['title'] and notice['title'] != "ì œëª© ì—†ìŒ":
            content = content.replace(notice['title'], '')
            content = content.replace(f"[{notice['title']}]", '')
        
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        content = re.sub(r'\n\s*\n', '\n\n', content)
        content = content.strip()
        
        return content
    
    def _calculate_urgency_score(self, notice: Dict[str, Any]) -> int:
        """ê¸´ê¸‰ë„ ì ìˆ˜ ê³„ì‚° (0-100)"""
        score = 0
        full_text = (notice['title'] + ' ' + notice['content']).lower()
        
        # ê¸´ê¸‰ í‚¤ì›Œë“œ ì ìˆ˜
        for keyword, weight in self.urgency_keywords.items():
            if keyword in full_text:
                score += weight
        
        # ìƒí™©ë³„ ê°€ì¤‘ì¹˜
        situation_weights = {
            'disaster': 90, 'health': 80, 'facility': 60,
            'system': 50, 'admin': 30, 'normal': 10
        }
        score += situation_weights.get(notice['situation_type'], 10)
        
        # ì‹œê°„ ë¯¼ê°ì„±
        for date_info in notice['dates']:
            if any(keyword in date_info['type'] for keyword in ['ë§ˆê°', 'ì œì¶œê¸°í•œ']):
                score += 20
        
        # ì²¨ë¶€íŒŒì¼ ìˆìœ¼ë©´ ì¤‘ìš”ë„ ì¦ê°€
        if notice['attachments']:
            score += 15
        
        # URL ìˆìœ¼ë©´ ê³µì‹ì„± ì¦ê°€
        if notice['url']:
            score += 10
        
        return min(score, 100)  # ìµœëŒ€ 100ì 
    
    def _detect_situation_type(self, text: str) -> str:
        """ìƒí™© ìœ í˜• ê°ì§€"""
        text_lower = text.lower()
        
        for situation, patterns in self.situation_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text_lower):
                    return situation
        
        return 'normal'
    
    def _extract_semantic_keywords(self, notice: Dict[str, Any]) -> List[str]:
        """ì˜ë¯¸ë¡ ì  í‚¤ì›Œë“œ ì¶”ì¶œ - ëª¨ë“  ê¸°ë²• ì´ë™ì›"""
        keywords = set()
        full_text = notice['title'] + ' ' + notice['content']
        
        # ğŸ¯ ë°©ë²• 1: ë¹ˆë„ ê¸°ë°˜ ì¤‘ìš” ë‹¨ì–´
        keywords.update(self._extract_frequency_keywords(full_text))
        
        # ğŸ¯ ë°©ë²• 2: íŒ¨í„´ ê¸°ë°˜ í‚¤ì›Œë“œ
        keywords.update(self._extract_pattern_keywords(full_text))
        
        # ğŸ¯ ë°©ë²• 3: ë¬¸ë²• ê¸°ë°˜ í‚¤ì›Œë“œ
        keywords.update(self._extract_grammar_keywords(full_text))
        
        # ğŸ¯ ë°©ë²• 4: ë™ì˜ì–´ í™•ì¥
        keywords.update(self._expand_synonyms(keywords))
        
        # ğŸ¯ ë°©ë²• 5: ì˜¤íƒ€ ë³€í˜• ì¶”ê°€
        keywords.update(self._generate_typo_variants(keywords))
        
        # ğŸ¯ ë°©ë²• 6: ìƒí™©ë³„ íŠ¹í™” í‚¤ì›Œë“œ
        keywords.update(self._extract_situation_keywords(notice))
        
        # ğŸ¯ ë°©ë²• 7: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í‚¤ì›Œë“œ
        keywords.update(self._extract_metadata_keywords(notice))
        
        # í•„í„°ë§ ë° ì •ì œ
        filtered_keywords = self._premium_filter_keywords(keywords)
        
        return sorted(list(filtered_keywords))
    
    def _extract_frequency_keywords(self, text: str) -> Set[str]:
        """ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ"""
        # í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ (1ê¸€ì ì´ìƒ)
        words = re.findall(r'[ê°€-í£]+', text)
        
        # ë¹ˆë„ ê³„ì‚°
        word_freq = Counter(words)
        
        # ìƒìœ„ ë¹ˆë„ ë‹¨ì–´ë“¤
        top_words = [word for word, freq in word_freq.most_common(10) if len(word) >= 2]
        
        return set(top_words)
    
    def _extract_pattern_keywords(self, text: str) -> Set[str]:
        """íŒ¨í„´ ê¸°ë°˜ í‚¤ì›Œë“œ"""
        keywords = set()
        
        patterns = [
            (r'([ê°€-í£\s]+)(?:ë¡œ|ìœ¼ë¡œ)\s*ì¸í•œ', 'ì›ì¸'),
            (r'([ê°€-í£\s]+)\s*ê´€ë ¨', 'ê´€ë ¨ì‚¬í•­'),
            (r'([ê°€-í£\s]+)\s*ì•ˆë‚´', 'ì•ˆë‚´ëŒ€ìƒ'),
            (r'([ê°€-í£\s]+)\s*ë³€ê²½', 'ë³€ê²½ëŒ€ìƒ'),
            (r'([ê°€-í£\s]+)\s*ì·¨ì†Œ', 'ì·¨ì†ŒëŒ€ìƒ'),
            (r'([ê°€-í£\s]+)\s*ê³¼ì •', 'êµìœ¡ê³¼ì •'),
            (r'([ê°€-í£\s]+)\s*êµìœ¡', 'êµìœ¡ìœ í˜•'),
        ]
        
        for pattern, category in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                clean_match = match.strip()
                if len(clean_match) >= 2:
                    keywords.add(clean_match)
        
        return keywords
    
    def _extract_grammar_keywords(self, text: str) -> Set[str]:
        """ë¬¸ë²• ê¸°ë°˜ í‚¤ì›Œë“œ"""
        keywords = set()
        
        # ëª…ì‚¬ ì–´ë¯¸ íŒ¨í„´
        noun_patterns = [
            r'([ê°€-í£]+)(?:ê³¼ì •|êµìœ¡|í›ˆë ¨|ê°•ì˜|ìˆ˜ì—…)',
            r'([ê°€-í£]+)(?:ì„¼í„°|ì›|ê´€|ì‹¤|ì‹¤)',
            r'([ê°€-í£]+)(?:ì‹œì„¤|ê±´ë¬¼|ì¥ì†Œ)',
            r'([ê°€-í£]+)(?:ì‹œìŠ¤í…œ|í”„ë¡œê·¸ë¨|ì„œë¹„ìŠ¤)',
            r'([ê°€-í£]+)(?:ì•ˆë‚´|ê³µì§€|ì•Œë¦¼)',
        ]
        
        for pattern in noun_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                if len(match) >= 1:
                    keywords.add(match)
        
        return keywords
    
    def _expand_synonyms(self, keywords: Set[str]) -> Set[str]:
        """ë™ì˜ì–´ í™•ì¥"""
        expanded = set(keywords)
        
        for keyword in keywords:
            if keyword in self.synonym_dict:
                expanded.update(self.synonym_dict[keyword])
        
        return expanded
    
    def _generate_typo_variants(self, keywords: Set[str]) -> Set[str]:
        """ì˜¤íƒ€ ë³€í˜• ìƒì„±"""
        variants = set()
        
        # ê¸°ì¡´ ì˜¤íƒ€ ì‚¬ì „
        for original, typo in self.typo_corrections.items():
            if typo in keywords:
                variants.add(original)
        
        # ìë™ ì˜¤íƒ€ ìƒì„± (ê°„ë‹¨í•œ ë²„ì „)
        for keyword in keywords:
            if len(keyword) >= 3:
                # ìëª¨ ì¹˜í™˜ ì˜¤íƒ€ (ã…—/ã…œ, ã…“/ã…• ë“±)
                typo_variants = self._generate_simple_typos(keyword)
                variants.update(typo_variants)
        
        return variants
    
    def _generate_simple_typos(self, word: str) -> List[str]:
        """ê°„ë‹¨í•œ ì˜¤íƒ€ ìƒì„±"""
        typos = []
        
        # í”í•œ ì˜¤íƒ€ íŒ¨í„´
        typo_map = {
            'ã…—': 'ã…œ', 'ã…œ': 'ã…—',
            'ã…“': 'ã…•', 'ã…•': 'ã…“',
            'ã…': 'ã…‘', 'ã…‘': 'ã…',
        }
        
        # ë‹¨ìˆœí™”: ë§ˆì§€ë§‰ ê¸€ìë§Œ ë³€í˜•
        if len(word) >= 2:
            for original, replacement in typo_map.items():
                if original in word:
                    typo = word.replace(original, replacement)
                    if typo != word:
                        typos.append(typo)
        
        return typos[:2]  # ìµœëŒ€ 2ê°œê¹Œì§€
    
    def _extract_situation_keywords(self, notice: Dict[str, Any]) -> Set[str]:
        """ìƒí™©ë³„ íŠ¹í™” í‚¤ì›Œë“œ"""
        keywords = set()
        situation = notice['situation_type']
        
        situation_specific = {
            'disaster': ['ì¬í•´', 'ì¬ë‚œ', 'ì‘ê¸‰', 'ëŒ€í”¼', 'ì•ˆì „', 'ìœ„í—˜'],
            'health': ['ê±´ê°•', 'ë°©ì—­', 'ê°ì—¼', 'ì˜ˆë°©', 'ê²©ë¦¬', 'í™•ì§„'],
            'facility': ['ì‹œì„¤', 'ê³µì‚¬', 'ë³´ìˆ˜', 'ì ê²€', 'ì •ë¹„', 'ìˆ˜ë¦¬'],
            'system': ['ì‹œìŠ¤í…œ', 'ì „ì‚°', 'ë„¤íŠ¸ì›Œí¬', 'ì„œë²„', 'ì¥ì• ', 'ë³µêµ¬'],
            'admin': ['í–‰ì •', 'ì‹ ì²­', 'ì ‘ìˆ˜', 'ì„œë¥˜', 'ì œì¶œ', 'ì²˜ë¦¬']
        }
        
        if situation in situation_specific:
            keywords.update(situation_specific[situation])
        
        return keywords
    
    def _extract_metadata_keywords(self, notice: Dict[str, Any]) -> Set[str]:
        """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í‚¤ì›Œë“œ"""
        keywords = set()
        
        # ë‚ ì§œ ì •ë³´ì—ì„œ í‚¤ì›Œë“œ
        for date_info in notice['dates']:
            keywords.add(date_info['type'])  # 'ì œì¶œê¸°í•œ', 'ë§ˆê°' ë“±
            keywords.add(date_info['context'])  # ì»¨í…ìŠ¤íŠ¸
        
        # ì²¨ë¶€íŒŒì¼ì—ì„œ í‚¤ì›Œë“œ
        for attachment in notice['attachments']:
            filename = attachment['filename']
            # íŒŒì¼ëª…ì—ì„œ ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ì¶”ì¶œ
            file_words = re.findall(r'[ê°€-í£]+', filename)
            keywords.update([word for word in file_words if len(word) >= 2])
        
        # URLì—ì„œ í‚¤ì›Œë“œ
        if notice['url']:
            try:
                parsed = urlparse(notice['url'])
                if 'board' in parsed.path:
                    keywords.add('ê²Œì‹œíŒ')
                if 'notice' in parsed.path:
                    keywords.add('ê³µì§€')
            except:
                pass
        
        return keywords
    
    def _premium_filter_keywords(self, keywords: Set[str]) -> Set[str]:
        """í”„ë¦¬ë¯¸ì—„ í‚¤ì›Œë“œ í•„í„°ë§"""
        # ë¶ˆìš©ì–´ (ìµœì†Œí•œìœ¼ë¡œ ìœ ì§€)
        stopwords = {
            'ì…ë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'í–ˆìŠµë‹ˆë‹¤', 'ë©ë‹ˆë‹¤',
            'í•˜ì‹œê¸°', 'ë°”ëë‹ˆë‹¤', 'í•´ì£¼ì„¸ìš”', 'ë¶€íƒë“œë¦½ë‹ˆë‹¤',
            'ë•Œë¬¸', 'ì¸í•´', 'ìœ¼ë¡œ', 'ì—ì„œ', 'ì—ê²Œ', 'ê»˜ì„œ',
            'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ì™€', 'ê³¼', 'ì˜', 'ë¥¼', 'ì„'
        }
        
        filtered = set()
        
        for keyword in keywords:
            if (len(keyword) >= 2 and 
                keyword not in stopwords and
                not keyword.isdigit() and
                not re.match(r'^[a-zA-Z]+, keyword)):  # ìˆœìˆ˜ ì˜ë¬¸ ì œì™¸
                filtered.add(keyword)
        
        return filtered
    
    def _create_premium_chunk(self, notice: Dict[str, Any], notice_number: int) -> TextChunk:
        """
        í”„ë¦¬ë¯¸ì—„ TextChunk ìƒì„± - ì™„ë²½í•œ ë©”íƒ€ë°ì´í„°
        """
        # í”„ë¦¬ë¯¸ì—„ í…œí”Œë¦¿ ë³€ìˆ˜
        template_vars = {
            'title': notice['title'],
            'notice_number': notice_number,
            'urgency_score': notice['urgency_score'],
            'situation_type': notice['situation_type'],
            'url': notice['url'] if notice['url'] else 'ì—†ìŒ',
            'attachments': self._format_premium_attachments(notice['attachments']),
            'content': notice['content'],
            'dates': self._format_premium_dates(notice['dates']),
            'contacts': ', '.join(notice['contacts']) if notice['contacts'] else 'ì—†ìŒ',
            'emails': ', '.join(notice['emails']) if notice['emails'] else 'ì—†ìŒ',
            'keywords': ', '.join(notice['semantic_keywords'][:20]),
            'generation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # í”„ë¦¬ë¯¸ì—„ í…œí”Œë¦¿ ì ìš©
        enhanced_text = self.template.format(**template_vars)
        
        # ì™„ë²½í•œ ë©”íƒ€ë°ì´í„°
        metadata = {
            'source_file': 'notice.txt',
            'notice_number': notice_number,
            'section_number': notice['section_number'],
            'notice_title': notice['title'],
            'notice_url': notice['url'],
            'urgency_score': notice['urgency_score'],
            'situation_type': notice['situation_type'],
            
            # ì²¨ë¶€íŒŒì¼ ì •ë³´
            'has_attachments': len(notice['attachments']) > 0,
            'attachment_count': len(notice['attachments']),
            'attachment_types': list(set([att['type'] for att in notice['attachments']])),
            
            # ë‚ ì§œ ì •ë³´
            'has_dates': len(notice['dates']) > 0,
            'date_count': len(notice['dates']),
            'date_types': list(set([date['type'] for date in notice['dates']])),
            
            # ì—°ë½ ì •ë³´
            'has_contacts': len(notice['contacts']) > 0,
            'has_emails': len(notice['emails']) > 0,
            
            # í‚¤ì›Œë“œ ì •ë³´
            'primary_keywords': notice['semantic_keywords'][:10],
            'all_keywords': notice['semantic_keywords'],
            'keyword_count': len(notice['semantic_keywords']),
            
            # ê²€ìƒ‰ ìµœì í™”
            'search_keywords': notice['semantic_keywords'] + [notice['title']],
            'synonyms': self._get_keyword_synonyms(notice['semantic_keywords']),
            'typo_variants': self._get_keyword_typos(notice['semantic_keywords']),
            
            # ë¬¸ì„œ ì •ë³´
            'document_type': 'ê³µì§€ì‚¬í•­',
            'document_category': 'notice',
            'processing_date': datetime.now().isoformat(),
            'quality_level': 'premium'
        }
        
        # URL ë©”íƒ€ë°ì´í„°
        if notice['url']:
            try:
                parsed_url = urlparse(notice['url'])
                metadata['url_domain'] = parsed_url.netloc
                metadata['url_path'] = parsed_url.path
            except:
                metadata['url_domain'] = ''
                metadata['url_path'] = ''
        
        return TextChunk(text=enhanced_text, metadata=metadata)
    
    def _create_fallback_chunk(self, notice: Dict[str, Any], notice_number: int) -> TextChunk:
        """í´ë°± ì²­í¬ ìƒì„± (ì—ëŸ¬ ë°œìƒ ì‹œ)"""
        simple_text = f"""
[ê³µì§€ì‚¬í•­ #{notice_number}] {notice.get('title', 'ì œëª© ì—†ìŒ')}

{notice.get('content', notice.get('full_text', ''))}

[ì¶œì²˜] notice.txt (í´ë°± ì²˜ë¦¬)
        """.strip()
        
        simple_metadata = {
            'source_file': 'notice.txt',
            'notice_number': notice_number,
            'notice_title': notice.get('title', 'ì œëª© ì—†ìŒ'),
            'document_type': 'ê³µì§€ì‚¬í•­',
            'document_category': 'notice',
            'processing_date': datetime.now().isoformat(),
            'quality_level': 'fallback'
        }
        
        return TextChunk(text=simple_text, metadata=simple_metadata)
    
    def _format_premium_attachments(self, attachments: List[Dict[str, str]]) -> str:
        """í”„ë¦¬ë¯¸ì—„ ì²¨ë¶€íŒŒì¼ í¬ë§·íŒ…"""
        if not attachments:
            return 'ì—†ìŒ'
        
        formatted = []
        for att in attachments:
            line = f"ğŸ“ {att['filename']}"
            if att['size']:
                line += f" ({att['size']})"
            if att['type']:
                line += f" [{att['type'].upper()}]"
            formatted.append(line)
        
        return '\n'.join(formatted)
    
    def _format_premium_dates(self, dates: List[Dict[str, Any]]) -> str:
        """í”„ë¦¬ë¯¸ì—„ ë‚ ì§œ í¬ë§·íŒ…"""
        if not dates:
            return 'ì—†ìŒ'
        
        formatted = []
        for date_info in dates:
            line = f"ğŸ“… {date_info['type']}: {date_info['raw_text']}"
            formatted.append(line)
        
        return '\n'.join(formatted)
    
    def _get_keyword_synonyms(self, keywords: List[str]) -> List[str]:
        """í‚¤ì›Œë“œ ë™ì˜ì–´ ëª©ë¡"""
        synonyms = []
        for keyword in keywords:
            if keyword in self.synonym_dict:
                synonyms.extend(self.synonym_dict[keyword])
        return list(set(synonyms))
    
    def _get_keyword_typos(self, keywords: List[str]) -> List[str]:
        """í‚¤ì›Œë“œ ì˜¤íƒ€ ë³€í˜• ëª©ë¡"""
        typos = []
        for keyword in keywords:
            if keyword in self.typo_corrections.values():
                # ì—­ë°©í–¥ ê²€ìƒ‰
                for typo, correct in self.typo_corrections.items():
                    if correct == keyword:
                        typos.append(typo)
        return list(set(typos))
    
    def _get_premium_template(self) -> str:
        """í”„ë¦¬ë¯¸ì—„ ê³µì§€ì‚¬í•­ í…œí”Œë¦¿"""
        return """
ğŸ”” [ê³µì§€ì‚¬í•­ #{notice_number}] {title}

â”Œâ”€ ê¸°ë³¸ ì •ë³´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ¯ ê¸´ê¸‰ë„: {urgency_score}/100ì  ({situation_type})    â”‚
â”‚ ğŸ”— ë§í¬: {url}                                      â”‚
â”‚ ğŸ“ ì—°ë½ì²˜: {contacts}                               â”‚
â”‚ ğŸ“§ ì´ë©”ì¼: {emails}                                 â”‚
â”‚ ğŸ•’ ì²˜ë¦¬ì‹œê°„: {generation_date}                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ ì²¨ë¶€íŒŒì¼
{attachments}

ğŸ“… ì¤‘ìš” ì¼ì •
{dates}

ğŸ“ ê³µì§€ ë‚´ìš©
{content}

ğŸ·ï¸ ê²€ìƒ‰ í‚¤ì›Œë“œ
{keywords}

ğŸ” ê²€ìƒ‰ ìµœì í™”
ì´ ê³µì§€ì‚¬í•­ì€ ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì›ì˜ ì¤‘ìš”í•œ ì•ˆë‚´ì‚¬í•­ì…ë‹ˆë‹¤.
ê¸´ê¸‰ë„ {urgency_score}ì , ìƒí™©ìœ í˜• {situation_type}ë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤.
êµìœ¡ìƒ, ì§ì›, ê´€ë ¨ìëŠ” ë°˜ë“œì‹œ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

ğŸ“ ì¶œì²˜: notice.txt (ê³µì§€ì‚¬í•­ #{notice_number}) | í”„ë¦¬ë¯¸ì—„ ì²˜ë¦¬
        """.strip()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - í”„ë¦¬ë¯¸ì—„ ì²˜ë¦¬"""
    try:
        logger.info("ğŸš€ === í”„ë¦¬ë¯¸ì—„ ê³µì§€ì‚¬í•­ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì‹œì‘ ===")
        
        # í”„ë¦¬ë¯¸ì—„ ë¡œë” ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
        loader = PremiumNoticeLoader()
        success = loader.build_vectorstore()
        
        if success:
            logger.info("ğŸ‰ === í”„ë¦¬ë¯¸ì—„ ê³µì§€ì‚¬í•­ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ ===")
        else:
            logger.error("ğŸ’¥ === í”„ë¦¬ë¯¸ì—„ ê³µì§€ì‚¬í•­ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì‹¤íŒ¨ ===")
            
    except Exception as e:
        logger.error(f"ğŸ’¥ í”„ë¦¬ë¯¸ì—„ ë¡œë” ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        raise


if __name__ == "__main__":
    main()
