#!/usr/bin/env python3
"""
ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› RAG ì±—ë´‡ - ê°œì„ ëœ ìŠ¤ë§ˆíŠ¸ ê³µì§€ì‚¬í•­ ë¡œë” (BaseLoader íŒ¨í„´ ì ìš©)

ì£¼ìš” ê°œì„ ì‚¬í•­:
- BaseLoader í‘œì¤€ íŒ¨í„´ ì¤€ìˆ˜
- í”ŒëŸ¬ê·¸ì¸ ê¸°ë°˜ ë™ì  íŒŒì‹± ì‹œìŠ¤í…œ ìœ ì§€
- ì„¤ì • íŒŒì¼ ê²½ë¡œ í‘œì¤€í™” (/schemas)
- ìºì‹œ TTL 6ì‹œê°„ ì ìš©
- í•´ì‹œ ê¸°ë°˜ ì¦ë¶„ ë¹Œë“œ ì§€ì›
- Citation ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬ ì˜¤ë¥˜ ìˆ˜ì • (context ê¸¸ì´ ì œí•œ)
"""

import logging
import re
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from abc import ABC, abstractmethod

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from modules.base_loader import BaseLoader
from utils.textifier import TextChunk
from utils.config import config

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ================================================================
# 1. ë™ì  íŒŒì‹±ì„ ìœ„í•œ í”ŒëŸ¬ê·¸ì¸ êµ¬ì¡° (ê°œì„ ë¨)
# ================================================================

class NoticeParser(ABC):
    """
    ê³µì§€ì‚¬í•­ íŒŒì„œì˜ ê¸°ë³¸ ì¶”ìƒ í´ë˜ìŠ¤.
    ëª¨ë“  íŒŒì„œëŠ” ì´ í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ ìë™ ë“±ë¡ë©ë‹ˆë‹¤.
    """
    _registry = {}

    def __init_subclass__(cls, **kwargs):
        super().__init_subclass__(**kwargs)
        if hasattr(cls, 'TOPIC_TYPE') and cls.TOPIC_TYPE:
            NoticeParser._registry[cls.TOPIC_TYPE] = cls

    @abstractmethod
    def can_parse(self, title: str, text: str, patterns: Dict[str, Any]) -> bool:
        """ì´ íŒŒì„œê°€ í•´ë‹¹ ê³µì§€ì‚¬í•­ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨"""
        pass

    @abstractmethod
    def parse(self, notice_text: str, patterns: Dict[str, Any]) -> Dict[str, Any]:
        """ê³µì§€ì‚¬í•­ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ë°˜í™˜"""
        pass
    
    @abstractmethod
    def create_chunks(self, parsed_notice: Dict[str, Any], notice_number: int) -> List[TextChunk]:
        """íŒŒì‹±ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ RAGìš© ì²­í¬ë¥¼ ìƒì„±"""
        pass

# ================================================================
# 2. êµ¬ì²´ì ì¸ íŒŒì„œ êµ¬í˜„ (TTL ìºì‹œ ë©”íƒ€ë°ì´í„° í¬í•¨)
# ================================================================

class EvaluationNoticeParser(NoticeParser):
    """í‰ê°€ ê´€ë ¨ ê³µì§€ì‚¬í•­ ì „ë¬¸ íŒŒì„œ"""
    TOPIC_TYPE = "evaluation"
    
    def can_parse(self, title: str, text: str, patterns: Dict[str, Any]) -> bool:
        keywords = patterns.get('topic_patterns', {}).get(self.TOPIC_TYPE, {}).get('keywords', [])
        combined_text = (title + " " + text).lower()
        return any(keyword.lower() in combined_text for keyword in keywords)

    def parse(self, notice_text: str, patterns: Dict[str, Any]) -> Dict[str, Any]:
        """í‰ê°€ ê´€ë ¨ ì£¼ìš” ì •ë³´(ë§ˆê°ê¸°í•œ, ì ìˆ˜, ì œì¶œë°©ë²•) ì¶”ì¶œ"""
        parsed = {}
        
        # ë§ˆê°ê¸°í•œ ì¶”ì¶œ (ë‹¤ì–‘í•œ íŒ¨í„´ ì§€ì›)
        deadline_patterns = [
            r'ì œì¶œê¸°í•œ\s*[:ï¼š]\s*([^\n]+)',
            r'ë§ˆê°ì¼\s*[:ï¼š]\s*([^\n]+)',
            r'(?:ê¹Œì§€|ì´ë‚´)\s*ì œì¶œ'
        ]
        for pattern in deadline_patterns:
            match = re.search(pattern, notice_text)
            if match:
                parsed['deadline'] = match.group(1).strip()
                break
        
        # ì ìˆ˜ ì •ë³´ ì¶”ì¶œ
        score_match = re.search(r'(\d+)\s*ì \s*ë§Œì ', notice_text)
        if score_match:
            parsed['max_score'] = int(score_match.group(1))
        
        # ì œì¶œë°©ë²• ì¶”ì¶œ
        submit_method = re.search(r'ì œì¶œë°©ë²•\s*[:ï¼š]\s*([^\n]+)', notice_text)
        if submit_method:
            parsed['submit_method'] = submit_method.group(1).strip()
        
        return parsed
        
    def create_chunks(self, parsed_notice: Dict[str, Any], notice_number: int) -> List[TextChunk]:
        """í‰ê°€ ê³µì§€ì‚¬í•­ ì²­í¬ ìƒì„± (ìºì‹œ TTL 6ì‹œê°„ ì ìš©)"""
        title = parsed_notice.get('title', 'ì œëª© ì—†ìŒ')
        full_text = parsed_notice.get('full_text', '')
        
        # Pydantic Citation ëª¨ë¸ì˜ context í•„ë“œ ìœ íš¨ì„± ê²€ì‚¬ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ 
        # contentì˜ ê¸¸ì´ë¥¼ 200ìë¡œ ì œí•œí•©ë‹ˆë‹¤.
        truncated_content = full_text[:200] + '...' if len(full_text) > 200 else full_text
        
        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° (ìºì‹œ TTL 6ì‹œê°„)
        base_metadata = {
            'source_file': 'notice.txt',
            'notice_number': notice_number,
            'notice_title': title,
            'topic_type': self.TOPIC_TYPE,
            'cache_ttl': 21600,  # 6ì‹œê°„ TTL
            'processing_date': datetime.now().isoformat(),
            'source_id': f'notice/notice.txt#section_{notice_number}',
            # âœ… context ê¸¸ì´ ì œí•œ ë¡œì§ì„ ì ìš©í•œ content í•„ë“œ
            'content': truncated_content
        }
        
        chunks = []
        
        # 1. ë©”ì¸ ìš”ì•½ ì²­í¬
        deadline_info = f"ë§ˆê°ê¸°í•œ: {parsed_notice.get('deadline', 'ë³„ë„ ëª…ì‹œ ì—†ìŒ')}"
        score_info = f"ì ìˆ˜: {parsed_notice.get('max_score', 'ë¯¸ëª…ì‹œ')}ì " if parsed_notice.get('max_score') else ""
        
        summary = f"[{title}] í‰ê°€ ê´€ë ¨ ê³µì§€ì‚¬í•­ì…ë‹ˆë‹¤. {deadline_info}. {score_info}"
        
        main_chunk = TextChunk(
            text=summary.strip(),
            source_id=base_metadata['source_id'],
            metadata={**base_metadata, 'chunk_type': 'summary', 'priority': 'high'}
        )
        chunks.append(main_chunk)
        
        # 2. ì„¸ë¶€ ì •ë³´ ì²­í¬
        if parsed_notice.get('submit_method'):
            detail_chunk = TextChunk(
                text=f"[{title} - ì œì¶œë°©ë²•]\n\n{parsed_notice.get('submit_method')}",
                source_id=base_metadata['source_id'],
                metadata={**base_metadata, 'chunk_type': 'details'}
            )
            chunks.append(detail_chunk)
        
        # 3. ì›ë¬¸ ì „ì²´ ì²­í¬
        full_chunk = TextChunk(
            text=f"[{title} - ì›ë¬¸]\n\n{full_text}",
            source_id=base_metadata['source_id'],
            metadata={**base_metadata, 'chunk_type': 'full_text'}
        )
        chunks.append(full_chunk)
        
        return chunks


class EnrollmentNoticeParser(NoticeParser):
    """ì…êµ ì¤€ë¹„ì‚¬í•­ ê´€ë ¨ ê³µì§€ì‚¬í•­ ì „ë¬¸ íŒŒì„œ"""
    TOPIC_TYPE = "enrollment"

    def can_parse(self, title: str, text: str, patterns: Dict[str, Any]) -> bool:
        keywords = patterns.get('topic_patterns', {}).get(self.TOPIC_TYPE, {}).get('keywords', [])
        combined_text = (title + " " + text).lower()
        return any(keyword.lower() in combined_text for keyword in keywords)

    def parse(self, notice_text: str, patterns: Dict[str, Any]) -> Dict[str, Any]:
        """ì²´í¬ë¦¬ìŠ¤íŠ¸ì™€ ì¤€ë¹„ë¬¼ ëª©ë¡ ì¶”ì¶œ"""
        parsed = {}
        
        # ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª© ì¶”ì¶œ (ì—¬ëŸ¬ í˜•ì‹ ì§€ì›)
        checklist_patterns = [
            r'(?:\d+\.|\-|â—‹|â—|â–ª)\s*([^\n]+)',
            r'(?:ì¤€ë¹„ë¬¼|ì§€ì°¸ë¬¼)\s*[:ï¼š]\s*([^\n]+)'
        ]
        
        checklist_items = []
        for pattern in checklist_patterns:
            items = re.findall(pattern, notice_text)
            checklist_items.extend([item.strip() for item in items if item.strip()])
        
        if checklist_items:
            parsed['checklist'] = list(set(checklist_items))  # ì¤‘ë³µ ì œê±°
        
        # ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ
        contact_match = re.search(r'ë¬¸ì˜\s*[:ï¼š]\s*([^\n]+)', notice_text)
        if contact_match:
            parsed['contact'] = contact_match.group(1).strip()
        
        return parsed

    def create_chunks(self, parsed_notice: Dict[str, Any], notice_number: int) -> List[TextChunk]:
        """ì…êµ ê³µì§€ì‚¬í•­ ì²­í¬ ìƒì„±"""
        title = parsed_notice.get('title', 'ì œëª© ì—†ìŒ')
        full_text = parsed_notice.get('full_text', '')
        
        # Pydantic Citation ëª¨ë¸ì˜ context í•„ë“œ ìœ íš¨ì„± ê²€ì‚¬ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ 
        # contentì˜ ê¸¸ì´ë¥¼ 200ìë¡œ ì œí•œí•©ë‹ˆë‹¤.
        truncated_content = full_text[:200] + '...' if len(full_text) > 200 else full_text
        
        base_metadata = {
            'source_file': 'notice.txt',
            'notice_number': notice_number,
            'notice_title': title,
            'topic_type': self.TOPIC_TYPE,
            'cache_ttl': 21600,  # 6ì‹œê°„ TTL
            'processing_date': datetime.now().isoformat(),
            'source_id': f'notice/notice.txt#section_{notice_number}',
            # âœ… context ê¸¸ì´ ì œí•œ ë¡œì§ì„ ì ìš©í•œ content í•„ë“œ
            'content': truncated_content
        }
        
        chunks = []
        
        # 1. ë©”ì¸ ìš”ì•½ ì²­í¬
        summary = f"[{title}] ì…êµ ì¤€ë¹„ì‚¬í•­ì— ëŒ€í•œ ì•ˆë‚´ì…ë‹ˆë‹¤. ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        main_chunk = TextChunk(
            text=summary,
            metadata={**base_metadata, 'chunk_type': 'summary', 'priority': 'high'}
        )
        chunks.append(main_chunk)

        # 2. ì²´í¬ë¦¬ìŠ¤íŠ¸ ì „ìš© ì²­í¬
        if parsed_notice.get('checklist'):
            checklist_text = "\n".join(f"â€¢ {item}" for item in parsed_notice.get('checklist', []))
            checklist_chunk = TextChunk(
                text=f"[{title} - ì¤€ë¹„ì‚¬í•­ ì²´í¬ë¦¬ìŠ¤íŠ¸]\n\n{checklist_text}",
                metadata={**base_metadata, 'chunk_type': 'checklist'}
            )
            chunks.append(checklist_chunk)

        # 3. ì—°ë½ì²˜ ì •ë³´ ì²­í¬
        if parsed_notice.get('contact'):
            contact_chunk = TextChunk(
                text=f"[{title} - ë¬¸ì˜ì²˜]\n\n{parsed_notice.get('contact')}",
                metadata={**base_metadata, 'chunk_type': 'contact'}
            )
            chunks.append(contact_chunk)

        return chunks


class RecruitmentNoticeParser(NoticeParser):
    """ëª¨ì§‘ ê³µê³  ê´€ë ¨ ì „ë¬¸ íŒŒì„œ"""
    TOPIC_TYPE = "recruitment"

    def can_parse(self, title: str, text: str, patterns: Dict[str, Any]) -> bool:
        keywords = patterns.get('topic_patterns', {}).get(self.TOPIC_TYPE, {}).get('keywords', ['ëª¨ì§‘', 'ì‹ ì²­', 'ì ‘ìˆ˜'])
        combined_text = (title + " " + text).lower()
        return any(keyword.lower() in combined_text for keyword in keywords)

    def parse(self, notice_text: str, patterns: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ì§‘ ê¸°ê°„, ëŒ€ìƒ, ë°©ë²• ì¶”ì¶œ"""
        parsed = {}
        
        # ëª¨ì§‘ê¸°ê°„ ì¶”ì¶œ
        period_match = re.search(r'(?:ëª¨ì§‘ê¸°ê°„|ì‹ ì²­ê¸°ê°„)\s*[:ï¼š]\s*([^\n]+)', notice_text)
        if period_match:
            parsed['recruitment_period'] = period_match.group(1).strip()
        
        # ëª¨ì§‘ëŒ€ìƒ ì¶”ì¶œ
        target_match = re.search(r'(?:ëª¨ì§‘ëŒ€ìƒ|ì‹ ì²­ëŒ€ìƒ)\s*[:ï¼š]\s*([^\n]+)', notice_text)
        if target_match:
            parsed['target'] = target_match.group(1).strip()
        
        return parsed

    def create_chunks(self, parsed_notice: Dict[str, Any], notice_number: int) -> List[TextChunk]:
        """ëª¨ì§‘ ê³µê³  ì²­í¬ ìƒì„±"""
        title = parsed_notice.get('title', 'ì œëª© ì—†ìŒ')
        full_text = parsed_notice.get('full_text', '')
        
        # Pydantic Citation ëª¨ë¸ì˜ context í•„ë“œ ìœ íš¨ì„± ê²€ì‚¬ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ 
        # contentì˜ ê¸¸ì´ë¥¼ 200ìë¡œ ì œí•œí•©ë‹ˆë‹¤.
        truncated_content = full_text[:200] + '...' if len(full_text) > 200 else full_text
        
        base_metadata = {
            'source_file': 'notice.txt',
            'notice_number': notice_number,
            'notice_title': title,
            'topic_type': self.TOPIC_TYPE,
            'cache_ttl': 21600,  # 6ì‹œê°„ TTL
            'processing_date': datetime.now().isoformat(),
            'source_id': f'notice/notice.txt#section_{notice_number}',
            # âœ… context ê¸¸ì´ ì œí•œ ë¡œì§ì„ ì ìš©í•œ content í•„ë“œ
            'content': truncated_content
        }
        
        summary_parts = [f"[{title}] ëª¨ì§‘ ê³µê³ ì…ë‹ˆë‹¤."]
        if parsed_notice.get('recruitment_period'):
            summary_parts.append(f"ëª¨ì§‘ê¸°ê°„: {parsed_notice.get('recruitment_period')}")
        if parsed_notice.get('target'):
            summary_parts.append(f"ëŒ€ìƒ: {parsed_notice.get('target')}")
        
        summary = " ".join(summary_parts)
        
        return [TextChunk(
            text=summary,
            metadata={**base_metadata, 'chunk_type': 'summary', 'priority': 'high'}
        )]


class FallbackNoticeParser(NoticeParser):
    """ë²”ìš© í´ë°± íŒŒì„œ (ëª¨ë“  ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ê°€ëŠ¥)"""
    TOPIC_TYPE = "general"
    
    def can_parse(self, title: str, text: str, patterns: Dict[str, Any]) -> bool:
        return True  # í•­ìƒ ì²˜ë¦¬ ê°€ëŠ¥ (ìµœí›„ì˜ ìˆ˜ë‹¨)

    def parse(self, notice_text: str, patterns: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ë³¸ íŒŒì‹±: ì œëª©ê³¼ ë³¸ë¬¸ ë¶„ë¦¬"""
        return {"full_text": notice_text}
        
    def create_chunks(self, parsed_notice: Dict[str, Any], notice_number: int) -> List[TextChunk]:
        """ì›ë¬¸ì„ ê¸°ë³¸ ì²­í¬ë¡œ ìƒì„±"""
        title = parsed_notice.get('title', 'ì œëª© ì—†ìŒ')
        full_text = parsed_notice.get('full_text', '')
        
        # Pydantic Citation ëª¨ë¸ì˜ context í•„ë“œ ìœ íš¨ì„± ê²€ì‚¬ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ 
        # contentì˜ ê¸¸ì´ë¥¼ 200ìë¡œ ì œí•œí•©ë‹ˆë‹¤.
        truncated_content = full_text[:200] + '...' if len(full_text) > 200 else full_text
        
        metadata = {
            'source_file': 'notice.txt',
            'notice_number': notice_number,
            'notice_title': title,
            'topic_type': 'general',
            'cache_ttl': 21600,  # 6ì‹œê°„ TTL
            'processing_date': datetime.now().isoformat(),
            'source_id': f'notice/notice.txt#section_{notice_number}',
            'chunk_type': 'general',
            # âœ… context ê¸¸ì´ ì œí•œ ë¡œì§ì„ ì ìš©í•œ content í•„ë“œ
            'content': truncated_content
        }
        
        return [TextChunk(text=f"[{title}]\n\n{full_text}", metadata=metadata)]

# ================================================================
# 3. BaseLoader íŒ¨í„´ì„ ì¤€ìˆ˜í•˜ëŠ” ë©”ì¸ ë¡œë”
# ================================================================

class NoticeLoader(BaseLoader):
    """
    BaseLoader íŒ¨í„´ì„ ì¤€ìˆ˜í•˜ëŠ” ê³µì§€ì‚¬í•­ ë¡œë”
    - í”ŒëŸ¬ê·¸ì¸ ê¸°ë°˜ ë™ì  íŒŒì‹± ì‹œìŠ¤í…œ
    - í•´ì‹œ ê¸°ë°˜ ì¦ë¶„ ë¹Œë“œ ì§€ì›
    - ìºì‹œ TTL 6ì‹œê°„ ì ìš©
    """
    
    def __init__(self):
        super().__init__(
            domain="notice",
            source_dir=config.ROOT_DIR / "data" / "notice",
            vectorstore_dir=config.ROOT_DIR / "vectorstores" / "vectorstore_notice",
            index_name="notice_index"
        )
        self.patterns_config = self._load_patterns_config()
        self.parsers = NoticeParser._registry
        logger.info(f"âœ¨ ë“±ë¡ëœ íŒŒì„œ: {list(self.parsers.keys())}")

    def _load_patterns_config(self) -> Dict[str, Any]:
        """schemas ë””ë ‰í† ë¦¬ì—ì„œ íŒ¨í„´ ì„¤ì • ë¡œë“œ"""
        config_path = config.ROOT_DIR / "schemas" / "notice_patterns.json"
        
        if config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"íŒ¨í„´ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ê¸°ë³¸ ì„¤ì • (íŒ¨í„´ íŒŒì¼ì´ ì—†ì„ ê²½ìš°)
        default_config = {
            "topic_patterns": {
                "evaluation": {
                    "keywords": ["í‰ê°€", "ê³¼ì œ", "ì œì¶œê¸°í•œ", "ë§ˆê°ì¼", "ì ìˆ˜"],
                    "priority": 25
                },
                "enrollment": {
                    "keywords": ["ì…êµ", "êµìœ¡ìƒ", "ì¤€ë¹„ë¬¼", "ì²´í¬ë¦¬ìŠ¤íŠ¸", "ì§€ì°¸"],
                    "priority": 20
                },
                "recruitment": {
                    "keywords": ["ëª¨ì§‘", "ì‹ ì²­", "ì ‘ìˆ˜", "ì„ ë°œ"],
                    "priority": 18
                },
                "general": {
                    "keywords": ["ê³µì§€", "ì•ˆë‚´", "ì•Œë¦¼"],
                    "priority": 10
                }
            }
        }
        
        # ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìë™ ìƒì„±
        try:
            config_path.parent.mkdir(parents=True, exist_ok=True)
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, ensure_ascii=False, indent=2)
            logger.info(f"ê¸°ë³¸ íŒ¨í„´ ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}")
        except Exception as e:
            logger.warning(f"íŒ¨í„´ ì„¤ì • íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return default_config

    def process_domain_data(self) -> List[TextChunk]:
        """
        BaseLoader ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„: ê³µì§€ì‚¬í•­ ë°ì´í„° ì²˜ë¦¬
        """
        all_chunks = []
        notice_file = self.source_dir / "notice.txt"
        
        if not notice_file.exists():
            logger.warning(f"ê³µì§€ì‚¬í•­ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {notice_file}")
            return all_chunks

        try:
            logger.info(f"ğŸ§  ìŠ¤ë§ˆíŠ¸ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì‹œì‘: {notice_file}")
            
            # íŒŒì¼ì„ ì„¹ì…˜ë³„ë¡œ ë¶„í•  ì²˜ë¦¬
            with open(notice_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            sections = [section.strip() for section in content.split('---') if section.strip()]

            for idx, section in enumerate(sections, 1):
                try:
                    # 1. ì œëª© ì¶”ì¶œ
                    title = self._extract_title(section)
                    
                    # 2. ìµœì  íŒŒì„œ ì„ íƒ
                    parser = self._select_best_parser(title, section)
                    
                    # 3. íŒŒì‹± ë° ì²­í¬ ìƒì„±
                    parsed_notice = parser.parse(section, self.patterns_config)
                    parsed_notice['title'] = title
                    parsed_notice['full_text'] = section
                    
                    chunks = parser.create_chunks(parsed_notice, idx)
                    all_chunks.extend(chunks)
                    
                    logger.info(f"ğŸ“‹ ê³µì§€ì‚¬í•­ #{idx} ({parser.TOPIC_TYPE}) ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")

                except Exception as e:
                    logger.error(f"ê³µì§€ì‚¬í•­ #{idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    # í´ë°± ì²˜ë¦¬
                    fallback_chunk = self._create_emergency_fallback(section, idx)
                    if fallback_chunk:
                        all_chunks.append(fallback_chunk)
            
            logger.info(f"âœ… ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ: {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")
            
        except Exception as e:
            logger.error(f"ê³µì§€ì‚¬í•­ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
        return all_chunks

    def _extract_title(self, text: str) -> str:
        """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ì œëª© ì¶”ì¶œ"""
        lines = text.strip().split('\n')
        if not lines:
            return "ì œëª© ì—†ìŒ"
        
        first_line = lines[0].strip()
        
        # ëŒ€ê´„í˜¸ íŒ¨í„´ ìš°ì„  ì¶”ì¶œ
        bracket_match = re.search(r'\[(.*?)\]', first_line)
        if bracket_match:
            return bracket_match.group(1).strip()
        
        # ì²« ë²ˆì§¸ ì¤„ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš© (50ì ì œí•œ)
        return first_line[:50] if len(first_line) > 50 else first_line

    def _select_best_parser(self, title: str, text: str) -> NoticeParser:
        """ê°€ì¥ ì í•©í•œ íŒŒì„œë¥¼ ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì„ íƒ"""
        best_parser = None
        best_score = -1
        
        # ë“±ë¡ëœ ëª¨ë“  íŒŒì„œë¥¼ ì ìˆ˜ìˆœìœ¼ë¡œ í‰ê°€
        for topic_type, parser_cls in self.parsers.items():
            try:
                parser_instance = parser_cls()
                if parser_instance.can_parse(title, text, self.patterns_config):
                    priority = self.patterns_config.get('topic_patterns', {}).get(topic_type, {}).get('priority', 0)
                    if priority > best_score:
                        best_score = priority
                        best_parser = parser_instance
            except Exception as e:
                logger.warning(f"íŒŒì„œ {topic_type} í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ì í•©í•œ íŒŒì„œê°€ ì—†ìœ¼ë©´ FallbackNoticeParser ì‚¬ìš©
        if best_parser is None:
            best_parser = FallbackNoticeParser()
            logger.debug("FallbackNoticeParser ì‚¬ìš©")
        
        return best_parser

    def _create_emergency_fallback(self, text: str, notice_number: int) -> Optional[TextChunk]:
        """ìµœí›„ì˜ ë¹„ìƒ í´ë°± ì²­í¬ ìƒì„±"""
        try:
            title = self._extract_title(text)
            # 500ì -> 200ìë¡œ ë³€ê²½
            content = text[:200] + "..." if len(text) > 200 else text
            
            fallback_text = f"""
[ê³µì§€ì‚¬í•­ #{notice_number}] {title}

{content}

âš ï¸ ì´ ê³µì§€ì‚¬í•­ì€ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ê¸°ë³¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.
ì •í™•í•œ ì •ë³´ëŠ” ì›ë³¸ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.
""".strip()
            
            return TextChunk(
                text=fallback_text,
                metadata={
                    'source_file': 'notice.txt',
                    'notice_number': notice_number,
                    'notice_title': title,
                    'topic_type': 'emergency_fallback',
                    'quality_level': 'fallback',
                    'cache_ttl': 21600,  # 6ì‹œê°„ TTL
                    'processing_date': datetime.now().isoformat(),
                    'source_id': f'notice/notice.txt#section_{notice_number}',
                    # âœ… context ê¸¸ì´ ì œí•œ ë¡œì§ì„ ì ìš©í•œ content í•„ë“œ
                    'content': content
                }
            )
        except Exception as e:
            logger.error(f"ë¹„ìƒ í´ë°± ìƒì„± ì‹¤íŒ¨: {e}")
            return None

# ================================================================
# 4. ëª¨ë“ˆ ì§„ì…ì 
# ================================================================

def main():
    """ê°œë°œ/í…ŒìŠ¤íŠ¸ìš© ì§„ì…ì """
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    loader = NoticeLoader()
    
    # BaseLoaderì˜ í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
    try:
        loader.load()  # FAISS ì¸ë±ìŠ¤ê¹Œì§€ ìë™ ìƒì„±
        logger.info("âœ… ê³µì§€ì‚¬í•­ ë¡œë” ì‹¤í–‰ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ë¡œë” ì‹¤í–‰ ì‹¤íŒ¨: {e}")

if __name__ == '__main__':
    main()
