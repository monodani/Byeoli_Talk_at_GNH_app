"""
ë§Œì¡±ë„ í†µí•© ë¡œë”
êµìœ¡ê³¼ì • ë§Œì¡±ë„(course_satisfaction.csv)ì™€ êµê³¼ëª© ë§Œì¡±ë„(subject_satisfaction.csv)ë¥¼ 
í†µí•© ì²˜ë¦¬í•˜ì—¬ ë‹¨ì¼ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
"""

import pandas as pd
from typing import List
from pathlib import Path

from .base_loader import BaseLoader
from utils.textifier import TextChunk
from utils.logging_utils import get_logger

logger = get_logger(__name__)

class SatisfactionLoader(BaseLoader):
    """ë§Œì¡±ë„ ë°ì´í„° í†µí•© ë¡œë”"""
    
    # ê¸°ì¡´ í…œí”Œë¦¿ ë³´ì¡´ (ì½”ë©ì—ì„œ ê²€ì¦ëœ ë¡œì§)
    COURSE_TEMPLATE = (
        "{êµìœ¡ì£¼ì°¨}ì— ê°œì„¤ëœ 'ì œ{êµìœ¡ê³¼ì •_ê¸°ìˆ˜}ê¸° {êµìœ¡ê³¼ì •}'ì€(ëŠ”) '{êµìœ¡ê³¼ì •_ìœ í˜•}'ìœ¼ë¡œ ë¶„ë¥˜ë˜ëŠ” êµìœ¡ê³¼ì •ìœ¼ë¡œ "
        "{êµìœ¡ì¼ì} {êµìœ¡ì¥ì†Œ}ì—ì„œ ì§„í–‰ë˜ì—ˆìœ¼ë©°, êµìœ¡ì¸ì›ì€ ì´ {êµìœ¡ì¸ì›}ëª…ì´ì—ˆìŠµë‹ˆë‹¤. "
        "'ì œ{êµìœ¡ê³¼ì •_ê¸°ìˆ˜}ê¸° {êµìœ¡ê³¼ì •}' êµìœ¡ìƒì˜ êµìœ¡ì— ëŒ€í•œ 'ì „ë°˜ì ì¸ ë§Œì¡±ë„'ëŠ” {ì „ë°˜ë§Œì¡±ë„}ì , "
        "êµìœ¡íš¨ê³¼ ì²´ê°ë„ ì§€í‘œì¸ 'ì—­ëŸ‰í–¥ìƒë„' ì ìˆ˜ëŠ” {ì—­ëŸ‰í–¥ìƒë„}ì , 'í˜„ì—…ì ìš©ë„'ëŠ” {í˜„ì—…ì ìš©ë„}ì ì´ì—ˆìŠµë‹ˆë‹¤. "
        "ë˜í•œ, 'êµê³¼í¸ì„± ë§Œì¡±ë„' {êµê³¼í¸ì„±_ë§Œì¡±ë„}ì , 'ì œ{êµìœ¡ê³¼ì •_ê¸°ìˆ˜}ê¸° {êµìœ¡ê³¼ì •}' ì „ì²´ ê°•ì˜ì— ëŒ€í•œ 'ê°•ì˜ë§Œì¡±ë„' í‰ê· ì€ {êµìœ¡ê³¼ì •ë³„_ê°•ì˜ë§Œì¡±ë„_í‰ê· }ì ì´ì—ˆìœ¼ë©°, "
        "'ì œ{êµìœ¡ê³¼ì •_ê¸°ìˆ˜}ê¸° {êµìœ¡ê³¼ì •}'ì— ëŒ€í•œ ëª¨ë“  ë§Œì¡±ë„ ì§€í‘œ í‰ê· ì¸ 'ì œ{êµìœ¡ê³¼ì •_ê¸°ìˆ˜}ê¸° {êµìœ¡ê³¼ì •}'ì˜ 'ì¢…í•©ë§Œì¡±ë„'ëŠ” {ì¢…í•©ë§Œì¡±ë„}ì ìœ¼ë¡œ "
        "'{êµìœ¡ì—°ë„}ë…„' ì „ì²´ êµìœ¡ê³¼ì • ì¤‘ '{êµìœ¡ê³¼ì •_ìˆœìœ„}ìœ„'ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤."
    )
    
    SUBJECT_TEMPLATE = (
        "{êµìœ¡ì£¼ì°¨}ì— ê°œì„¤ëœ 'ì œ{êµìœ¡ê³¼ì •_ê¸°ìˆ˜}ê¸° {êµìœ¡ê³¼ì •}'ì˜ '{êµê³¼ëª©(ê°•ì˜)}' êµê³¼ëª©(ê°•ì˜)ì— ëŒ€í•œ "
        "'ê°•ì˜ë§Œì¡±ë„'ëŠ” {ê°•ì˜ë§Œì¡±ë„}ì ìœ¼ë¡œ '{êµìœ¡ì—°ë„}ë…„' ìš´ì˜ëœ ì „ì²´ êµê³¼ëª©(ê°•ì˜) ì¤‘ '{êµê³¼ëª©(ê°•ì˜)_ìˆœìœ„}ìœ„'ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤."
    )
    
    def __init__(self):
        super().__init__(
            loader_id="satisfaction",
            source_dir="data/satisfaction",
            target_dir="vectorstores/vectorstore_unified_satisfaction",
            schema_dir="schemas"
        )
    
    def get_file_patterns(self) -> List[str]:
        """ì²˜ë¦¬í•  íŒŒì¼ íŒ¨í„´ ë°˜í™˜"""
        return ["course_satisfaction.csv", "subject_satisfaction.csv"]
    
    def process_domain_data(self, chunks: List[TextChunk]) -> List[TextChunk]:
        """
        ë§Œì¡±ë„ ë°ì´í„°ë¥¼ í†µí•© ì²˜ë¦¬
        
        Args:
            chunks: textifierë¡œ ì¶”ì¶œëœ CSV ì›ë³¸ ì²­í¬ë“¤
            
        Returns:
            í…œí”Œë¦¿ì´ ì ìš©ëœ í†µí•© ì²˜ë¦¬ëœ ì²­í¬ë“¤
        """
        course_chunks = []
        subject_chunks = []
        
        # ì†ŒìŠ¤ë³„ë¡œ ì²­í¬ ë¶„ë¥˜
        for chunk in chunks:
            source_file = chunk.metadata.get("file_path", "")
            
            if "course_satisfaction.csv" in source_file:
                course_chunks.append(chunk)
            elif "subject_satisfaction.csv" in source_file:
                subject_chunks.append(chunk)
            else:
                logger.warning(f"Unknown satisfaction source file: {source_file}")
        
        # ê°ê° ì²˜ë¦¬
        processed_course = self._process_course_chunks(course_chunks)
        processed_subject = self._process_subject_chunks(subject_chunks)
        
        # í†µí•© ë°˜í™˜
        all_processed = processed_course + processed_subject
        logger.info(f"Processed {len(processed_course)} course + {len(processed_subject)} subject = {len(all_processed)} total satisfaction chunks")
        
        return all_processed
    
    def _process_course_chunks(self, chunks: List[TextChunk]) -> List[TextChunk]:
        """êµìœ¡ê³¼ì • ë§Œì¡±ë„ ì²­í¬ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ë³´ì¡´)"""
        processed_chunks = []
        
        for chunk in chunks:
            try:
                # CSV í–‰ ë°ì´í„° íŒŒì‹±
                row_data = self._parse_csv_chunk(chunk.content)
                if not row_data:
                    continue
                
                # í•„ìˆ˜ í•„ë“œ í™•ì¸ (ê¸°ì¡´ í…œí”Œë¦¿ ê¸°ì¤€)
                required_fields = [
                    'êµìœ¡ì£¼ì°¨', 'êµìœ¡ê³¼ì •_ê¸°ìˆ˜', 'êµìœ¡ê³¼ì •', 'êµìœ¡ê³¼ì •_ìœ í˜•', 
                    'êµìœ¡ì¼ì', 'êµìœ¡ì¥ì†Œ', 'êµìœ¡ì¸ì›', 'ì „ë°˜ë§Œì¡±ë„', 'ì—­ëŸ‰í–¥ìƒë„', 
                    'í˜„ì—…ì ìš©ë„', 'êµê³¼í¸ì„±_ë§Œì¡±ë„', 'êµìœ¡ê³¼ì •ë³„_ê°•ì˜ë§Œì¡±ë„_í‰ê· ', 
                    'ì¢…í•©ë§Œì¡±ë„', 'êµìœ¡ì—°ë„', 'êµìœ¡ê³¼ì •_ìˆœìœ„'
                ]
                
                missing_fields = [field for field in required_fields if field not in row_data]
                if missing_fields:
                    logger.warning(f"Missing course fields: {missing_fields}")
                    continue
                
                # í…œí”Œë¦¿ ì ìš©
                formatted_content = self.COURSE_TEMPLATE.format(**row_data)
                
                # ë©”íƒ€ë°ì´í„° ìƒì„± (ê²€ìƒ‰ ìµœì í™”)
                new_metadata = {
                    **chunk.metadata,
                    "satisfaction_type": "course",
                    "education_course": row_data.get('êµìœ¡ê³¼ì •', ''),
                    "course_session": row_data.get('êµìœ¡ê³¼ì •_ê¸°ìˆ˜', ''),
                    "course_type": row_data.get('êµìœ¡ê³¼ì •_ìœ í˜•', ''),
                    "education_week": row_data.get('êµìœ¡ì£¼ì°¨', ''),
                    "education_year": row_data.get('êµìœ¡ì—°ë„', ''),
                    "overall_satisfaction": row_data.get('ì „ë°˜ë§Œì¡±ë„', ''),
                    "comprehensive_satisfaction": row_data.get('ì¢…í•©ë§Œì¡±ë„', ''),
                    "course_ranking": row_data.get('êµìœ¡ê³¼ì •_ìˆœìœ„', ''),
                    "capacity_improvement": row_data.get('ì—­ëŸ‰í–¥ìƒë„', ''),
                    "work_application": row_data.get('í˜„ì—…ì ìš©ë„', ''),
                    "curriculum_satisfaction": row_data.get('êµê³¼í¸ì„±_ë§Œì¡±ë„', ''),
                    "lecture_satisfaction_avg": row_data.get('êµìœ¡ê³¼ì •ë³„_ê°•ì˜ë§Œì¡±ë„_í‰ê· ', '')
                }
                
                processed_chunk = TextChunk(
                    content=formatted_content,
                    metadata=new_metadata,
                    source_id=chunk.source_id,
                    chunk_index=chunk.chunk_index
                )
                
                processed_chunks.append(processed_chunk)
                
            except Exception as e:
                logger.error(f"Failed to process course chunk from {chunk.source_id}: {e}")
                continue
        
        logger.info(f"Processed {len(processed_chunks)} course satisfaction chunks")
        return processed_chunks
    
    def _process_subject_chunks(self, chunks: List[TextChunk]) -> List[TextChunk]:
        """êµê³¼ëª© ë§Œì¡±ë„ ì²­í¬ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ë³´ì¡´)"""
        processed_chunks = []
        
        for chunk in chunks:
            try:
                # CSV í–‰ ë°ì´í„° íŒŒì‹±
                row_data = self._parse_csv_chunk(chunk.content)
                if not row_data:
                    continue
                
                # í•„ìˆ˜ í•„ë“œ í™•ì¸ (ê¸°ì¡´ í…œí”Œë¦¿ ê¸°ì¤€)
                required_fields = [
                    'êµìœ¡ì£¼ì°¨', 'êµìœ¡ê³¼ì •_ê¸°ìˆ˜', 'êµìœ¡ê³¼ì •', 'êµê³¼ëª©(ê°•ì˜)', 
                    'ê°•ì˜ë§Œì¡±ë„', 'êµìœ¡ì—°ë„', 'êµê³¼ëª©(ê°•ì˜)_ìˆœìœ„'
                ]
                
                missing_fields = [field for field in required_fields if field not in row_data]
                if missing_fields:
                    logger.warning(f"Missing subject fields: {missing_fields}")
                    continue
                
                # í…œí”Œë¦¿ ì ìš©
                formatted_content = self.SUBJECT_TEMPLATE.format(**row_data)
                
                # ë©”íƒ€ë°ì´í„° ìƒì„± (ê²€ìƒ‰ ìµœì í™”)
                new_metadata = {
                    **chunk.metadata,
                    "satisfaction_type": "subject",
                    "education_course": row_data.get('êµìœ¡ê³¼ì •', ''),
                    "course_session": row_data.get('êµìœ¡ê³¼ì •_ê¸°ìˆ˜', ''),
                    "subject_name": row_data.get('êµê³¼ëª©(ê°•ì˜)', ''),
                    "education_week": row_data.get('êµìœ¡ì£¼ì°¨', ''),
                    "education_year": row_data.get('êµìœ¡ì—°ë„', ''),
                    "lecture_satisfaction": row_data.get('ê°•ì˜ë§Œì¡±ë„', ''),
                    "subject_ranking": row_data.get('êµê³¼ëª©(ê°•ì˜)_ìˆœìœ„', '')
                }
                
                processed_chunk = TextChunk(
                    content=formatted_content,
                    metadata=new_metadata,
                    source_id=chunk.source_id,
                    chunk_index=chunk.chunk_index
                )
                
                processed_chunks.append(processed_chunk)
                
            except Exception as e:
                logger.error(f"Failed to process subject chunk from {chunk.source_id}: {e}")
                continue
        
        logger.info(f"Processed {len(processed_chunks)} subject satisfaction chunks")
        return processed_chunks
    
    def _parse_csv_chunk(self, content: str) -> dict:
        """
        CSV ì²­í¬ ë‚´ìš©ì„ íŒŒì‹±í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        
        Args:
            content: "í•„ë“œëª…: ê°’ | í•„ë“œëª…: ê°’" í˜•íƒœì˜ í…ìŠ¤íŠ¸
            
        Returns:
            í•„ë“œëª…-ê°’ ë”•ì…”ë„ˆë¦¬
        """
        try:
            data = {}
            
            # " | "ë¡œ í•„ë“œ ë¶„ë¦¬
            fields = content.split(" | ")
            
            for field in fields:
                if ":" in field:
                    key, value = field.split(":", 1)
                    data[key.strip()] = value.strip()
            
            return data
            
        except Exception as e:
            logger.error(f"Failed to parse CSV chunk content: {e}")
            return {}
    
    def get_satisfaction_statistics(self) -> dict:
        """ë§Œì¡±ë„ í†µê³„ ì •ë³´ ë°˜í™˜ (ëª¨ë‹ˆí„°ë§ìš©)"""
        metadata = self.load_metadata()
        if not metadata:
            return {"status": "not_built"}
        
        return {
            "loader_id": self.loader_id,
            "total_chunks": metadata.total_chunks,
            "total_files": metadata.total_files,
            "last_build": metadata.last_build.isoformat(),
            "supported_types": ["course", "subject"],
            "templates": {
                "course_fields": [
                    "êµìœ¡ì£¼ì°¨", "êµìœ¡ê³¼ì •_ê¸°ìˆ˜", "êµìœ¡ê³¼ì •", "êµìœ¡ê³¼ì •_ìœ í˜•",
                    "ì „ë°˜ë§Œì¡±ë„", "ì—­ëŸ‰í–¥ìƒë„", "í˜„ì—…ì ìš©ë„", "ì¢…í•©ë§Œì¡±ë„", "êµìœ¡ê³¼ì •_ìˆœìœ„"
                ],
                "subject_fields": [
                    "êµìœ¡ì£¼ì°¨", "êµìœ¡ê³¼ì •_ê¸°ìˆ˜", "êµìœ¡ê³¼ì •", "êµê³¼ëª©(ê°•ì˜)",
                    "ê°•ì˜ë§Œì¡±ë„", "êµê³¼ëª©(ê°•ì˜)_ìˆœìœ„"
                ]
            }
        }
    
    def search_by_satisfaction_type(self, satisfaction_type: str) -> dict:
        """
        ë§Œì¡±ë„ íƒ€ì…ë³„ ê²€ìƒ‰ ì§€ì› ì •ë³´
        
        Args:
            satisfaction_type: "course" ë˜ëŠ” "subject"
            
        Returns:
            í•´ë‹¹ íƒ€ì…ì˜ ë©”íƒ€ë°ì´í„° í•„í„° ì •ë³´
        """
        if satisfaction_type == "course":
            return {
                "filter": {"satisfaction_type": "course"},
                "searchable_fields": [
                    "education_course", "course_type", "education_year",
                    "overall_satisfaction", "comprehensive_satisfaction"
                ]
            }
        elif satisfaction_type == "subject":
            return {
                "filter": {"satisfaction_type": "subject"},
                "searchable_fields": [
                    "education_course", "subject_name", "education_year",
                    "lecture_satisfaction"
                ]
            }
        else:
            return {"error": "Invalid satisfaction_type. Use 'course' or 'subject'"}

# í¸ì˜ í•¨ìˆ˜ë“¤
def build_satisfaction_index(force_rebuild: bool = False) -> bool:
    """í†µí•© ë§Œì¡±ë„ ì¸ë±ìŠ¤ ë¹Œë“œ"""
    loader = SatisfactionLoader()
    return loader.build_index(force_rebuild=force_rebuild)

def get_satisfaction_status() -> dict:
    """ë§Œì¡±ë„ ë¡œë” ìƒíƒœ ì¡°íšŒ"""
    loader = SatisfactionLoader()
    return loader.get_satisfaction_statistics()

def search_course_satisfaction_info() -> dict:
    """êµìœ¡ê³¼ì • ë§Œì¡±ë„ ê²€ìƒ‰ ì§€ì› ì •ë³´"""
    loader = SatisfactionLoader()
    return loader.search_by_satisfaction_type("course")

def search_subject_satisfaction_info() -> dict:
    """êµê³¼ëª© ë§Œì¡±ë„ ê²€ìƒ‰ ì§€ì› ì •ë³´"""
    loader = SatisfactionLoader()
    return loader.search_by_satisfaction_type("subject")

# ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ìš©
if __name__ == "__main__":
    import sys
    
    # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
    force_rebuild = "--force" in sys.argv
    show_stats = "--stats" in sys.argv
    
    if show_stats:
        stats = get_satisfaction_status()
        print(f"ğŸ“Š Satisfaction Statistics:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
        sys.exit(0)
    
    print(f"Building unified satisfaction index (force_rebuild={force_rebuild})...")
    
    success = build_satisfaction_index(force_rebuild=force_rebuild)
    
    if success:
        stats = get_satisfaction_status()
        print(f"âœ… Build completed successfully!")
        print(f"ğŸ“Š Course + Subject combined: {stats.get('total_chunks', 0)} chunks")
        print(f"ğŸ” Search capabilities:")
        print(f"  - Course satisfaction: {search_course_satisfaction_info()}")
        print(f"  - Subject satisfaction: {search_subject_satisfaction_info()}")
    else:
        print("âŒ Build failed!")
        sys.exit(1)
