#!/usr/bin/env python3
"""
ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› RAG ì±—ë´‡ - ë§Œì¡±ë„ í†µí•© ë¡œë” (BaseLoader íŒ¨í„´ ì¤€ìˆ˜)

notice ë¡œë” íŒ¨í„´ì„ ë”°ë¼ ì™„ì „íˆ ìˆ˜ì •ë¨:
- process_domain_data(self) ì‹œê·¸ë‹ˆì²˜ë¡œ ë³€ê²½
- ì›ë³¸ CSV íŒŒì¼ ì§ì ‘ ì½ê¸° ë¡œì§ ì¶”ê°€
- BaseLoader í‘œì¤€ íŒ¨í„´ ì™„ì „ ì¤€ìˆ˜
- ê¸°ì¡´ í…œí”Œë¦¿ ì‹œìŠ¤í…œ ìœ ì§€
"""

import logging
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from modules.base_loader import BaseLoader
from utils.textifier import TextChunk
from utils.config import config

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class SatisfactionLoader(BaseLoader):
    """
    ë§Œì¡±ë„ ë°ì´í„° í†µí•© ë¡œë” - BaseLoader í‘œì¤€ íŒ¨í„´ ì¤€ìˆ˜
    
    ì²˜ë¦¬ ëŒ€ìƒ:
    - data/satisfaction/course_satisfaction.csv (êµìœ¡ê³¼ì • ë§Œì¡±ë„)
    - data/satisfaction/subject_satisfaction.csv (êµê³¼ëª© ë§Œì¡±ë„)
    
    íŠ¹ì§•:
    - notice ë¡œë”ì™€ ë™ì¼í•œ process_domain_data(self) ì‹œê·¸ë‹ˆì²˜
    - ì›ë³¸ CSV íŒŒì¼ ì§ì ‘ ì½ê¸°
    - ê¸°ì¡´ í…œí”Œë¦¿ ì‹œìŠ¤í…œ ì™„ë²½ ë³´ì¡´
    - í•´ì‹œ ê¸°ë°˜ ì¦ë¶„ ë¹Œë“œ ì§€ì›
    """
    
    # ê¸°ì¡´ í…œí”Œë¦¿ ë³´ì¡´ (ì½”ë©ì—ì„œ ê²€ì¦ëœ ë¡œì§)
    COURSE_TEMPLATE = (
        "{êµìœ¡ì£¼ì°¨}ì— ê°œì„¤ëœ 'ì œ{êµìœ¡ê³¼ì •_ê¸°ìˆ˜}ê¸° {êµìœ¡ê³¼ì •}'ì€(ëŠ”) '{êµìœ¡ê³¼ì •_ìœ í˜•}'ìœ¼ë¡œ ë¶„ë¥˜ë˜ëŠ” êµìœ¡ê³¼ì •ìœ¼ë¡œ "
        "{êµìœ¡ì¼ì} {êµìœ¡ì¥ì†Œ}ì—ì„œ ì§„í–‰ë˜ì—ˆìœ¼ë©°, êµìœ¡ì¸ì›ì€ ì´ {êµìœ¡ì¸ì›}ëª…ì´ì—ˆìŠµë‹ˆë‹¤. "
        "'ì œ{êµìœ¡ê³¼ì •_ê¸°ìˆ˜}ê¸° {êµìœ¡ê³¼ì •}' êµìœ¡ìƒì˜ êµìœ¡ì— ëŒ€í•œ 'ì „ë°˜ì ì¸ ë§Œì¡±ë„'ëŠ” {ì „ë°˜ë§Œì¡±ë„}ì , "
        "êµìœ¡íš¨ê³¼ ì²´ê°ë„ ì§€í‘œì¸ 'ì—­ëŸ‰í–¥ìƒë„' ì ìˆ˜ëŠ” {ì—­ëŸ‰í–¥ìƒë„}ì , 'í˜„ì—…ì ìš©ë„'ëŠ” {í˜„ì—…ì ìš©ë„}ì ì´ì—ˆìŠµë‹ˆë‹¤. "
        "ë˜í•œ, 'êµê³¼í¸ì„± ë§Œì¡±ë„' {êµê³¼í¸ì„±_ë§Œì¡±ë„}ì , 'ì œ{êµìœ¡ê³¼ì •_ê¸°ìˆ˜}ê¸° {êµìœ¡ê³¼ì •}' ì „ì²´ ê°•ì˜ì— ëŒ€í•œ 'ê°•ì˜ë§Œì¡±ë„' í‰ê· ì€ {êµìœ¡ê³¼ì •ë³„_ê°•ì˜ë§Œì¡±ë„_í‰ê· }ì ì´ì—ˆìœ¼ë©°, "
        "'ì œ{êµìœ¡ê³¼ì •_ê¸°ìˆ˜}ê¸° {êµìœ¡ê³¼ì •}'ì— ëŒ€í•œ ëª¨ë“  ë§Œì¡±ë„ ì§€í‘œ í‰ê· ì¸ 'ì œ{êµìœ¡ê³¼ì •_ê¸°ìˆ˜}ê¸° {êµìœ¡ê³¼ì •}'ì˜ 'ì¢…í•©ë§Œì¡±ë„'ëŠ” {ì¢…í•©ë§Œì¡±ë„}ì ìœ¼ë¡œ "
        "'{êµìœ¡ì—°ë„}ë…„' ì „ì²´ êµìœ¡ê³¼ì • ì¤‘ '{êµìœ¡ê³¼ì •_ìˆœìœ„}ìœ„'ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤."
    )
    
    SUBJECT_TEMPLATE = (
        "{êµìœ¡ì£¼ì°¨}ì— ê°œì„¤ëœ 'ì œ{êµìœ¡ê³¼ì •_ê¸°ìˆ˜}ê¸° {êµìœ¡ê³¼ì •}'ì˜ '{êµê³¼ëª©(ê°•ì˜)}' êµê³¼ëª©(ê°•ì˜)ì— ëŒ€í•œ "
        "'ê°•ì˜ë§Œì¡±ë„'ëŠ” {ê°•ì˜ë§Œì¡±ë„}ì ìœ¼ë¡œ '{êµìœ¡ì—°ë„}ë…„' ìš´ì˜ëœ ì „ì²´ êµê³¼ëª©(ê°•ì˜) ì¤‘ '{êµê³¼ëª©(ê°•ì˜)_ìˆœìœ„}ìœ„'ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤."
    )
    
    def __init__(self):
        super().__init__(
            domain="satisfaction",
            source_dir=config.ROOT_DIR / "data" / "satisfaction",
            vectorstore_dir=config.ROOT_DIR / "vectorstores" / "vectorstore_unified_satisfaction",
            index_name="satisfaction_index"
        )
        
        # ì²˜ë¦¬í•  íŒŒì¼ ì •ì˜
        self.course_file = self.source_dir / "course_satisfaction.csv"
        self.subject_file = self.source_dir / "subject_satisfaction.csv"
    
    def process_domain_data(self) -> List[TextChunk]:
        """
        BaseLoader ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„: ë§Œì¡±ë„ ë°ì´í„° ì²˜ë¦¬
        
        âœ… notice ë¡œë”ì™€ ë™ì¼í•œ ì‹œê·¸ë‹ˆì²˜: process_domain_data(self)
        """
        all_chunks = []
        
        # 1. êµìœ¡ê³¼ì • ë§Œì¡±ë„ ì²˜ë¦¬
        course_chunks = self._process_course_satisfaction()
        all_chunks.extend(course_chunks)
        
        # 2. êµê³¼ëª© ë§Œì¡±ë„ ì²˜ë¦¬
        subject_chunks = self._process_subject_satisfaction()
        all_chunks.extend(subject_chunks)
        
        logger.info(f"âœ… ë§Œì¡±ë„ í†µí•© ì²˜ë¦¬ ì™„ë£Œ: êµìœ¡ê³¼ì • {len(course_chunks)}ê°œ + êµê³¼ëª© {len(subject_chunks)}ê°œ = ì´ {len(all_chunks)}ê°œ ì²­í¬")
        
        return all_chunks
    
    def _process_course_satisfaction(self) -> List[TextChunk]:
        """êµìœ¡ê³¼ì • ë§Œì¡±ë„ CSV ì§ì ‘ ì½ê¸° ë° ì²˜ë¦¬"""
        chunks = []
        
        if not self.course_file.exists():
            logger.warning(f"êµìœ¡ê³¼ì • ë§Œì¡±ë„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.course_file}")
            return chunks
        
        try:
            logger.info(f"ğŸ“Š êµìœ¡ê³¼ì • ë§Œì¡±ë„ ì²˜ë¦¬ ì‹œì‘: {self.course_file}")
            
            # CSV ì§ì ‘ ì½ê¸° (ìë™ ì¸ì½”ë”© ê°ì§€)
            try:
                # 1. ë¨¼ì € utf-8ë¡œ ì‹œë„ (í‘œì¤€)
                df = pd.read_csv(self.course_file, encoding='utf-8')
            except UnicodeDecodeError:
                # 2. ì‹¤íŒ¨ ì‹œ, í•œêµ­ì–´ CSVì— ìì£¼ ì‚¬ìš©ë˜ëŠ” cp949ë¡œ ì¬ì‹œë„
                df = pd.read_csv(self.course_file, encoding='cp949')
                logger.warning("âš ï¸ UTF-8 ë””ì½”ë”© ì‹¤íŒ¨. CP949 ì¸ì½”ë”©ìœ¼ë¡œ ë‹¤ì‹œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

            logger.info(f"ğŸ“„ êµìœ¡ê³¼ì • ë§Œì¡±ë„ ë°ì´í„°: {len(df)}í–‰ ë¡œë“œë¨")
            
            # ê° í–‰ì„ TextChunkë¡œ ë³€í™˜
            for idx, row in df.iterrows():
                try:
                    # ë°ì´í„° ê²€ì¦ ë° ì •ì œ
                    clean_data = self._validate_and_clean_course_data(row.to_dict(), f"course_row_{idx}")
                    if not clean_data:
                        continue
                    
                    # í…œí”Œë¦¿ ì ìš©
                    try:
                        formatted_content = self.COURSE_TEMPLATE.format(**clean_data)
                    except KeyError as e:
                        logger.error(f"êµìœ¡ê³¼ì • í…œí”Œë¦¿ ì ìš© ì‹¤íŒ¨ (í–‰ {idx}): ëˆ„ë½ í•„ë“œ {e}")
                        continue
                    
                    # ë©”íƒ€ë°ì´í„° ìƒì„±
                    metadata = {
                        'source_file': 'course_satisfaction.csv',
                        'source_id': f'satisfaction/course_satisfaction.csv#row_{idx}',
                        'satisfaction_type': 'course',
                        'education_course': clean_data.get('êµìœ¡ê³¼ì •', ''),
                        'course_session': str(clean_data.get('êµìœ¡ê³¼ì •_ê¸°ìˆ˜', '')),
                        'education_year': str(clean_data.get('êµìœ¡ì—°ë„', '')),
                        'overall_satisfaction': self._safe_convert_to_float(clean_data.get('ì „ë°˜ë§Œì¡±ë„', '')),
                        'comprehensive_satisfaction': self._safe_convert_to_float(clean_data.get('ì¢…í•©ë§Œì¡±ë„', '')),
                        'course_ranking': self._safe_convert_to_int(clean_data.get('êµìœ¡ê³¼ì •_ìˆœìœ„', '')),
                        'cache_ttl': 2592000,  # 30ì¼ TTL
                        'processing_date': datetime.now().isoformat(),
                        'chunk_type': 'course_satisfaction'
                    }
                    
                    chunk = TextChunk(
                        text=formatted_content,
                        metadata=metadata
                    )
                    
                    chunks.append(chunk)
                    
                except Exception as e:
                    logger.error(f"êµìœ¡ê³¼ì • í–‰ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… êµìœ¡ê³¼ì • ë§Œì¡±ë„ ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
        except Exception as e:
            logger.error(f"âŒ êµìœ¡ê³¼ì • ë§Œì¡±ë„ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        return chunks
    
    def _process_subject_satisfaction(self) -> List[TextChunk]:
        """êµê³¼ëª© ë§Œì¡±ë„ CSV ì§ì ‘ ì½ê¸° ë° ì²˜ë¦¬"""
        chunks = []
        
        if not self.subject_file.exists():
            logger.warning(f"êµê³¼ëª© ë§Œì¡±ë„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.subject_file}")
            return chunks
        
        try:
            logger.info(f"ğŸ“Š êµê³¼ëª© ë§Œì¡±ë„ ì²˜ë¦¬ ì‹œì‘: {self.subject_file}")
            
            # CSV ì§ì ‘ ì½ê¸° (ìë™ ì¸ì½”ë”© ê°ì§€)
            try:
                # 1. ë¨¼ì € utf-8ë¡œ ì‹œë„ (í‘œì¤€)
                df = pd.read_csv(self.subject_file, encoding='utf-8')
            except UnicodeDecodeError:
                # 2. ì‹¤íŒ¨ ì‹œ, í•œêµ­ì–´ CSVì— ìì£¼ ì‚¬ìš©ë˜ëŠ” cp949ë¡œ ì¬ì‹œë„
                df = pd.read_csv(self.subject_file, encoding='cp949')
                logger.warning("âš ï¸ UTF-8 ë””ì½”ë”© ì‹¤íŒ¨. CP949 ì¸ì½”ë”©ìœ¼ë¡œ ë‹¤ì‹œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            
            logger.info(f"ğŸ“„ êµê³¼ëª© ë§Œì¡±ë„ ë°ì´í„°: {len(df)}í–‰ ë¡œë“œë¨")
            
            # ê° í–‰ì„ TextChunkë¡œ ë³€í™˜
            for idx, row in df.iterrows():
                try:
                    # ë°ì´í„° ê²€ì¦ ë° ì •ì œ
                    clean_data = self._validate_and_clean_subject_data(row.to_dict(), f"subject_row_{idx}")
                    if not clean_data:
                        continue
                    
                    # í…œí”Œë¦¿ ì ìš©
                    try:
                        formatted_content = self.SUBJECT_TEMPLATE.format(**clean_data)
                    except KeyError as e:
                        logger.error(f"êµê³¼ëª© í…œí”Œë¦¿ ì ìš© ì‹¤íŒ¨ (í–‰ {idx}): ëˆ„ë½ í•„ë“œ {e}")
                        continue
                    
                    # ë©”íƒ€ë°ì´í„° ìƒì„±
                    metadata = {
                        'source_file': 'subject_satisfaction.csv',
                        'source_id': f'satisfaction/subject_satisfaction.csv#row_{idx}',
                        'satisfaction_type': 'subject',
                        'education_course': clean_data.get('êµìœ¡ê³¼ì •', ''),
                        'course_session': str(clean_data.get('êµìœ¡ê³¼ì •_ê¸°ìˆ˜', '')),
                        'subject_name': clean_data.get('êµê³¼ëª©(ê°•ì˜)', ''),
                        'education_year': str(clean_data.get('êµìœ¡ì—°ë„', '')),
                        'lecture_satisfaction': self._safe_convert_to_float(clean_data.get('ê°•ì˜ë§Œì¡±ë„', '')),
                        'subject_ranking': self._safe_convert_to_int(clean_data.get('êµê³¼ëª©(ê°•ì˜)_ìˆœìœ„', '')),
                        'cache_ttl': 2592000,  # 30ì¼ TTL
                        'processing_date': datetime.now().isoformat(),
                        'chunk_type': 'subject_satisfaction'
                    }
                    
                    chunk = TextChunk(
                        text=formatted_content,
                        metadata=metadata
                    )
                    
                    chunks.append(chunk)
                    
                except Exception as e:
                    logger.error(f"êµê³¼ëª© í–‰ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… êµê³¼ëª© ë§Œì¡±ë„ ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
        except Exception as e:
            logger.error(f"âŒ êµê³¼ëª© ë§Œì¡±ë„ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        return chunks
    
    def _validate_and_clean_course_data(self, row_data: Dict[str, Any], source_id: str) -> Optional[Dict[str, str]]:
        """êµìœ¡ê³¼ì • ë°ì´í„° ê²€ì¦ ë° ì •ì œ"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            required_fields = ['êµìœ¡ê³¼ì •', 'êµìœ¡ê³¼ì •_ê¸°ìˆ˜', 'ì „ë°˜ë§Œì¡±ë„']
            for field in required_fields:
                if field not in row_data or pd.isna(row_data[field]):
                    logger.warning(f"êµìœ¡ê³¼ì • í•„ìˆ˜ í•„ë“œ ëˆ„ë½ ({source_id}): {field}")
                    return None
            
            # ë°ì´í„° ì •ì œ
            clean_data = {}
            for key, value in row_data.items():
                if pd.isna(value):
                    clean_data[key] = ''
                else:
                    clean_data[key] = str(value).strip()
            
            return clean_data
            
        except Exception as e:
            logger.error(f"êµìœ¡ê³¼ì • ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨ ({source_id}): {e}")
            return None
    
    def _validate_and_clean_subject_data(self, row_data: Dict[str, Any], source_id: str) -> Optional[Dict[str, str]]:
        """êµê³¼ëª© ë°ì´í„° ê²€ì¦ ë° ì •ì œ"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            required_fields = ['êµìœ¡ê³¼ì •', 'êµê³¼ëª©(ê°•ì˜)', 'ê°•ì˜ë§Œì¡±ë„']
            for field in required_fields:
                if field not in row_data or pd.isna(row_data[field]):
                    logger.warning(f"êµê³¼ëª© í•„ìˆ˜ í•„ë“œ ëˆ„ë½ ({source_id}): {field}")
                    return None
            
            # ë°ì´í„° ì •ì œ
            clean_data = {}
            for key, value in row_data.items():
                if pd.isna(value):
                    clean_data[key] = ''
                else:
                    clean_data[key] = str(value).strip()
            
            return clean_data
            
        except Exception as e:
            logger.error(f"êµê³¼ëª© ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨ ({source_id}): {e}")
            return None
    
    def _safe_convert_to_float(self, value: Any) -> float:
        """ì•ˆì „í•œ float ë³€í™˜"""
        try:
            if pd.isna(value) or value == '':
                return 0.0
            return float(str(value).strip())
        except (ValueError, TypeError):
            return 0.0
    
    def _safe_convert_to_int(self, value: Any) -> int:
        """ì•ˆì „í•œ int ë³€í™˜"""
        try:
            if pd.isna(value) or value == '':
                return 0
            return int(float(str(value).strip()))
        except (ValueError, TypeError):
            return 0


# ================================================================
# ê°œë°œ/í…ŒìŠ¤íŠ¸ìš© ì§„ì…ì 
# ================================================================

def main():
    """ê°œë°œ/í…ŒìŠ¤íŠ¸ìš© ì§„ì…ì """
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    loader = SatisfactionLoader()
    
    try:
        # BaseLoaderì˜ í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
        success = loader.build_vectorstore()
        
        if success:
            logger.info("âœ… ë§Œì¡±ë„ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ")
        else:
            logger.error("âŒ ë§Œì¡±ë„ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì‹¤íŒ¨")
            
    except Exception as e:
        logger.error(f"âŒ ë¡œë” ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise


if __name__ == '__main__':
    main()
