#!/usr/bin/env python3
"""
ë²¼ë¦¬í†¡@ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› (ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› RAG ì±—ë´‡) - index_manager.py (OpenAI í˜¸í™˜ì„± ìˆ˜ì • ë²„ì „)

IndexManager ì‹±ê¸€í†¤: ëª¨ë“  ë²¡í„°ìŠ¤í† ì–´ ì¤‘ì•™ ê´€ë¦¬
- ì•± ê¸°ë™ ì‹œ ëª¨ë“  FAISS ì¸ë±ìŠ¤ ì‚¬ì „ ë¡œë“œ
- ì €ì¥ëœ BM25 íŒŒì¼ ë¡œë“œ
- í•´ì‹œ ê¸°ë°˜ íŒŒì¼ ë³€ê²½ ê°ì§€ë¡œ í•«ìŠ¤ì™‘
- ì „ì—­ ê³µìœ ë¡œ í•¸ë“¤ëŸ¬ ê°„ ì¼ê´€ì„± ë³´ì¥
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬

ğŸš¨ ì£¼ìš” ìˆ˜ì •ì‚¬í•­:
âœ… OpenAIEmbeddings ì´ˆê¸°í™” ë°©ì‹ ìˆ˜ì • (í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)
âœ… API í‚¤ ëª…ì‹œì  ì „ë‹¬
âœ… Graceful Degradation ì ìš©
âœ… ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
âœ… BM25 dict ê°ì²´ ì˜¬ë°”ë¥¸ ì¶”ì¶œ (í•µì‹¬ ìˆ˜ì •)
âœ… ë¬¸ì„œ ì¶”ì¶œ ë°©ì‹ ê°œì„  (ì•ˆì „í•œ ì ‘ê·¼)
ğŸ”§ ì¶”ê°€: FAISS ì°¨ì› ê²€ì¦ ë¡œì§ (text-embedding-3-large í†µì¼ ëŒ€ì‘)
"""

import hashlib
import logging
import threading
import time
import pickle
import traceback
from pathlib import Path
from typing import Dict, Optional, Any, List, Tuple
from datetime import datetime
from dataclasses import dataclass, field

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from utils.config import config
from utils.contracts import HandlerType
from utils.textifier import TextChunk

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.vectorstores import FAISS
from rank_bm25 import BM25Okapi
import numpy as np

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ================================================================
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
# ================================================================
try:
    ROOT_DIR = Path(__file__).parent.parent.absolute()
except NameError:
    ROOT_DIR = Path(".").absolute()

# ================================================================
# 1. ë²¡í„°ìŠ¤í† ì–´ ë©”íƒ€ë°ì´í„° í´ë˜ìŠ¤
# ================================================================

@dataclass
class VectorStoreMetadata:
    """
    ë²¡í„°ìŠ¤í† ì–´ ë©”íƒ€ë°ì´í„° ë° ìƒíƒœ ê´€ë¦¬ (BM25 íŒŒì¼ ì§€ì›)
    """
    domain: str
    vectorstore_base_dir: Path
    
    # post_initì—ì„œ ì„¤ì •ë˜ë¯€ë¡œ init=Falseë¡œ ì„¤ì •
    vectorstore_path: Path = field(init=False)
    faiss_path: Path = field(init=False)
    pkl_path: Path = field(init=False)
    bm25_path: Path = field(init=False)
    
    # ëŸ°íƒ€ì„ ì†ì„±
    embeddings: Optional[Any] = None  # OpenAIEmbeddings íƒ€ì… íŒíŠ¸ ì œê±°
    vectorstore: Optional[FAISS] = None
    bm25: Optional[BM25Okapi] = None
    documents: List[TextChunk] = field(default_factory=list)
    last_loaded: Optional[datetime] = None
    load_count: int = 0
    error_count: int = 0
    last_hash: Optional[str] = None
    
    def __post_init__(self):
        # data_ingestion.pyì™€ ë™ì¼í•œ ê²½ë¡œ ë§¤í•‘ ì‚¬ìš©
        self.vectorstore_path = self._get_vectorstore_path()
        self.faiss_path = self.vectorstore_path / f"{self.domain}_index.faiss"
        self.pkl_path = self.vectorstore_path / f"{self.domain}_index.pkl"
        self.bm25_path = self.vectorstore_path / f"{self.domain}_index.bm25"
        
        # âœ… OpenAIEmbeddings ì•ˆì „í•œ ì´ˆê¸°í™”
        self.embeddings = self._init_embeddings()
    
    def _init_embeddings(self) -> Optional[Any]:
        """
        OpenAIEmbeddings ì•ˆì „í•œ ì´ˆê¸°í™” (Streamlit Secrets ì§€ì›)
        """
        try:
            from langchain_openai import OpenAIEmbeddings
            
            # ğŸš¨ í•µì‹¬ ìˆ˜ì •: ë™ì ìœ¼ë¡œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
            api_key = config.get('OPENAI_API_KEY') or config.OPENAI_API_KEY
            
            if not api_key:
                logger.warning(f"âš ï¸ {self.domain} ë„ë©”ì¸: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì„ë² ë”©ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ìµœì†Œí•œì˜ ë§¤ê°œë³€ìˆ˜ë¡œ ì•ˆì „í•œ ì´ˆê¸°í™”
            embeddings = OpenAIEmbeddings(
                api_key=api_key,
                model=config.EMBEDDING_MODEL
            )
            
            logger.debug(f"âœ… {self.domain} ë„ë©”ì¸ìš© OpenAIEmbeddings ì´ˆê¸°í™” ì™„ë£Œ")
            return embeddings
            
        except ImportError as e:
            logger.error(f"âŒ LangChain OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ {self.domain} ë„ë©”ì¸ OpenAIEmbeddings ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None

    
    def _get_vectorstore_path(self) -> Path:
        """ë„ë©”ì¸ë³„ ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ ë§¤í•‘"""
        domain_mapping = {
            "course_satisfaction": "vectorstore_course_satisfaction",
            "subject_satisfaction": "vectorstore_subject_satisfaction", 
            "satisfaction": "vectorstore_unified_satisfaction",
            "publish": "vectorstore_unified_publish",
            "general": "vectorstore_general",
            "cyber": "vectorstore_cyber",
            "notice": "vectorstore_notice",
            "menu": "vectorstore_menu"
        }
        
        vectorstore_dir_name = domain_mapping.get(self.domain, f"vectorstore_{self.domain}")
        return self.vectorstore_base_dir / vectorstore_dir_name
    
    def exists(self) -> bool:
        """í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        return (
            self.faiss_path.exists() and 
            self.pkl_path.exists() and
            self.vectorstore_path.exists()
        )
    
    def get_file_hash(self) -> str:
        """íŒŒì¼ ë³€ê²½ ê°ì§€ìš© í•´ì‹œ ê³„ì‚°"""
        try:
            if not self.exists():
                return ""
                
            hash_content = ""
            
            # FAISS íŒŒì¼ í•´ì‹œ
            if self.faiss_path.exists():
                hash_content += str(self.faiss_path.stat().st_mtime)
            
            # PKL íŒŒì¼ í•´ì‹œ
            if self.pkl_path.exists():
                hash_content += str(self.pkl_path.stat().st_mtime)
                
            # BM25 íŒŒì¼ í•´ì‹œ (ì„ íƒì )
            if self.bm25_path.exists():
                hash_content += str(self.bm25_path.stat().st_mtime)
            
            return hashlib.md5(hash_content.encode()).hexdigest()
        except Exception as e:
            logger.warning(f"âš ï¸ {self.domain} í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return ""

# ================================================================
# 2. IndexManager ì‹±ê¸€í†¤ í´ë˜ìŠ¤
# ================================================================

class IndexManager:
    """
    ëª¨ë“  ë²¡í„°ìŠ¤í† ì–´ë¥¼ ê´€ë¦¬í•˜ëŠ” ì‹±ê¸€í†¤ í´ë˜ìŠ¤ (OpenAI í˜¸í™˜ì„± ìˆ˜ì •)
    """
    _instance = None
    _instance_lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._instance_lock:
                if cls._instance is None:
                    cls._instance = super(IndexManager, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if hasattr(self, '_initialized'):
            return
            
        self.metadata: Dict[str, VectorStoreMetadata] = {}
        
        # âœ… ê¸€ë¡œë²Œ OpenAIEmbeddings ì•ˆì „í•œ ì´ˆê¸°í™”
        self.embeddings = self._init_global_embeddings()
        
        for domain in config.HANDLERS:
            self.metadata[domain] = VectorStoreMetadata(
                domain=domain,
                vectorstore_base_dir=Path(config.VECTORSTORE_DIR)
            )
        
        logger.info(f"ğŸ“Š ì¸ë±ìŠ¤ ë¡œë“œ ìƒíƒœ: {status['loaded_domains']}/{status['total_domains']}ê°œ ì„±ê³µ")
        
        # app.pyì—ì„œ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        return {
            "success": status["loaded_domains"] > 0,
            "loaded_indexes": list(status["domains_detail"].keys()),
            "performance": {
                "load_time": elapsed_time,
                "loaded_domains": status["loaded_domains"],
                "total_domains": status["total_domains"]
            },
            "error": None if status["loaded_domains"] > 0 else "No domains loaded"
        }
        
    except Exception as e:
        logger.error(f"âŒ ì¸ë±ìŠ¤ ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "loaded_indexes": [],
            "performance": {},
            "error": str(e)
        }

def index_health_check() -> Dict[str, Any]:
    """
    IndexManager í—¬ìŠ¤ì²´í¬ (app.py í˜¸í™˜ì„± í•¨ìˆ˜)
    
    Returns:
        Dict[str, Any]: ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´
    """
    try:
        manager = get_index_manager()
        return manager.health_check()
    except Exception as e:
        logger.error(f"âŒ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
        return {
            "total_domains": 0,
            "loaded_domains": 0,
            "failed_domains": 0,
            "domains_detail": {},
            "global_embeddings": False,
            "error": str(e)
        }

# ================================================================
# 4. í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ 
# ================================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸ§ª IndexManager í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # ì‹±ê¸€í†¤ í…ŒìŠ¤íŠ¸
        manager1 = get_index_manager()
        manager2 = get_index_manager()
        assert manager1 is manager2, "ì‹±ê¸€í†¤ íŒ¨í„´ ì‹¤íŒ¨"
        print("âœ… ì‹±ê¸€í†¤ íŒ¨í„´ í…ŒìŠ¤íŠ¸ í†µê³¼")
        
        # ìƒíƒœ ì²´í¬
        status = manager1.health_check()
        print(f"ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ: {status}")
        
        # ğŸ”§ ì¶”ê°€: ì°¨ì› ê²€ì¦ í…ŒìŠ¤íŠ¸
        print("\nğŸ” ë„ë©”ì¸ë³„ ì°¨ì› ê²€ì¦:")
        for domain in config.HANDLERS:
            vectorstore = manager1.get_vectorstore(domain)
            if vectorstore:
                try:
                    stored_dim = getattr(vectorstore.index, 'd', 'Unknown')
                    print(f"  âœ… {domain}: {stored_dim}ì°¨ì›")
                except Exception as e:
                    print(f"  âš ï¸ {domain}: ì°¨ì› í™•ì¸ ì‹¤íŒ¨ - {e}")
            else:
                print(f"  âŒ {domain}: FAISS ë¡œë“œ ì‹¤íŒ¨")
        
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        logger.info(f"ğŸš€ IndexManager ì‹±ê¸€í†¤ ì´ˆê¸°í™” ì™„ë£Œ: {len(self.metadata)}ê°œ ë„ë©”ì¸")
        self.load_all_domains()
        self._initialized = True

    def _init_global_embeddings(self) -> Optional[Any]:
        """
        ê¸€ë¡œë²Œ OpenAIEmbeddings ì•ˆì „í•œ ì´ˆê¸°í™” (Streamlit Secrets ì§€ì›)
        """
        try:
            from langchain_openai import OpenAIEmbeddings
            
            # ğŸš¨ í•µì‹¬ ìˆ˜ì •: ë™ì ìœ¼ë¡œ API í‚¤ ê°€ì ¸ì˜¤ê¸°  
            api_key = config.get('OPENAI_API_KEY') or config.OPENAI_API_KEY
            
            if not api_key:
                logger.warning("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì„ë² ë”© ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
                return None
            
            embeddings = OpenAIEmbeddings(
                api_key=api_key,
                model=config.EMBEDDING_MODEL
            )
            
            logger.info(f"âœ… ê¸€ë¡œë²Œ OpenAIEmbeddings ì´ˆê¸°í™” ì™„ë£Œ: {config.EMBEDDING_MODEL}")
            return embeddings
            
        except ImportError as e:
            logger.error(f"âŒ LangChain OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
            logger.info("ğŸ”„ Graceful Degradation: ì„ë² ë”© ì—†ì´ ê¸°ë³¸ ê¸°ëŠ¥ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
            return None
        except Exception as e:
            logger.error(f"âŒ ê¸€ë¡œë²Œ OpenAIEmbeddings ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.info("ğŸ”„ Graceful Degradation: ì„ë² ë”© ì—†ì´ ê¸°ë³¸ ê¸°ëŠ¥ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
            return None

    def _validate_faiss_dimensions(self, domain: str, vectorstore: FAISS) -> bool:
        """
        ğŸ”§ ì¶”ê°€: FAISS ì¸ë±ìŠ¤ ì°¨ì› ê²€ì¦ (text-embedding-3-large í†µì¼ ëŒ€ì‘)
        
        Args:
            domain: ë„ë©”ì¸ ì´ë¦„
            vectorstore: ë¡œë“œëœ FAISS ë²¡í„°ìŠ¤í† ì–´
            
        Returns:
            bool: ì°¨ì› ì¼ì¹˜ ì—¬ë¶€
        """
        try:
            if not hasattr(vectorstore, 'index'):
                logger.warning(f"âš ï¸ {domain} FAISSì— index ì†ì„±ì´ ì—†ìŒ")
                return False
            
            # ì €ì¥ëœ ì¸ë±ìŠ¤ ì°¨ì›
            stored_dim = getattr(vectorstore.index, 'd', None)
            if stored_dim is None:
                logger.warning(f"âš ï¸ {domain} FAISS ì¸ë±ìŠ¤ì—ì„œ ì°¨ì› ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return False
            
            # í˜„ì¬ ì„¤ì •ëœ ì„ë² ë”© ì°¨ì›
            expected_dim = config.EMBEDDING_DIMENSION
            
            # ì°¨ì› ì¼ì¹˜ í™•ì¸
            if stored_dim != expected_dim:
                logger.error(f"âŒ {domain} FAISS ì°¨ì› ë¶ˆì¼ì¹˜:")
                logger.error(f"   ì €ì¥ëœ ì°¨ì›: {stored_dim}")
                logger.error(f"   í˜„ì¬ ëª¨ë¸ ì°¨ì›: {expected_dim} ({config.EMBEDDING_MODEL})")
                logger.error(f"   í•´ê²°ë°©ë²•: {domain} ë„ë©”ì¸ì„ {config.EMBEDDING_MODEL}ë¡œ ì¬ë¹Œë“œ í•„ìš”")
                return False
            
            logger.info(f"âœ… {domain} FAISS ì°¨ì› ê²€ì¦ ì„±ê³µ: {stored_dim}ì°¨ì›")
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ {domain} FAISS ì°¨ì› ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _load_domain(self, domain: str):
        """
        ë‹¨ì¼ ë„ë©”ì¸ì˜ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œ (ìˆ˜ì •ëœ ë²„ì „)
        """
        meta = self.metadata[domain]
        logger.info(f"ğŸ”„ ë„ë©”ì¸ {domain} ë¡œë“œ ì‹œì‘...")
        logger.debug(f" - FAISS ê²½ë¡œ: {meta.faiss_path}")
        logger.debug(f" - PKL ê²½ë¡œ: {meta.pkl_path}")
        logger.debug(f" - BM25 ê²½ë¡œ: {meta.bm25_path}")
        try:
            if not meta.exists():
                logger.warning(f"âš ï¸ ë„ë©”ì¸ {domain}ì— í•„ìš”í•œ ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                meta.vectorstore = None
                meta.bm25 = None
                return
            start_time = time.time()
            # ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
            embeddings_to_use = meta.embeddings or self.embeddings
            if not embeddings_to_use:
                logger.warning(f"âš ï¸ {domain} ì„ë² ë”© ëª¨ë¸ì´ ì—†ì–´ FAISS ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                meta.vectorstore = None
            else:
                try:
                    vectorstore_path = meta.vectorstore_path.absolute()
                    logger.info(f"ğŸ“ FAISS ë¡œë“œ ì‹œë„:")
                    logger.info(f" ê²½ë¡œ: {vectorstore_path}")
                    logger.info(f" ì¸ë±ìŠ¤ëª…: {domain}_index")
                    meta.vectorstore = FAISS.load_local(
                        str(vectorstore_path),
                        embeddings_to_use,
                        index_name=f"{domain}_index",
                        allow_dangerous_deserialization=True
                    )
                    # --- [ìˆ˜ì •] FAISS ë¡œë“œ í›„ ì¸ë±ìŠ¤ ìœ íš¨ì„± ê²€ì¦ ë¡œì§ ì¶”ê°€ ---
                    if hasattr(meta.vectorstore, 'index') and hasattr(meta.vectorstore.index, 'ntotal'):
                        doc_count = meta.vectorstore.index.ntotal
                        # ì¸ë±ìŠ¤ì— ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ìœ íš¨í•˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ê°„ì£¼
                        if doc_count == 0:
                            logger.warning(f"âš ï¸ {domain} FAISS ì¸ë±ìŠ¤ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤ (ntotal=0). FAISSë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
                            meta.vectorstore = None
                        else:
                            logger.info(f"âœ… {domain} FAISS ë¡œë“œ ì„±ê³µ: {doc_count}ê°œ ë²¡í„°")
                        
                        # ğŸ”§ ì¶”ê°€: ì°¨ì› ê²€ì¦ ë¡œì§
                        if not self._validate_faiss_dimensions(domain, meta.vectorstore):
                            logger.error(f"âŒ {domain} ì°¨ì› ë¶ˆì¼ì¹˜ë¡œ FAISS ë¹„í™œì„±í™”")
                            meta.vectorstore = None
                    else:
                        logger.info(f"âœ… {domain} FAISS ë¡œë“œ ì™„ë£Œ (ntotal í™•ì¸ ë¶ˆê°€)")
                        
                        # ğŸ”§ ì¶”ê°€: ntotal í™•ì¸ ë¶ˆê°€ ì‹œì—ë„ ì°¨ì› ê²€ì¦ ìˆ˜í–‰
                        if not self._validate_faiss_dimensions(domain, meta.vectorstore):
                            logger.error(f"âŒ {domain} ì°¨ì› ë¶ˆì¼ì¹˜ë¡œ FAISS ë¹„í™œì„±í™”")
                            meta.vectorstore = None
                except Exception as faiss_error:
                    logger.error(f"âŒ {domain} FAISS ë¡œë“œ ì‹¤íŒ¨: {faiss_error}")
                    logger.debug(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
                    meta.vectorstore = None
            
            # âœ… ìˆ˜ì •: ì •í™•í•œ ë¬¸ì œ í•´ê²° - í•œê¸€ ì²˜ë¦¬ ë° TextChunk ë³€í™˜ ê°œì„ 
            meta.documents = []
            if meta.vectorstore:
                try:
                    # ë°©ë²• 1: _dict ì§ì ‘ ì ‘ê·¼ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)
                    docstore_docs = []
                    if hasattr(meta.vectorstore, 'docstore') and hasattr(meta.vectorstore.docstore, '_dict'):
                        raw_documents = list(meta.vectorstore.docstore._dict.values())
                        logger.info(f"ğŸ“„ {domain} _dictì—ì„œ {len(raw_documents)}ê°œ ì›ë³¸ ë¬¸ì„œ ë°œê²¬")
                        docstore_docs = raw_documents
                    
                    # ë°©ë²• 2: index_to_docstore_id ë§¤í•‘ ì‚¬ìš© (í´ë°±)
                    elif hasattr(meta.vectorstore, 'index_to_docstore_id'):
                        doc_ids = list(meta.vectorstore.index_to_docstore_id.values())
                        logger.info(f"ğŸ“„ {domain} docstore ID ê°œìˆ˜: {len(doc_ids)}")
                        
                        for doc_id in doc_ids[:100]:  # ìµœëŒ€ 100ê°œë§Œ ë¡œë“œ
                            try:
                                doc = meta.vectorstore.docstore.search(doc_id)
                                if doc and hasattr(doc, 'page_content'):
                                    docstore_docs.append(doc)
                            except Exception as id_error:
                                logger.debug(f"ğŸ“„ {domain} docstore ID {doc_id} ê²€ìƒ‰ ì‹¤íŒ¨: {id_error}")
                                continue
                        
                        logger.info(f"ğŸ“„ {domain} docstoreì—ì„œ {len(docstore_docs)}ê°œ ë¬¸ì„œ ì¶”ì¶œ")
                    
                    # TextChunk ë³€í™˜ (ê°œì„ ëœ ë²„ì „)
                    successful_chunks = 0
                    for i, doc in enumerate(docstore_docs[:50]):  # ìµœëŒ€ 50ê°œë¡œ ì œí•œ
                        try:
                            # Document ê°ì²´ ê²€ì¦
                            if not hasattr(doc, 'page_content'):
                                logger.debug(f"ğŸ“„ {domain} ë¬¸ì„œ {i}: page_content ì†ì„± ì—†ìŒ")
                                continue
                            
                            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬
                            text = doc.page_content
                            if not text or not isinstance(text, str) or not text.strip():
                                logger.debug(f"ğŸ“„ {domain} ë¬¸ì„œ {i}: ë¹ˆ í…ìŠ¤íŠ¸")
                                continue
                            
                            # ë©”íƒ€ë°ì´í„° ì•ˆì „í•œ ì¶”ì¶œ
                            doc_metadata = {}
                            if hasattr(doc, 'metadata') and doc.metadata:
                                try:
                                    doc_metadata = dict(doc.metadata)
                                except Exception as meta_error:
                                    logger.debug(f"ğŸ“„ {domain} ë¬¸ì„œ {i} ë©”íƒ€ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {meta_error}")
                                    doc_metadata = {'conversion_error': str(meta_error)}
                            
                            # TextChunk ìƒì„± (ì•ˆì „í•œ ë°©ì‹)
                            chunk = TextChunk(
                                text=text.strip(),
                                metadata=doc_metadata,
                                source_id=doc_metadata.get('source_id', f'{domain}_{successful_chunks}'),
                                chunk_index=successful_chunks
                            )
                            
                            meta.documents.append(chunk)
                            successful_chunks += 1
                            
                        except Exception as chunk_error:
                            logger.warning(f"ğŸ“„ {domain} ì²­í¬ {i} ë³€í™˜ ì‹¤íŒ¨: {chunk_error}")
                            logger.debug(f"ğŸ“„ {domain} ì²­í¬ ë³€í™˜ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                            continue
                    
                    if meta.documents:
                        logger.info(f"âœ… {domain} ë¬¸ì„œ ë¡œë“œ ì„±ê³µ: {len(meta.documents)}ê°œ (ì›ë³¸: {len(docstore_docs)}ê°œ)")
                        
                        # ì²« ë²ˆì§¸ ë¬¸ì„œ ìƒ˜í”Œ ë¡œê¹… (ë””ë²„ê¹…ìš©)
                        if meta.documents:
                            sample_text = meta.documents[0].text[:100] + "..." if len(meta.documents[0].text) > 100 else meta.documents[0].text
                            logger.debug(f"ğŸ“„ {domain} ìƒ˜í”Œ í…ìŠ¤íŠ¸: {sample_text}")
                    else:
                        logger.warning(f"âš ï¸ {domain} TextChunk ë³€í™˜ ê²°ê³¼ ì—†ìŒ (ì›ë³¸: {len(docstore_docs)}ê°œ)")
                        # í´ë°±: ë”ë¯¸ ë¬¸ì„œ ìƒì„±
                        meta.documents = [TextChunk(
                            text=f"{domain} ë„ë©”ì¸ ì •ë³´ (ë³€í™˜ ì‹¤íŒ¨)",
                            metadata={'domain': domain, 'conversion_failed': True},
                            source_id=f'{domain}_dummy',
                            chunk_index=0
                        )]
                    
                except Exception as doc_error:
                    logger.error(f"âš ï¸ {domain} ë¬¸ì„œ ì¶”ì¶œ ì¹˜ëª…ì  ì‹¤íŒ¨: {doc_error}")
                    logger.debug(f"ë¬¸ì„œ ì¶”ì¶œ ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
                    
                    # í´ë°±: ë”ë¯¸ ë¬¸ì„œ ìƒì„±
                    meta.documents = [TextChunk(
                        text=f"{domain} ë„ë©”ì¸ ì •ë³´ (ì¶”ì¶œ ì‹¤íŒ¨)",
                        metadata={'domain': domain, 'extraction_failed': True},
                        source_id=f'{domain}_dummy',
                        chunk_index=0
                    )]
            
            # âœ… ìˆ˜ì •: BM25 ì˜¬ë°”ë¥¸ ë¡œë“œ ë°©ì‹
            if meta.bm25_path.exists():
                try:
                    with open(meta.bm25_path, 'rb') as f:
                        bm25_data = pickle.load(f)
                        
                        # ğŸš¨ í•µì‹¬ ìˆ˜ì •: BM25 ê°ì²´ íƒ€ì…ë³„ ì˜¬ë°”ë¥¸ ì²˜ë¦¬
                        if isinstance(bm25_data, dict):
                            # base_loader.pyê°€ ì €ì¥í•œ dict í˜•íƒœì—ì„œ ì‹¤ì œ BM25 ê°ì²´ ì¶”ì¶œ
                            if 'bm25_index' in bm25_data:
                                meta.bm25 = bm25_data['bm25_index']
                                logger.info(f"âœ… {domain} BM25 dictì—ì„œ ê°ì²´ ì¶”ì¶œ ì™„ë£Œ")
                            else:
                                logger.warning(f"âš ï¸ {domain} BM25 dictì— 'bm25_index' í‚¤ê°€ ì—†ìŒ")
                                meta.bm25 = None
                        elif isinstance(bm25_data, tuple):
                            # ê¸°ì¡´ íŠœí”Œ í˜•íƒœ ì²˜ë¦¬
                            meta.bm25, _ = bm25_data
                            logger.info(f"âœ… {domain} BM25 íŠœí”Œì—ì„œ ê°ì²´ ì¶”ì¶œ ì™„ë£Œ")
                        elif isinstance(bm25_data, BM25Okapi):
                            # ì§ì ‘ BM25 ê°ì²´ì¸ ê²½ìš°
                            meta.bm25 = bm25_data
                            logger.info(f"âœ… {domain} BM25 ì§ì ‘ ê°ì²´ ë¡œë“œ ì™„ë£Œ")
                        else:
                            logger.warning(f"âš ï¸ {domain} BM25 ì•Œ ìˆ˜ ì—†ëŠ” í˜•íƒœ: {type(bm25_data)}")
                            meta.bm25 = None
                            
                        # BM25 ê°ì²´ íƒ€ì… ê²€ì¦
                        if meta.bm25 and not isinstance(meta.bm25, BM25Okapi):
                            logger.warning(f"âš ï¸ {domain} BM25 ê°ì²´ íƒ€ì… ì˜¤ë¥˜: {type(meta.bm25)}")
                            meta.bm25 = None
                        elif meta.bm25:
                            logger.info(f"âœ… {domain} BM25 ê²€ì¦ ì™„ë£Œ: {type(meta.bm25).__name__}")
                            
                except Exception as bm25_error:
                    logger.warning(f"âš ï¸ {domain} BM25 ë¡œë“œ ì‹¤íŒ¨: {bm25_error}")
                    logger.debug(f"BM25 ë¡œë“œ ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
                    meta.bm25 = None
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            meta.last_loaded = datetime.now()
            meta.load_count += 1
            meta.last_hash = meta.get_file_hash()
            elapsed = time.time() - start_time
            logger.info(f"âœ… {domain} ì „ì²´ ë¡œë“œ ì™„ë£Œ ({elapsed:.2f}ì´ˆ)")
            
        except Exception as e:
            meta.error_count += 1
            logger.error(f"âŒ {domain} ë¡œë“œ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            logger.debug(traceback.format_exc())
            # í´ë°± ì„¤ì •
            meta.vectorstore = None
            meta.bm25 = None
            meta.documents = [TextChunk(
                text=f"{domain} ë¡œë“œ ì˜¤ë¥˜: {str(e)}",
                metadata={'error': str(e), 'domain': domain},
                source_id=f'{domain}_error',
                chunk_index=0
            )]

    def load_all_domains(self):
        """ëª¨ë“  ë„ë©”ì¸ ë¡œë“œ"""
        logger.info(f"ğŸ”„ ì „ì²´ ë„ë©”ì¸ ë¡œë“œ ì‹œì‘: {list(self.metadata.keys())}")
        
        start_time = time.time()
        loaded_count = 0
        
        for domain in self.metadata.keys():
            try:
                self._load_domain(domain)
                loaded_count += 1
            except Exception as e:
                logger.error(f"âŒ ë„ë©”ì¸ {domain} ë¡œë“œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        
        elapsed = time.time() - start_time
        logger.info(f"ğŸ‰ ë„ë©”ì¸ ë¡œë“œ ì™„ë£Œ: {loaded_count}/{len(self.metadata)}ê°œ ì„±ê³µ ({elapsed:.2f}ì´ˆ)")

    def get_vectorstore(self, domain: str) -> Optional[FAISS]:
        """ë„ë©”ì¸ë³„ ë²¡í„°ìŠ¤í† ì–´ íšë“"""
        if domain not in self.metadata:
            logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë„ë©”ì¸: {domain}")
            return None
        
        return self.metadata[domain].vectorstore

    def get_bm25(self, domain: str) -> Optional[BM25Okapi]:
        """ë„ë©”ì¸ë³„ BM25 ì¸ë±ìŠ¤ íšë“"""
        if domain not in self.metadata:
            logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë„ë©”ì¸: {domain}")
            return None
        
        return self.metadata[domain].bm25

    def get_documents(self, domain: str) -> List[TextChunk]:
        """ë„ë©”ì¸ë³„ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ íšë“"""
        if domain not in self.metadata:
            logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë„ë©”ì¸: {domain}")
            return []
        
        return self.metadata[domain].documents

    def health_check(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì²´í¬"""
        status = {
            "total_domains": len(self.metadata),
            "loaded_domains": 0,
            "failed_domains": 0,
            "domains_detail": {},
            "global_embeddings": self.embeddings is not None
        }
        
        for domain, meta in self.metadata.items():
            domain_status = {
                "loaded": meta.vectorstore is not None,
                "bm25_available": meta.bm25 is not None,
                "documents_count": len(meta.documents),
                "load_count": meta.load_count,
                "error_count": meta.error_count,
                "last_loaded": meta.last_loaded.isoformat() if meta.last_loaded else None
            }
            
            if domain_status["loaded"]:
                status["loaded_domains"] += 1
            else:
                status["failed_domains"] += 1
            
            status["domains_detail"][domain] = domain_status
        
        return status

# ================================================================
# 3. ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ íŒ©í„°ë¦¬ ë° í˜¸í™˜ì„± í•¨ìˆ˜
# ================================================================

_index_manager_instance = None

def get_index_manager() -> IndexManager:
    """IndexManager ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ íšë“"""
    global _index_manager_instance
    if _index_manager_instance is None:
        _index_manager_instance = IndexManager()
    return _index_manager_instance

def preload_all_indexes() -> Dict[str, Any]:
    """
    ëª¨ë“  ì¸ë±ìŠ¤ ì‚¬ì „ ë¡œë“œ (app.py í˜¸í™˜ì„± ê°œì„ )
    
    Returns:
        Dict[str, Any]: ë¡œë“œ ê²°ê³¼ ì •ë³´
    """
    logger.info("ğŸš€ ì¸ë±ìŠ¤ ì‚¬ì „ ë¡œë“œ ì‹œì‘")
    start_time = time.time()
    
    try:
        manager = get_index_manager()
        
        # ì¬ë¡œë“œ ì‹¤í–‰
        manager.load_all_domains()
        
        # ìƒíƒœ ì²´í¬
        status = manager.health_check()
        elapsed_time = time.time() - start_time
        
        logger.info(f"
