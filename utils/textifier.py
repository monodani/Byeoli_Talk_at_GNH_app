#!/usr/bin/env python3
"""
ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› RAG ì±—ë´‡ - utils/textifier.py

ë¬¸ì„œ í…ìŠ¤íŠ¸í™” ë° ì²­í‚¹ ìœ í‹¸ë¦¬í‹°:
- PDF, CSV, ì´ë¯¸ì§€ â†’ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ë³€í™˜
- BaseLoader í˜¸í™˜ì„± ë³´ì¥
- ì›ë³¸ ë©”íƒ€ë°ì´í„° ë³´ì¡´
- ê²€ìƒ‰ ìµœì í™”ëœ ì²­í‚¹

ì£¼ìš” í´ë˜ìŠ¤:
- TextChunk: í…ìŠ¤íŠ¸ ì²­í¬ í‘œì¤€ ëª¨ë¸
- DocumentProcessor: ë¬¸ì„œ ì²˜ë¦¬ ê¸°ë³¸ í´ë˜ìŠ¤  
- PDFProcessor: PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
- CSVProcessor: CSV ë°ì´í„° ì²˜ë¦¬
- ImageProcessor: OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ
"""

import csv
import json
import logging
import hashlib
import re
from abc import ABC, abstractmethod
from io import StringIO
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Union
from datetime import datetime, timezone
from dataclasses import dataclass, field

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import tiktoken
from pypdf import PdfReader
from PIL import Image
import pytesseract

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ================================================================
# 1. TextChunk í•µì‹¬ ëª¨ë¸ (ëª¨ë“  ë¡œë”/í•¸ë“¤ëŸ¬ì—ì„œ ì‚¬ìš©)
# ================================================================

@dataclass
class TextChunk:
    """
    í…ìŠ¤íŠ¸ ì²­í¬ í‘œì¤€ ëª¨ë¸
    
    ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬ê¸°ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” í…ìŠ¤íŠ¸ ë‹¨ìœ„
    """
    text: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    source_id: str = ""
    chunk_index: int = 0
    
    def __post_init__(self):
        """í›„ì²˜ë¦¬: ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì„¤ì •"""
        if not self.source_id and 'source_file' in self.metadata:
            self.source_id = self.metadata['source_file']
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ë° í•´ì‹œ ìë™ ê³„ì‚°
        self.metadata.setdefault('text_length', len(self.text))
        self.metadata.setdefault('text_hash', self._calculate_hash())
        self.metadata.setdefault('created_at', datetime.now(timezone.utc).isoformat())
    
    def _calculate_hash(self) -> str:
        """í…ìŠ¤íŠ¸ ë‚´ìš© í•´ì‹œ ê³„ì‚° (ì¤‘ë³µ ì²´í¬ìš©)"""
        return hashlib.md5(self.text.encode('utf-8')).hexdigest()[:16]
    
    def estimate_tokens(self, model: str = "gpt-4o-mini") -> int:
        """í† í° ìˆ˜ ì¶”ì •"""
        try:
            encoding = tiktoken.encoding_for_model(model)
            return len(encoding.encode(self.text))
        except Exception:
            # fallback: ëŒ€ëµ 4ê¸€ì = 1í† í°
            return len(self.text) // 4
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜ (ì§ë ¬í™”ìš©)"""
        return {
            'text': self.text,
            'metadata': self.metadata,
            'source_id': self.source_id,
            'chunk_index': self.chunk_index
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'TextChunk':
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ë³µì›"""
        return cls(
            text=data['text'],
            metadata=data.get('metadata', {}),
            source_id=data.get('source_id', ''),
            chunk_index=data.get('chunk_index', 0)
        )


# ================================================================
# 2. ë¬¸ì„œ ì²˜ë¦¬ê¸° ê¸°ë³¸ í´ë˜ìŠ¤
# ================================================================

class DocumentProcessor(ABC):
    """ë¬¸ì„œ ì²˜ë¦¬ê¸° ê¸°ë³¸ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    def __init__(self, root_dir: Path):
        self.root_dir = Path(root_dir)
        self.chunk_size = 1000  # ê¸°ë³¸ ì²­í¬ í¬ê¸°
        self.chunk_overlap = 100  # ê¸°ë³¸ ì˜¤ë²„ë©
    
    @abstractmethod
    def process(self, file_path: Path, **kwargs) -> List[TextChunk]:
        """íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ TextChunk ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        pass
    
    def _create_chunks(self, text: str, source_id: str, base_metadata: Dict[str, Any]) -> List[TextChunk]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        if not text.strip():
            return []
        
        chunks = []
        
        # ê°„ë‹¨í•œ ì²­í¬ ë¶„í•  (ë¬¸ì¥ ë‹¨ìœ„ ê³ ë ¤)
        sentences = self._split_sentences(text)
        
        current_chunk = ""
        current_tokens = 0
        chunk_index = 0
        
        for sentence in sentences:
            sentence_tokens = len(sentence) // 4  # ê·¼ì‚¬ì¹˜
            
            # ì²­í¬ í¬ê¸° ì´ˆê³¼ ì‹œ ìƒˆ ì²­í¬ ìƒì„±
            if current_tokens + sentence_tokens > self.chunk_size and current_chunk:
                chunk_metadata = {
                    **base_metadata,
                    'chunk_index': chunk_index,
                    'token_count': current_tokens
                }
                
                chunk = TextChunk(
                    text=current_chunk.strip(),
                    metadata=chunk_metadata,
                    source_id=f"{source_id}#chunk{chunk_index}",
                    chunk_index=chunk_index
                )
                chunks.append(chunk)
                
                # ì˜¤ë²„ë© ì²˜ë¦¬
                overlap_text = current_chunk[-self.chunk_overlap:] if len(current_chunk) > self.chunk_overlap else ""
                current_chunk = overlap_text + " " + sentence
                current_tokens = len(current_chunk) // 4
                chunk_index += 1
            else:
                current_chunk += " " + sentence
                current_tokens += sentence_tokens
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk.strip():
            chunk_metadata = {
                **base_metadata,
                'chunk_index': chunk_index,
                'token_count': current_tokens
            }
            
            chunk = TextChunk(
                text=current_chunk.strip(),
                metadata=chunk_metadata,
                source_id=f"{source_id}#chunk{chunk_index}",
                chunk_index=chunk_index
            )
            chunks.append(chunk)
        
        return chunks
    
    def _split_sentences(self, text: str) -> List[str]:
        """ë¬¸ì¥ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë¶„í• """
        # í•œêµ­ì–´/ì˜ì–´ ë¬¸ì¥ ë¶„í•  íŒ¨í„´
        sentence_pattern = r'[.!?]+\s+|[ã€‚ï¼ï¼Ÿ]+\s*'
        sentences = re.split(sentence_pattern, text)
        return [s.strip() for s in sentences if s.strip()]


# ================================================================
# 3. PDF ì²˜ë¦¬ê¸°
# ================================================================

class PDFProcessor(DocumentProcessor):
    """PDF ë¬¸ì„œ ì²˜ë¦¬ê¸°"""
    
    def process(self, file_path: Path, **kwargs) -> List[TextChunk]:
        """PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ TextChunk ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        logger.info(f"Processing PDF: {file_path}")
        
        try:
            return self.process_file(file_path)
        except Exception as e:
            logger.error(f"Failed to process PDF {file_path}: {e}")
            return []
    
    def process_file(self, pdf_path: Path) -> List[TextChunk]:
        """PDF íŒŒì¼ ì²˜ë¦¬ (ê¸°ì¡´ ë©”ì„œë“œ ì´ë¦„ í˜¸í™˜ì„±)"""
        chunks = []
        
        try:
            with open(pdf_path, 'rb') as file:
                reader = PdfReader(file)
                
                logger.info(f"PDF pages: {len(reader.pages)}")
                
                for page_num, page in enumerate(reader.pages, 1):
                    try:
                        text = page.extract_text()
                        if not text.strip():
                            logger.warning(f"Empty text on page {page_num}")
                            continue
                        
                        # í˜ì´ì§€ë³„ ë©”íƒ€ë°ì´í„°
                        page_metadata = {
                            'source_type': 'pdf',
                            'source_file': pdf_path.name,
                            'file_path': str(pdf_path),
                            'page_number': page_num,
                            'total_pages': len(reader.pages)
                        }
                        
                        # í˜ì´ì§€ë³„ ì²­í¬ ìƒì„±
                        source_id = str(pdf_path.relative_to(self.root_dir)) + f"#page{page_num}"
                        page_chunks = self._create_chunks(text, source_id, page_metadata)
                        chunks.extend(page_chunks)
                        
                    except Exception as e:
                        logger.warning(f"Failed to process page {page_num} of {pdf_path}: {e}")
                        continue
                
        except Exception as e:
            logger.error(f"Failed to read PDF {pdf_path}: {e}")
            return []
        
        logger.info(f"Extracted {len(chunks)} chunks from {pdf_path}")
        return chunks
    
    def extract_text_by_page(self, pdf_path: Path) -> List[Dict[str, Any]]:
        """í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë©”íƒ€ë°ì´í„° í¬í•¨)"""
        pages = []
        
        try:
            with open(pdf_path, 'rb') as file:
                reader = PdfReader(file)
                
                for page_num, page in enumerate(reader.pages, 1):
                    text = page.extract_text()
                    
                    pages.append({
                        'page_number': page_num,
                        'text': text,
                        'char_count': len(text),
                        'word_count': len(text.split()) if text else 0
                    })
        
        except Exception as e:
            logger.error(f"Failed to extract PDF pages {pdf_path}: {e}")
        
        return pages


# ================================================================
# 4. CSV ì²˜ë¦¬ê¸° 
# ================================================================

class CSVProcessor(DocumentProcessor):
    """CSV íŒŒì¼ ì²˜ë¦¬ - BaseLoader í˜¸í™˜ì„± ê°•í™”"""
    
    def process(self, file_path: Path, schema_path: Optional[Path] = None) -> List[TextChunk]:
        """CSV íŒŒì¼ì„ í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë³€í™˜ (ì›ë³¸ row_data ë³´ì¡´)"""
        logger.info(f"Processing CSV: {file_path}")
        
        try:
            chunks = []
            source_id = str(file_path.relative_to(self.root_dir))
            
            # ìŠ¤í‚¤ë§ˆ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
            schema = None
            if schema_path and schema_path.exists():
                with open(schema_path, 'r', encoding='utf-8') as f:
                    schema = json.load(f)
            
            # ì¸ì½”ë”© ìë™ ê°ì§€ ë° ë‹¤ì¤‘ ì‹œë„
            content = self._read_csv_with_encoding_detection(file_path)
            if not content:
                logger.error(f"Failed to read CSV file with any encoding: {file_path}")
                return []
            
            # CSV íŒŒì‹±
            reader = csv.DictReader(StringIO(content))
            
            # í—¤ë” ì •ê·œí™”
            fieldnames = [name.strip() for name in reader.fieldnames] if reader.fieldnames else []
            if not fieldnames:
                logger.warning(f"No fieldnames found in CSV: {file_path}")
                return []
            
            logger.info(f"CSV fieldnames: {fieldnames}")
            
            for row_num, row in enumerate(reader, 1):
                try:
                    # ì›ë³¸ row ë°ì´í„° ì •ë¦¬ (strip ì ìš©)
                    clean_row = {key.strip(): str(value).strip() for key, value in row.items() if key}
                    
                    # ë¹ˆ í–‰ ìŠ¤í‚µ
                    if not any(clean_row.values()):
                        continue
                    
                    # í–‰ì„ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    row_text = self._row_to_text(clean_row, fieldnames, schema)
                    if not row_text.strip():
                        logger.warning(f"Empty text generated for row {row_num} in {file_path}")
                        continue
                    
                    # í–‰ë³„ ë©”íƒ€ë°ì´í„° (â˜… í•µì‹¬: ì›ë³¸ row_data ì €ì¥)
                    row_metadata = {
                        "source_type": "csv",
                        "source_file": file_path.name,
                        "row_number": row_num,
                        "fieldnames": fieldnames,
                        "file_path": str(file_path),
                        "row_data": clean_row,  # â˜… ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ì €ì¥
                        "text_representation": row_text  # ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ë„ ë³„ë„ ì €ì¥
                    }
                    
                    # ìŠ¤í‚¤ë§ˆ ì •ë³´ë„ ë©”íƒ€ë°ì´í„°ì— í¬í•¨
                    if schema:
                        row_metadata["schema_applied"] = True
                        row_metadata["schema_file"] = str(schema_path) if schema_path else None
                    
                    # í–‰ ë‹¨ìœ„ë¡œ ì²­í¬í™” (ë³´í†µ 1í–‰ = 1ì²­í¬)
                    row_source_id = f"{source_id}#row{row_num}"
                    
                    chunk = TextChunk(
                        text=row_text,
                        metadata=row_metadata,
                        source_id=row_source_id,
                        chunk_index=row_num - 1
                    )
                    chunks.append(chunk)
                    
                except Exception as e:
                    logger.warning(f"Failed to process row {row_num} of {file_path}: {e}")
                    # ê°œë³„ í–‰ ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰
                    continue
            
            logger.info(f"Extracted {len(chunks)} chunks from {file_path}")
            return chunks
            
        except Exception as e:
            logger.error(f"Failed to process CSV {file_path}: {e}")
            return []
    
    def _read_csv_with_encoding_detection(self, file_path: Path) -> Optional[str]:
        """ë‹¤ì¤‘ ì¸ì½”ë”© ì‹œë„ë¡œ CSV íŒŒì¼ ì½ê¸°"""
        encodings = ['utf-8', 'cp949', 'euc-kr', 'utf-8-sig', 'latin-1']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    content = f.read()
                    logger.info(f"Successfully read {file_path} with encoding: {encoding}")
                    return content
            except UnicodeDecodeError:
                logger.debug(f"Failed to read {file_path} with encoding: {encoding}")
                continue
            except Exception as e:
                logger.warning(f"Unexpected error reading {file_path} with {encoding}: {e}")
                continue
        
        return None
    
    def _row_to_text(self, row: Dict[str, str], fieldnames: List[str], schema: Optional[Dict] = None) -> str:
        """CSV í–‰ì„ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ìŠ¤í‚¤ë§ˆ í™œìš© ê°•í™”)"""
        text_parts = []
        
        for field in fieldnames:
            value = row.get(field, '').strip()
            if not value:
                continue
                
            # ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ í•„ë“œ ì •ë³´ ì¶”ì¶œ
            field_info = None
            if schema and 'properties' in schema:
                field_info = schema['properties'].get(field, {})
            
            # í•„ë“œ ì„¤ëª… í™œìš©í•œ ìì—°ì–´ í˜•íƒœ ìƒì„±
            if field_info and field_info.get('description'):
                # ìŠ¤í‚¤ë§ˆì— ì„¤ëª…ì´ ìˆìœ¼ë©´ ë” ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ ìƒì„±
                description = field_info['description']
                text_parts.append(f"{description}: {value}")
            else:
                # ê¸°ë³¸ í˜•íƒœ
                text_parts.append(f"{field}: {value}")
        
        return " | ".join(text_parts)
    
    def validate_row_against_schema(self, row: Dict[str, str], schema: Dict) -> Tuple[bool, List[str]]:
        """í–‰ ë°ì´í„°ì˜ ìŠ¤í‚¤ë§ˆ ìœ íš¨ì„± ê²€ì¦"""
        errors = []
        
        if 'required' in schema:
            missing_fields = []
            for required_field in schema['required']:
                if required_field not in row or not row[required_field].strip():
                    missing_fields.append(required_field)
            
            if missing_fields:
                errors.append(f"Missing required fields: {missing_fields}")
        
        # íƒ€ì… ê²€ì¦ (ê¸°ë³¸ì ì¸ ì²´í¬)
        if 'properties' in schema:
            for field, value in row.items():
                if field in schema['properties'] and value.strip():
                    field_schema = schema['properties'][field]
                    
                    # ìˆ«ì íƒ€ì… ê²€ì¦
                    if field_schema.get('type') == 'number' or (
                        isinstance(field_schema.get('type'), list) and 'number' in field_schema['type']
                    ):
                        try:
                            float(value)
                        except ValueError:
                            errors.append(f"Field '{field}' should be numeric, got: {value}")
        
        return len(errors) == 0, errors


# ================================================================
# 5. ì´ë¯¸ì§€ ì²˜ë¦¬ê¸° (OCR)
# ================================================================

class ImageProcessor(DocumentProcessor):
    """ì´ë¯¸ì§€ OCR ì²˜ë¦¬ê¸°"""
    
    def __init__(self, root_dir: Path):
        super().__init__(root_dir)
        self.supported_formats = ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']
    
    def process(self, file_path: Path, **kwargs) -> List[TextChunk]:
        """ì´ë¯¸ì§€ íŒŒì¼ì„ OCRë¡œ ì²˜ë¦¬í•˜ì—¬ TextChunk ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        logger.info(f"Processing image with OCR: {file_path}")
        
        try:
            text = self.extract_text_from_image(file_path)
            if not text.strip():
                logger.warning(f"No text extracted from image: {file_path}")
                return []
            
            # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°
            image_metadata = {
                'source_type': 'image',
                'source_file': file_path.name,
                'file_path': str(file_path),
                'ocr_engine': 'tesseract'
            }
            
            # ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
            try:
                with Image.open(file_path) as img:
                    image_metadata.update({
                        'image_width': img.width,
                        'image_height': img.height,
                        'image_mode': img.mode,
                        'image_format': img.format
                    })
            except Exception as e:
                logger.warning(f"Failed to get image info for {file_path}: {e}")
            
            # ì²­í¬ ìƒì„±
            source_id = str(file_path.relative_to(self.root_dir))
            chunks = self._create_chunks(text, source_id, image_metadata)
            
            logger.info(f"Extracted {len(chunks)} chunks from {file_path}")
            return chunks
            
        except Exception as e:
            logger.error(f"Failed to process image {file_path}: {e}")
            return []
    
    def extract_text_from_image(self, image_path: Path, lang: str = 'kor+eng') -> str:
        """ì´ë¯¸ì§€ì—ì„œ OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            with Image.open(image_path) as img:
                # OCR ì‹¤í–‰
                text = pytesseract.image_to_string(img, lang=lang)
                
                # í…ìŠ¤íŠ¸ ì •ë¦¬
                cleaned_text = self._clean_ocr_text(text)
                return cleaned_text
                
        except Exception as e:
            logger.error(f"OCR failed for {image_path}: {e}")
            return ""
    
    def _clean_ocr_text(self, text: str) -> str:
        """OCR ê²°ê³¼ í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        
        # ì—°ì† ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
        cleaned = re.sub(r'\s+', ' ', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (ê¸°ë³¸ì ì¸ ê²ƒë§Œ)
        cleaned = re.sub(r'[^\w\sê°€-í£.,!?()/-]', '', cleaned)
        
        return cleaned.strip()


# ================================================================
# 6. íŒ©í† ë¦¬ í´ë˜ìŠ¤
# ================================================================

class ProcessorFactory:
    """ë¬¸ì„œ ì²˜ë¦¬ê¸° íŒ©í† ë¦¬"""
    
    @staticmethod
    def get_processor(file_path: Path, root_dir: Path) -> Optional[DocumentProcessor]:
        """íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ì ì ˆí•œ ì²˜ë¦¬ê¸° ë°˜í™˜"""
        ext = file_path.suffix.lower()
        
        if ext == '.pdf':
            return PDFProcessor(root_dir)
        elif ext == '.csv':
            return CSVProcessor(root_dir)
        elif ext in ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']:
            return ImageProcessor(root_dir)
        else:
            logger.warning(f"Unsupported file type: {ext}")
            return None
    
    @staticmethod
    def process_file(file_path: Path, root_dir: Path, **kwargs) -> List[TextChunk]:
        """íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬"""
        processor = ProcessorFactory.get_processor(file_path, root_dir)
        if processor:
            return processor.process(file_path, **kwargs)
        return []


# ================================================================
# 7. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ================================================================

def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ê°„ë‹¨ ë²„ì „)"""
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
        if end < len(text):
            # ë‹¤ìŒ ë¬¸ì¥ ëì„ ì°¾ê¸°
            sentence_end = max(
                text.rfind('.', start, end),
                text.rfind('!', start, end),
                text.rfind('?', start, end)
            )
            
            if sentence_end > start + chunk_size // 2:
                end = sentence_end + 1
        
        chunks.append(text[start:end])
        start = max(end - overlap, start + 1)  # ë¬´í•œë£¨í”„ ë°©ì§€
    
    return chunks


def estimate_reading_time(text: str, wpm: int = 200) -> float:
    """í…ìŠ¤íŠ¸ ì½ê¸° ì‹œê°„ ì¶”ì • (ë¶„)"""
    word_count = len(text.split())
    return word_count / wpm


def extract_key_phrases(text: str, max_phrases: int = 10) -> List[str]:
    """í•µì‹¬ êµ¬ë¬¸ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)"""
    # í•œêµ­ì–´ ëª…ì‚¬ íŒ¨í„´ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
    korean_noun_pattern = r'[ê°€-í£]{2,}'
    english_word_pattern = r'[A-Za-z]{3,}'
    
    korean_words = re.findall(korean_noun_pattern, text)
    english_words = re.findall(english_word_pattern, text)
    
    # ë¹ˆë„ ê³„ì‚°
    from collections import Counter
    word_freq = Counter(korean_words + english_words)
    
    # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
    return [word for word, _ in word_freq.most_common(max_phrases)]


# ================================================================
# 8. í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# ================================================================

def test_textifier():
    """textifier ëª¨ë“ˆ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Textifier ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # TextChunk í…ŒìŠ¤íŠ¸
    chunk = TextChunk(
        text="ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
        metadata={"test": True},
        source_id="test.txt"
    )
    
    print(f"âœ… TextChunk ìƒì„±: {chunk.source_id}")
    print(f"   í† í° ìˆ˜: {chunk.estimate_tokens()}")
    print(f"   í•´ì‹œ: {chunk.metadata['text_hash']}")
    
    # ì²­í‚¹ í…ŒìŠ¤íŠ¸
    long_text = "ì´ê²ƒì€ ê¸´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. " * 100
    chunks = chunk_text(long_text, chunk_size=200, overlap=50)
    print(f"âœ… í…ìŠ¤íŠ¸ ì²­í‚¹: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    keywords = extract_key_phrases("ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì›ì—ì„œ êµìœ¡ê³¼ì • ë§Œì¡±ë„ ì¡°ì‚¬ë¥¼ ì‹¤ì‹œí–ˆìŠµë‹ˆë‹¤.")
    print(f"âœ… í‚¤ì›Œë“œ ì¶”ì¶œ: {keywords}")
    
    print("ğŸ‰ Textifier í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    # ëª¨ë“ˆ ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
    test_textifier()
    
    print("\nâœ… utils/textifier.py ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
    print("ğŸ“¦ ì‚¬ìš© ê°€ëŠ¥í•œ í´ë˜ìŠ¤:")
    print("   - TextChunk: í…ìŠ¤íŠ¸ ì²­í¬ ëª¨ë¸")
    print("   - DocumentProcessor: ë¬¸ì„œ ì²˜ë¦¬ê¸° ê¸°ë³¸ í´ë˜ìŠ¤")
    print("   - PDFProcessor: PDF ì²˜ë¦¬")
    print("   - CSVProcessor: CSV ì²˜ë¦¬") 
    print("   - ImageProcessor: ì´ë¯¸ì§€ OCR ì²˜ë¦¬")
    print("   - ProcessorFactory: ìë™ ì²˜ë¦¬ê¸° ì„ íƒ")
