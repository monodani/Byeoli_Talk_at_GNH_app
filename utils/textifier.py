#!/usr/bin/env python3
"""
ë²¼ë¦¬í†¡@ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› (ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› RAG ì±—ë´‡) - utils/textifier.py (Pydantic v2 í˜¸í™˜)

ë¬¸ì„œ í…ìŠ¤íŠ¸í™” ë° ì²­í‚¹ ìœ í‹¸ë¦¬í‹°:
- PDF, CSV, ì´ë¯¸ì§€ â†’ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ë³€í™˜
- BaseLoader í˜¸í™˜ì„± ë³´ì¥
- ì›ë³¸ ë©”íƒ€ë°ì´í„° ë³´ì¡´
- ê²€ìƒ‰ ìµœì í™”ëœ ì²­í‚¹

ì£¼ìš” í´ë˜ìŠ¤:
- TextChunk: í…ìŠ¤íŠ¸ ì²­í¬ í‘œì¤€ ëª¨ë¸ (Pydantic v2)
- DocumentProcessor: ë¬¸ì„œ ì²˜ë¦¬ ê¸°ë³¸ í´ë˜ìŠ¤  
- PDFProcessor: PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
- CSVProcessor: CSV ë°ì´í„° ì²˜ë¦¬
- ImageProcessor: OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ
"""

import csv
import json
import logging
import hashlib
import re
from abc import ABC, abstractmethod
from io import StringIO
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Union
from datetime import datetime, timezone

# ğŸ”§ Pydantic v2 import
from pydantic import BaseModel, Field, ConfigDict

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import tiktoken
from pypdf import PdfReader
from PIL import Image
import pytesseract

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ================================================================
# 1. TextChunk í•µì‹¬ ëª¨ë¸ (Pydantic v2 ì™„ì „ í˜¸í™˜)
# ================================================================

class TextChunk(BaseModel):
    """
    í…ìŠ¤íŠ¸ ì²­í¬ í‘œì¤€ ëª¨ë¸ (Pydantic v2 í˜¸í™˜)
    
    ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬ê¸°ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” í…ìŠ¤íŠ¸ ë‹¨ìœ„
    """
    model_config = ConfigDict(
        extra='forbid',
        str_strip_whitespace=True,
        validate_assignment=True
    )
    
    text: str = Field(..., min_length=1, max_length=50000, description="ì²­í¬ í…ìŠ¤íŠ¸")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="ì²­í¬ ë©”íƒ€ë°ì´í„°")
    source_id: str = Field(default="", description="ì†ŒìŠ¤ ì‹ë³„ì")
    chunk_index: int = Field(default=0, ge=0, description="ì²­í¬ ì¸ë±ìŠ¤")
    
    def model_post_init(self, __context: Any) -> None:
        """Pydantic v2 ë°©ì‹ í›„ì²˜ë¦¬: ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì„¤ì •"""
        if not self.source_id and 'source_file' in self.metadata:
            self.source_id = self.metadata['source_file']
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ë° í•´ì‹œ ìë™ ê³„ì‚°
        self.metadata.setdefault('text_length', len(self.text))
        self.metadata.setdefault('text_hash', self._calculate_hash())
        self.metadata.setdefault('created_at', datetime.now(timezone.utc).isoformat())
    
    def _calculate_hash(self) -> str:
        """í…ìŠ¤íŠ¸ ë‚´ìš© í•´ì‹œ ê³„ì‚° (ì¤‘ë³µ ì²´í¬ìš©)"""
        return hashlib.md5(self.text.encode('utf-8')).hexdigest()[:16]
    
    def estimate_tokens(self, model: str = "gpt-4o-mini") -> int:
        """í† í° ìˆ˜ ì¶”ì •"""
        try:
            encoding = tiktoken.encoding_for_model(model)
            return len(encoding.encode(self.text))
        except Exception:
            # fallback: ëŒ€ëµ 4ê¸€ì = 1í† í°
            return len(self.text) // 4
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜ (ì§ë ¬í™”ìš©)"""
        return {
            'text': self.text,
            'metadata': self.metadata,
            'source_id': self.source_id,
            'chunk_index': self.chunk_index
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'TextChunk':
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ë³µì›"""
        return cls(
            text=data['text'],
            metadata=data.get('metadata', {}),
            source_id=data.get('source_id', ''),
            chunk_index=data.get('chunk_index', 0)
        )
    
    def __hash__(self) -> int:
        """í•´ì‹œ ê°’ ê³„ì‚° (ìºì‹œ í‚¤ ìš©ë„)"""
        return hash((self.text, str(sorted(self.metadata.items()))))
    
    def get_source_id(self) -> str:
        """ì†ŒìŠ¤ ID ë°˜í™˜"""
        return self.source_id or self.metadata.get('source_id', 'unknown')
    
    def get_cache_ttl(self) -> int:
        """ìºì‹œ TTL ë°˜í™˜"""
        return self.metadata.get('cache_ttl', 86400)  # ê¸°ë³¸ 24ì‹œê°„


# ================================================================
# 2. ë¬¸ì„œ ì²˜ë¦¬ê¸° ê¸°ë³¸ í´ë˜ìŠ¤
# ================================================================

class DocumentProcessor(ABC):
    """ë¬¸ì„œ ì²˜ë¦¬ê¸° ê¸°ë³¸ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    def __init__(self, root_dir: Optional[Path] = None):
        """root_dir ë§¤ê°œë³€ìˆ˜ì— None ê¸°ë³¸ê°’ ì œê³µ"""
        self.root_dir = root_dir if root_dir is not None else Path(".")
        self.chunk_size = 1000  # ê¸°ë³¸ ì²­í¬ í¬ê¸°
        self.chunk_overlap = 100  # ê¸°ë³¸ ì˜¤ë²„ë©
    
    @abstractmethod
    def process(self, file_path: Path, **kwargs) -> List[TextChunk]:
        """íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ TextChunk ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        pass
    
    def _create_chunks(self, text: str, source_id: str, base_metadata: Dict[str, Any]) -> List[TextChunk]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        if not text.strip():
            return []
        
        chunks = []
        
        # ê°„ë‹¨í•œ ì²­í¬ ë¶„í•  (ë¬¸ì¥ ë‹¨ìœ„ ê³ ë ¤)
        sentences = self._split_sentences(text)
        
        current_chunk = ""
        current_tokens = 0
        chunk_index = 0
        
        for sentence in sentences:
            sentence_tokens = len(sentence) // 4  # ê·¼ì‚¬ì¹˜
            
            # ì²­í¬ í¬ê¸° ì´ˆê³¼ ì‹œ ìƒˆ ì²­í¬ ìƒì„±
            if current_tokens + sentence_tokens > self.chunk_size and current_chunk:
                chunk_metadata = {
                    **base_metadata,
                    'chunk_index': chunk_index,
                    'token_count': current_tokens
                }
                
                chunk = TextChunk(
                    text=current_chunk.strip(),
                    metadata=chunk_metadata,
                    source_id=f"{source_id}#chunk{chunk_index}",
                    chunk_index=chunk_index
                )
                chunks.append(chunk)
                
                # ì˜¤ë²„ë© ì²˜ë¦¬
                overlap_text = current_chunk[-self.chunk_overlap:] if len(current_chunk) > self.chunk_overlap else ""
                current_chunk = overlap_text + " " + sentence
                current_tokens = len(current_chunk) // 4
                chunk_index += 1
            else:
                current_chunk += " " + sentence
                current_tokens += sentence_tokens
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk.strip():
            chunk_metadata = {
                **base_metadata,
                'chunk_index': chunk_index,
                'token_count': current_tokens
            }
            
            chunk = TextChunk(
                text=current_chunk.strip(),
                metadata=chunk_metadata,
                source_id=f"{source_id}#chunk{chunk_index}",
                chunk_index=chunk_index
            )
            chunks.append(chunk)
        
        return chunks
    
    def _split_sentences(self, text: str) -> List[str]:
        """ë¬¸ì¥ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë¶„í• """
        # í•œêµ­ì–´/ì˜ì–´ ë¬¸ì¥ ë¶„í•  íŒ¨í„´
        sentence_pattern = r'[.!?]+\s+|[ã€‚ï¼ï¼Ÿ]+\s*'
        sentences = re.split(sentence_pattern, text)
        return [s.strip() for s in sentences if s.strip()]


# ================================================================
# 3. PDF ì²˜ë¦¬ê¸°
# ================================================================

class PDFProcessor(DocumentProcessor):
    """PDF ë¬¸ì„œ ì²˜ë¦¬ê¸°"""
    
    def process(self, file_path: Path, **kwargs) -> List[TextChunk]:
        """PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ TextChunk ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        chunks = []
        
        try:
            reader = PdfReader(file_path)
            total_pages = len(reader.pages)
            
            logger.info(f"ğŸ“„ PDF ì²˜ë¦¬ ì‹œì‘: {file_path.name} ({total_pages} í˜ì´ì§€)")
            
            full_text = ""
            page_texts = []
            
            # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for page_num, page in enumerate(reader.pages, 1):
                try:
                    page_text = page.extract_text()
                    if page_text.strip():
                        page_texts.append(page_text)
                        full_text += f"\n\n=== í˜ì´ì§€ {page_num} ===\n\n{page_text}"
                    
                except Exception as e:
                    logger.warning(f"í˜ì´ì§€ {page_num} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
            
            if not full_text.strip():
                logger.warning(f"PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                return chunks
            
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
            base_metadata = {
                'source_type': 'pdf',
                'source_file': file_path.name,
                'file_path': str(file_path),
                'total_pages': total_pages,
                'processing_date': datetime.now().isoformat()
            }
            
            # ì²­í‚¹ ì²˜ë¦¬
            source_id = f"pdf/{file_path.stem}"
            chunks = self._create_chunks(full_text, source_id, base_metadata)
            
            logger.info(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
        except Exception as e:
            logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
        
        return chunks


# ================================================================
# 4. CSV ì²˜ë¦¬ê¸°
# ================================================================

class CSVProcessor(DocumentProcessor):
    """CSV ë°ì´í„° ì²˜ë¦¬ê¸°"""
    
    def process(self, file_path: Path, **kwargs) -> List[TextChunk]:
        """CSV íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ TextChunk ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        chunks = []
        
        try:
            logger.info(f"ğŸ“Š CSV ì²˜ë¦¬ ì‹œì‘: {file_path.name}")
            
            # ì¸ì½”ë”© ê°ì§€ ë° ì½ê¸°
            import chardet
            with open(file_path, 'rb') as f:
                raw_data = f.read()
                encoding = chardet.detect(raw_data)['encoding']
            
            # CSV ì½ê¸°
            with open(file_path, 'r', encoding=encoding, newline='') as f:
                csv_reader = csv.DictReader(f)
                fieldnames = csv_reader.fieldnames
                
                if not fieldnames:
                    logger.warning(f"CSV íŒŒì¼ì— í—¤ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                    return chunks
                
                source_id = f"csv/{file_path.stem}"
                
                for row_num, row in enumerate(csv_reader, 1):
                    try:
                        # ë¹ˆ í–‰ ê±´ë„ˆë›°ê¸°
                        if not any(str(v).strip() for v in row.values()):
                            continue
                        
                        # í–‰ì„ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        row_text = self._row_to_text(row, fieldnames)
                        if not row_text.strip():
                            continue
                        
                        # í–‰ë³„ ë©”íƒ€ë°ì´í„°
                        row_metadata = {
                            "source_type": "csv",
                            "source_file": file_path.name,
                            "row_number": row_num,
                            "fieldnames": fieldnames,
                            "file_path": str(file_path),
                            "row_data": dict(row),  # ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ì €ì¥
                            "processing_date": datetime.now().isoformat()
                        }
                        
                        # í–‰ ë‹¨ìœ„ë¡œ ì²­í¬í™”
                        chunk = TextChunk(
                            text=row_text,
                            metadata=row_metadata,
                            source_id=f"{source_id}#row{row_num}",
                            chunk_index=row_num - 1
                        )
                        chunks.append(chunk)
                        
                    except Exception as e:
                        logger.warning(f"í–‰ {row_num} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
            
            logger.info(f"âœ… CSV ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
        except Exception as e:
            logger.error(f"âŒ CSV ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
        
        return chunks
    
    def _row_to_text(self, row: Dict[str, Any], fieldnames: List[str]) -> str:
        """CSV í–‰ì„ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        text_parts = []
        
        for field in fieldnames:
            value = str(row.get(field, '')).strip()
            if value and value.lower() not in ['', 'nan', 'null', 'none']:
                text_parts.append(f"{field}: {value}")
        
        return " | ".join(text_parts)


# ================================================================
# 5. ì´ë¯¸ì§€ ì²˜ë¦¬ê¸° (OCR)
# ================================================================

class ImageProcessor(DocumentProcessor):
    """ì´ë¯¸ì§€ OCR ì²˜ë¦¬ê¸°"""
    
    def process(self, file_path: Path, **kwargs) -> List[TextChunk]:
        """ì´ë¯¸ì§€ íŒŒì¼ì„ OCR ì²˜ë¦¬í•˜ì—¬ TextChunk ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        chunks = []
        
        try:
            logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì‹œì‘: {file_path.name}")
            
            # ì´ë¯¸ì§€ ì—´ê¸°
            image = Image.open(file_path)
            
            # OCR ì‹¤í–‰ (í•œêµ­ì–´ + ì˜ì–´)
            ocr_text = pytesseract.image_to_string(
                image, 
                lang='kor+eng',
                config='--psm 6'  # ê· ì¼í•œ í…ìŠ¤íŠ¸ ë¸”ë¡ ê°€ì •
            )
            
            if not ocr_text.strip():
                logger.warning(f"ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                return chunks
            
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
            base_metadata = {
                'source_type': 'image',
                'source_file': file_path.name,
                'file_path': str(file_path),
                'image_size': image.size,
                'ocr_confidence': 'estimated',
                'processing_date': datetime.now().isoformat()
            }
            
            # ì²­í‚¹ ì²˜ë¦¬
            source_id = f"image/{file_path.stem}"
            chunks = self._create_chunks(ocr_text, source_id, base_metadata)
            
            logger.info(f"âœ… OCR ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
        
        return chunks


# ================================================================
# 6. ì²˜ë¦¬ê¸° íŒ©í† ë¦¬
# ================================================================

class ProcessorFactory:
    """íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ì²˜ë¦¬ê¸°ë¥¼ ì„ íƒí•˜ëŠ” íŒ©í† ë¦¬"""
    
    _processors = {
        '.pdf': PDFProcessor,
        '.csv': CSVProcessor,
        '.png': ImageProcessor,
        '.jpg': ImageProcessor,
        '.jpeg': ImageProcessor,
        '.bmp': ImageProcessor,
        '.tiff': ImageProcessor
    }
    
    @classmethod
    def get_processor(cls, file_path: Path, root_dir: Optional[Path] = None) -> Optional[DocumentProcessor]:
        """íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì ì ˆí•œ ì²˜ë¦¬ê¸° ë°˜í™˜"""
        suffix = file_path.suffix.lower()
        
        if suffix in cls._processors:
            processor_class = cls._processors[suffix]
            return processor_class(root_dir)
        
        logger.warning(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {suffix}")
        return None
    
    @classmethod
    def process_file(cls, file_path: Path, root_dir: Optional[Path] = None, **kwargs) -> List[TextChunk]:
        """íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ TextChunk ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        processor = cls.get_processor(file_path, root_dir)
        if processor:
            return processor.process(file_path, **kwargs)
        return []


# ================================================================
# 7. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ================================================================

def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ í¬ê¸°ë¡œ ì²­í‚¹"""
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
        if end < len(text):
            # ë‹¤ìŒ ë¬¸ì¥ ë ì°¾ê¸°
            next_sentence = text.find('.', end)
            if next_sentence != -1 and next_sentence - end < 100:
                end = next_sentence + 1
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        start = end - overlap
    
    return chunks


def extract_key_phrases(text: str, max_phrases: int = 10) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP í•„ìš”)
    words = re.findall(r'\b\w+\b', text)
    
    # ë¶ˆìš©ì–´ ì œê±° (ê°„ë‹¨í•œ ë²„ì „)
    stopwords = {'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ê°€', 'ì€', 'ëŠ”', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ'}
    keywords = [word for word in words if len(word) > 1 and word not in stopwords]
    
    # ë¹ˆë„ ê³„ì‚° ë° ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
    from collections import Counter
    word_counts = Counter(keywords)
    
    return [word for word, count in word_counts.most_common(max_phrases)]


# ================================================================
# 8. í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# ================================================================

def test_textifier():
    """textifier ëª¨ë“ˆ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Textifier ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # TextChunk í…ŒìŠ¤íŠ¸
    chunk = TextChunk(
        text="ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
        metadata={"test": True},
        source_id="test.txt"
    )
    
    print(f"âœ… TextChunk ìƒì„±: {chunk.source_id}")
    print(f"   í† í° ìˆ˜: {chunk.estimate_tokens()}")
    print(f"   í•´ì‹œ: {chunk.metadata.get('text_hash', 'N/A')}")
    
    # ì²­í‚¹ í…ŒìŠ¤íŠ¸
    long_text = "ì´ê²ƒì€ ê¸´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. " * 100
    chunks = chunk_text(long_text, chunk_size=200, overlap=50)
    print(f"âœ… í…ìŠ¤íŠ¸ ì²­í‚¹: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    keywords = extract_key_phrases("ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì›ì—ì„œ êµìœ¡ê³¼ì • ë§Œì¡±ë„ ì¡°ì‚¬ë¥¼ ì‹¤ì‹œí–ˆìŠµë‹ˆë‹¤.")
    print(f"âœ… í‚¤ì›Œë“œ ì¶”ì¶œ: {keywords}")
    
    print("ğŸ‰ Textifier í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    # ëª¨ë“ˆ ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
    test_textifier()
    
    print("\nâœ… utils/textifier.py ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
    print("ğŸ“¦ ì‚¬ìš© ê°€ëŠ¥í•œ í´ë˜ìŠ¤:")
    print("   - TextChunk: í…ìŠ¤íŠ¸ ì²­í¬ ëª¨ë¸ (Pydantic v2)")
    print("   - DocumentProcessor: ë¬¸ì„œ ì²˜ë¦¬ê¸° ê¸°ë³¸ í´ë˜ìŠ¤")
    print("   - PDFProcessor: PDF ì²˜ë¦¬")
    print("   - CSVProcessor: CSV ì²˜ë¦¬") 
    print("   - ImageProcessor: ì´ë¯¸ì§€ OCR ì²˜ë¦¬")
    print("   - ProcessorFactory: ìë™ ì²˜ë¦¬ê¸° ì„ íƒ")
