#!/usr/bin/env python3
"""
ë²¼ë¦¬í†¡@ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› (ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› RAG ì±—ë´‡) - base_handler ê¸°ë°˜ í´ë˜ìŠ¤ (OpenAI í˜¸í™˜ì„± ìˆ˜ì •)

ëª¨ë“  í•¸ë“¤ëŸ¬ì˜ ê³µí†µ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤:
- IndexManager ì‹±ê¸€í†¤ì„ í™œìš©í•œ ì¤‘ì•™ì§‘ì¤‘ì‹ ë²¡í„°ìŠ¤í† ì–´ ê´€ë¦¬
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (FAISS + BM25 + RRF)
- ì»¨í”¼ë˜ìŠ¤ ê³„ì‚° (ë‹¨ìˆœí™”ëœ ë²„ì „)
- Citation ì¶”ì¶œ (200ì snippet)
- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (50í† í° ë‹¨ìœ„)
- í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ (QueryRequest â†’ HandlerResponse)

ğŸš¨ ì£¼ìš” ìˆ˜ì •ì‚¬í•­:
âœ… OpenAIEmbeddings ì´ˆê¸°í™” ë°©ì‹ ìˆ˜ì • (í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)
âœ… Graceful Degradation ê°•í™”
âœ… ì—ëŸ¬ ì²˜ë¦¬ ê°œì„ 
"""

import logging
import time
import hashlib
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Dict, Any, Optional, Iterator, Tuple
from datetime import datetime

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from utils.config import config
from utils.contracts import QueryRequest, HandlerResponse, Citation
from utils.textifier import TextChunk
from utils.index_manager import get_index_manager

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from rank_bm25 import BM25Okapi

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class base_handler(ABC):
    """
    ëª¨ë“  í•¸ë“¤ëŸ¬ì˜ ê¸°ë°˜ í´ë˜ìŠ¤ (OpenAI í˜¸í™˜ì„± ìˆ˜ì •)
    
    ì£¼ìš” ê¸°ëŠ¥:
    - IndexManagerë¥¼ í†µí•œ ì¤‘ì•™ì§‘ì¤‘ì‹ ë²¡í„°ìŠ¤í† ì–´ ê´€ë¦¬
    - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (FAISS + BM25 + RRF)
    - ì»¨í”¼ë˜ìŠ¤ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸
    - Citation ìë™ ì¶”ì¶œ
    - ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›
    """
    
    def __init__(self, domain: str, index_name: str, confidence_threshold: float):
        """
        base_handler ì´ˆê¸°í™” (OpenAI í˜¸í™˜ì„± ìˆ˜ì •)
        
        Args:
            domain: ë„ë©”ì¸ ì´ë¦„ (ì˜ˆ: "satisfaction")
            index_name: ë²¡í„°ìŠ¤í† ì–´ ì¸ë±ìŠ¤ ì´ë¦„
            confidence_threshold: ì»¨í”¼ë˜ìŠ¤ ì„ê³„ê°’ (Î¸)
        """
        self.domain = domain
        self.index_name = index_name
        self.confidence_threshold = confidence_threshold
        
        # IndexManager ì‹±ê¸€í†¤ ì°¸ì¡°
        self.index_manager = get_index_manager()
        
        # âœ… OpenAIEmbeddings ì•ˆì „í•œ ì´ˆê¸°í™” (í˜¸í™˜ì„± ìˆ˜ì •)
        self.embeddings = self._init_embeddings()
        
        # LLM ì´ˆê¸°í™”
        self.llm = self._init_llm()
        
        logger.info(f"âœ¨ {domain.upper()} Handler ì´ˆê¸°í™” ì™„ë£Œ (Î¸={confidence_threshold}, IndexManager í†µí•©)")
    
    def _init_embeddings(self) -> Optional[Any]:
        """
        OpenAIEmbeddings ì•ˆì „í•œ ì´ˆê¸°í™” (í˜¸í™˜ì„± ìˆ˜ì •)
        """
        try:
            from langchain_openai import OpenAIEmbeddings
            
            api_key = config.OPENAI_API_KEY
            if not api_key:
                logger.warning(f"âš ï¸ {self.domain} í•¸ë“¤ëŸ¬: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì„ë² ë”©ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            embeddings = OpenAIEmbeddings(
                api_key=api_key,
                model=config.EMBEDDING_MODEL
            )
            
            logger.debug(f"âœ… {self.domain} í•¸ë“¤ëŸ¬ OpenAIEmbeddings ì´ˆê¸°í™” ì™„ë£Œ")
            return embeddings
            
        except ImportError as e:
            logger.error(f"âŒ {self.domain} í•¸ë“¤ëŸ¬: LangChain OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ {self.domain} í•¸ë“¤ëŸ¬: OpenAIEmbeddings ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    def _init_llm(self) -> Optional[ChatOpenAI]:
        """
        ChatOpenAI LLM ì•ˆì „í•œ ì´ˆê¸°í™”
        """
        try:
            api_key = config.OPENAI_API_KEY
            if not api_key:
                logger.warning(f"âš ï¸ {self.domain} í•¸ë“¤ëŸ¬: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            llm = ChatOpenAI(
                api_key=api_key,
                temperature=0.1,
                model="gpt-4o-mini",
                streaming=True
            )
            
            logger.debug(f"âœ… {self.domain} í•¸ë“¤ëŸ¬ ChatOpenAI ì´ˆê¸°í™” ì™„ë£Œ")
            return llm
            
        except Exception as e:
            logger.error(f"âŒ {self.domain} í•¸ë“¤ëŸ¬: ChatOpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    def _get_vectorstore(self) -> Optional[FAISS]:
        """
        IndexManagerë¥¼ í†µí•´ ë²¡í„°ìŠ¤í† ì–´ íšë“ (ì¤‘ì•™ì§‘ì¤‘ì‹)
        
        Returns:
            Optional[FAISS]: ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None
        """
        try:
            vectorstore = self.index_manager.get_vectorstore(self.domain)
            if vectorstore is None:
                logger.warning(f"âš ï¸ {self.domain} ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
            return vectorstore
        except Exception as e:
            logger.error(f"âŒ {self.domain} ë²¡í„°ìŠ¤í† ì–´ íšë“ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_bm25(self) -> Optional[BM25Okapi]:
        """
        IndexManagerë¥¼ í†µí•´ BM25 ì¸ë±ìŠ¤ íšë“
        
        Returns:
            Optional[BM25Okapi]: BM25 ì¸ë±ìŠ¤ ë˜ëŠ” None
        """
        try:
            bm25 = self.index_manager.get_bm25(self.domain)
            if bm25 is None:
                logger.warning(f"âš ï¸ {self.domain} BM25 ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
            return bm25
        except Exception as e:
            logger.error(f"âŒ {self.domain} BM25 ì¸ë±ìŠ¤ íšë“ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_documents(self) -> List[TextChunk]:
        """
        IndexManagerë¥¼ í†µí•´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ íšë“
        
        Returns:
            List[TextChunk]: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            documents = self.index_manager.get_documents(self.domain)
            return documents
        except Exception as e:
            logger.error(f"âŒ {self.domain} ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ íšë“ ì‹¤íŒ¨: {e}")
            return []
    
    def _hybrid_search(self, query: str, k: int = 5) -> List[Tuple[TextChunk, float]]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (FAISS + BM25 + RRF) - ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Tuple[TextChunk, float]]: (ë¬¸ì„œ, ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            vectorstore = self._get_vectorstore()
            bm25 = self._get_bm25()
            documents = self._get_documents()
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ê²€ìƒ‰ ë°©ë²• í™•ì¸
            faiss_available = vectorstore is not None and self.embeddings is not None
            bm25_available = bm25 is not None and len(documents) > 0
            
            if not faiss_available and not bm25_available:
                logger.warning(f"âš ï¸ {self.domain}: ê²€ìƒ‰ ì¸ë±ìŠ¤ê°€ ì—†ì–´ ë¹ˆ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
                return []
            
            # FAISS ê²€ìƒ‰ (ì˜ë¯¸ë¡ ì  ê²€ìƒ‰)
            faiss_results = []
            if faiss_available:
                try:
                    faiss_docs = vectorstore.similarity_search_with_score(query, k=k*2)
                    for doc, score in faiss_docs:
                        # LangChain Document â†’ TextChunk ë³€í™˜
                        text_chunk = TextChunk(
                            text=doc.page_content,
                            source_id=doc.metadata.get('source_id', 'unknown'),
                            chunk_id=doc.metadata.get('chunk_id', 0),
                            metadata=doc.metadata
                        )
                        faiss_results.append((text_chunk, 1.0 - score))  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                    logger.debug(f"âœ… {self.domain} FAISS ê²€ìƒ‰: {len(faiss_results)}ê°œ ê²°ê³¼")
                except Exception as e:
                    logger.warning(f"âš ï¸ {self.domain} FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    faiss_results = []
            
            # BM25 ê²€ìƒ‰ (í‚¤ì›Œë“œ ê²€ìƒ‰)
            bm25_results = []
            if bm25_available:
                try:
                    # ì¿¼ë¦¬ í† í°í™”
                    query_tokens = query.lower().split()
                    
                    # BM25 ì ìˆ˜ ê³„ì‚°
                    bm25_scores = bm25.get_scores(query_tokens)
                    
                    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ ë° ìƒìœ„ k*2 ì„ íƒ
                    scored_docs = list(zip(documents, bm25_scores))
                    scored_docs.sort(key=lambda x: x[1], reverse=True)
                    
                    # ìŠ¬ë¼ì´ì‹± ì˜¤ë¥˜ ìˆ˜ì •: min ì‚¬ìš©
                    top_k = min(k*2, len(scored_docs))
                    bm25_results = scored_docs[:top_k]
                    
                    logger.debug(f"âœ… {self.domain} BM25 ê²€ìƒ‰: {len(bm25_results)}ê°œ ê²°ê³¼")
                except Exception as e:
                    logger.warning(f"âš ï¸ {self.domain} BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    bm25_results = []
            
            # RRF (Reciprocal Rank Fusion) ê²°í•©
            combined_results = self._rrf_combine(faiss_results, bm25_results, k=k)
            
            logger.debug(f"ğŸ” {self.domain} í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(combined_results)}ê°œ ìµœì¢… ê²°ê³¼")
            return combined_results
            
        except Exception as e:
            logger.error(f"âŒ {self.domain} í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _rrf_combine(self, faiss_results: List[Tuple[TextChunk, float]], 
                     bm25_results: List[Tuple[TextChunk, float]], 
                     k: int = 60) -> List[Tuple[TextChunk, float]]:
        """
        RRF (Reciprocal Rank Fusion)ë¡œ ê²€ìƒ‰ ê²°ê³¼ ê²°í•©
        """
        try:
            doc_scores = {}
            
            # FAISS ê²°ê³¼ ì²˜ë¦¬
            for rank, (doc, score) in enumerate(faiss_results, 1):
                # âœ… getattrë¡œ ì•ˆì „í•˜ê²Œ chunk_id ì ‘ê·¼
                doc_id = f"{doc.source_id}_{getattr(doc, 'chunk_id', rank)}"
                rrf_score = 1.0 / (k + rank)
                
                if doc_id not in doc_scores:
                    doc_scores[doc_id] = {'doc': doc, 'score': 0}
                doc_scores[doc_id]['score'] += rrf_score * 0.6  # FAISS ê°€ì¤‘ì¹˜ 60%
            
            # BM25 ê²°ê³¼ ì²˜ë¦¬
            for rank, (doc, score) in enumerate(bm25_results, 1):
                # âœ… getattrë¡œ ì•ˆì „í•˜ê²Œ chunk_id ì ‘ê·¼
                doc_id = f"{doc.source_id}_{getattr(doc, 'chunk_id', rank)}"
                rrf_score = 1.0 / (k + rank)
                
                if doc_id not in doc_scores:
                    doc_scores[doc_id] = {'doc': doc, 'score': 0}
                doc_scores[doc_id]['score'] += rrf_score * 0.4  # BM25 ê°€ì¤‘ì¹˜ 40%
            
            # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
            if doc_scores:
                combined = [(info['doc'], info['score']) for info in doc_scores.values()]
                combined.sort(key=lambda x: x[1], reverse=True)
                return combined[:min(k, len(combined))]
            else:
                return []
                
        except Exception as e:
            logger.error(f"âŒ RRF ê²°í•© ì‹¤íŒ¨: {e}")
            # í´ë°±: FAISS ê²°ê³¼ ìš°ì„  ë°˜í™˜
            return faiss_results[:k] if faiss_results else bm25_results[:k]

    
    def _calculate_confidence(self, query: str, retrieved_docs: List[Tuple[TextChunk, float]], 
                            response: str) -> float:
        """
        ì»¨í”¼ë˜ìŠ¤ ì ìˆ˜ ê³„ì‚° (ë‹¨ìˆœí™”ëœ ë²„ì „)
        """
        try:
            if not retrieved_docs:
                return 0.0
            
            # ê¸°ë³¸ ì ìˆ˜: ê²€ìƒ‰ëœ ë¬¸ì„œì˜ í‰ê·  ìœ ì‚¬ë„
            avg_similarity = sum(score for _, score in retrieved_docs) / len(retrieved_docs)
            
            # ì¿¼ë¦¬ ê¸¸ì´ ë³´ì • (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ì¿¼ë¦¬ëŠ” ì‹ ë¢°ë„ ê°ì†Œ)
            query_length_factor = min(1.0, len(query.split()) / 10.0)
            
            # ì‘ë‹µ ê¸¸ì´ ë³´ì • (ë„ˆë¬´ ì§§ì€ ì‘ë‹µì€ ì‹ ë¢°ë„ ê°ì†Œ)
            response_length_factor = min(1.0, len(response.split()) / 20.0)
            
            confidence = avg_similarity * query_length_factor * response_length_factor
            
            return min(1.0, max(0.0, confidence))
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì»¨í”¼ë˜ìŠ¤ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5  # ê¸°ë³¸ê°’
    
    def _extract_citations(self, retrieved_docs: List[Tuple[TextChunk, float]]) -> List[Citation]:
        """
        Citation ìë™ ì¶”ì¶œ (200ì snippet)
        """
        citations = []
        
        try:
            for doc, score in retrieved_docs:
                # snippet ê¸¸ì´ ì œí•œ (200ì)
                snippet = doc.text[:200] + "..." if len(doc.text) > 200 else doc.text
                
                citation = Citation(
                    source_id=doc.source_id,
                    text=snippet,  # âœ… text í•„ë“œ ì¶”ê°€
                    relevance_score=score,  # âœ… relevance_score í•„ë“œ ì¶”ê°€
                    snippet=snippet,
                    metadata=doc.metadata
                )
                citations.append(citation)
                
        except Exception as e:
            logger.warning(f"âš ï¸ Citation ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return citations
    
    def _stream_response(self, prompt: str) -> Iterator[str]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (50í† í° ë‹¨ìœ„) - ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
        """
        try:
            if not self.llm:
                logger.warning(f"âš ï¸ {self.domain}: LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
                yield "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ëª¨ë¸ì´ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                return
            
            response_stream = self.llm.stream(prompt)
            buffer = ""
            
            for chunk in response_stream:
                if hasattr(chunk, 'content'):
                    buffer += chunk.content
                    
                    # 50í† í° ë‹¨ìœ„ë¡œ ë°©ì¶œ
                    if len(buffer.split()) >= 50:
                        yield buffer
                        buffer = ""
            
            # ë‚¨ì€ ë‚´ìš© ë°©ì¶œ
            if buffer.strip():
                yield buffer
                
        except Exception as e:
            logger.error(f"âŒ {self.domain} ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹¤íŒ¨: {e}")
            yield f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    @abstractmethod
    def _generate_prompt(self, query: str, retrieved_docs: List[Tuple[TextChunk, float]]) -> str:
        """
        ë„ë©”ì¸ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„± (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)
        """
        pass
    
    def handle(self, request: QueryRequest) -> HandlerResponse:
        """
        í‘œì¤€ í•¸ë“¤ëŸ¬ ì²˜ë¦¬ ë¡œì§ (ëª¨ë“  í•˜ìœ„ í•¸ë“¤ëŸ¬ì—ì„œ ê³µí†µ ì‚¬ìš©)
        """
        start_time = time.time()
        
        try:
            # QueryRequestì—ì„œ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            query = getattr(request, 'query', None) or getattr(request, 'text', '')
            
            # 1. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
            retrieved_docs = self._hybrid_search(query, k=5)
            
            # 2. í”„ë¡¬í”„íŠ¸ ìƒì„± (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)
            prompt = self._generate_prompt(query, retrieved_docs)
            
            # 3. LLM ì‘ë‹µ ìƒì„±
            if not self.llm:
                logger.warning(f"âš ï¸ {self.domain}: LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ fallback ì‘ë‹µ ì‚¬ìš©")
                return self._fallback_response(query, "LLM ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìˆ˜ì§‘
            response_chunks = list(self._stream_response(prompt))
            answer = "".join(response_chunks)
            
            # 4. ì»¨í”¼ë˜ìŠ¤ ê³„ì‚°
            confidence = self._calculate_confidence(query, retrieved_docs, answer)
            
            # 5. Citation ì¶”ì¶œ
            citations = self._extract_citations(retrieved_docs)
            
            # 6. ì‘ë‹µ ìƒì„±
            elapsed_ms = (time.time() - start_time) * 1000
            
            return HandlerResponse(
                answer=answer,
                confidence=confidence,
                handler_id=self.domain,
                citations=citations,
                elapsed_ms=elapsed_ms,
                success=confidence >= self.confidence_threshold,
                diagnostics={
                    "handler": self.domain,
                    "search_results": len(retrieved_docs),
                    "confidence_threshold": self.confidence_threshold,
                    "timestamp": datetime.now().isoformat()
                }
            )
            
        except Exception as e:
            logger.error(f"âŒ {self.domain} í•¸ë“¤ëŸ¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._fallback_response(query, str(e))

    
    def _fallback_response(self, query: str, error_msg: str = "") -> HandlerResponse:
        """
        í´ë°± ì‘ë‹µ ìƒì„± (ì—ëŸ¬ ìƒí™© ëŒ€ì‘)
        """
        fallback_text = (
            f"ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
            f"ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› ê´€ë ¨ ì§ˆë¬¸ì´ì‹œë¼ë©´ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì‹œê±°ë‚˜, "
            f"ë‹´ë‹¹ìì—ê²Œ ì§ì ‘ ë¬¸ì˜í•´ ì£¼ì„¸ìš”."
        )
        
        if error_msg:
            fallback_text += f"\n\n(ê¸°ìˆ ì  ì˜¤ë¥˜: {error_msg})"
        
        return HandlerResponse(
            answer=fallback_text,
            confidence=0.1,
            handler_id=self.domain,
            citations=[],            # âœ… citations í•„ë“œ ì‚¬ìš©
            elapsed_ms=0,           # âœ… elapsed_ms í•„ë“œ ì¶”ê°€
            success=False,          # âœ… success í•„ë“œ ì¶”ê°€
            diagnostics={           # âœ… diagnostics í•„ë“œ ì‚¬ìš©
                "handler": self.domain,
                "fallback": True,
                "error": error_msg,
                "timestamp": datetime.now().isoformat()
            }
        )
