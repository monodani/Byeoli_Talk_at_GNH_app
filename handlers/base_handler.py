#!/usr/bin/env python3
"""
ë²¼ë¦¬í†¡@ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› - ìˆ˜ì •ëœ base_handler ê¸°ë°˜ í´ë˜ìŠ¤

ì£¼ìš” ìˆ˜ì •ì‚¬í•­:
âœ… í´ë˜ìŠ¤ëª… ì¼ì¹˜ ë¬¸ì œ í•´ê²° (base_handlerë¡œ í†µì¼)
âœ… ë¶ˆí•„ìš”í•œ ë³„ì¹­ ì œê±°
âœ… ì„í¬íŠ¸ ì—ëŸ¬ í•´ê²°
âœ… íƒ€ì… íŒíŒ… ê°œì„ 
"""

import logging
import time
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Iterator, Tuple
from datetime import datetime

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from utils.config import config
from utils.contracts import QueryRequest, HandlerResponse, Citation
from utils.textifier import TextChunk
from utils.index_manager import get_index_manager

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from rank_bm25 import BM25Okapi

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class base_handler(ABC):
    """
    ëª¨ë“  í•¸ë“¤ëŸ¬ì˜ ê¸°ë°˜ í´ë˜ìŠ¤
    
    ì£¼ìš” ê¸°ëŠ¥:
    - IndexManagerë¥¼ í†µí•œ ì¤‘ì•™ì§‘ì¤‘ì‹ ë²¡í„°ìŠ¤í† ì–´ ê´€ë¦¬
    - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (FAISS + BM25 + RRF)
    - ì»¨í”¼ë˜ìŠ¤ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸
    - Citation ìë™ ì¶”ì¶œ
    - ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›
    """
    
    def __init__(self, domain: str, index_name: str, confidence_threshold: float):
        """
        BaseHandler ì´ˆê¸°í™”
        
        Args:
            domain: ë„ë©”ì¸ ì´ë¦„ (ì˜ˆ: "satisfaction")
            index_name: ë²¡í„°ìŠ¤í† ì–´ ì¸ë±ìŠ¤ ì´ë¦„
            confidence_threshold: ì»¨í”¼ë˜ìŠ¤ ì„ê³„ê°’ (Î¸)
        """
        self.domain = domain
        self.index_name = index_name
        self.confidence_threshold = confidence_threshold
        
        # IndexManager ì‹±ê¸€í†¤ ì°¸ì¡°
        self.index_manager = get_index_manager()
        
        # OpenAI ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.embeddings = self._init_embeddings()
        self.llm = self._init_llm()
        
        logger.info(f"âœ¨ {domain.upper()} Handler ì´ˆê¸°í™” ì™„ë£Œ (Î¸={confidence_threshold})")
    
    def _init_embeddings(self) -> Optional[OpenAIEmbeddings]:
        """
        OpenAIEmbeddings ì•ˆì „í•œ ì´ˆê¸°í™”
        """
        try:
            # configì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
            api_key = getattr(config, 'OPENAI_API_KEY', None)
            if not api_key:
                api_key = config.get('OPENAI_API_KEY') if hasattr(config, 'get') else None
            
            if not api_key:
                logger.warning(f"âš ï¸ {self.domain}: OPENAI_API_KEY ì—†ìŒ")
                return None
            
            embeddings = OpenAIEmbeddings(
                api_key=api_key,
                model=getattr(config, 'EMBEDDING_MODEL', 'text-embedding-3-small')
            )
            
            logger.debug(f"âœ… {self.domain} OpenAIEmbeddings ì´ˆê¸°í™” ì„±ê³µ")
            return embeddings
            
        except Exception as e:
            logger.error(f"âŒ {self.domain} OpenAIEmbeddings ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    def _init_llm(self) -> Optional[ChatOpenAI]:
        """
        ChatOpenAI LLM ì•ˆì „í•œ ì´ˆê¸°í™”
        """
        try:
            # configì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
            api_key = getattr(config, 'OPENAI_API_KEY', None)
            if not api_key:
                api_key = config.get('OPENAI_API_KEY') if hasattr(config, 'get') else None
            
            if not api_key:
                logger.warning(f"âš ï¸ {self.domain}: OPENAI_API_KEY ì—†ìŒ")
                return None
            
            llm = ChatOpenAI(
                api_key=api_key,
                temperature=0.1,
                model="gpt-4o-mini",
                streaming=True
            )
            
            logger.debug(f"âœ… {self.domain} ChatOpenAI ì´ˆê¸°í™” ì„±ê³µ")
            return llm
            
        except Exception as e:
            logger.error(f"âŒ {self.domain} ChatOpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    def _get_vectorstore(self) -> Optional[FAISS]:
        """
        IndexManagerë¥¼ í†µí•´ ë²¡í„°ìŠ¤í† ì–´ íšë“
        """
        try:
            vectorstore = self.index_manager.get_vectorstore(self.domain)
            if vectorstore is None:
                logger.warning(f"âš ï¸ {self.domain} ë²¡í„°ìŠ¤í† ì–´ ì—†ìŒ")
            return vectorstore
        except Exception as e:
            logger.error(f"âŒ {self.domain} ë²¡í„°ìŠ¤í† ì–´ íšë“ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_bm25(self) -> Optional[BM25Okapi]:
        """
        IndexManagerë¥¼ í†µí•´ BM25 ì¸ë±ìŠ¤ íšë“
        """
        try:
            bm25 = self.index_manager.get_bm25(self.domain)
            if bm25 is None:
                logger.warning(f"âš ï¸ {self.domain} BM25 ì¸ë±ìŠ¤ ì—†ìŒ")
            return bm25
        except Exception as e:
            logger.error(f"âŒ {self.domain} BM25 íšë“ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_documents(self) -> List[TextChunk]:
        """
        IndexManagerë¥¼ í†µí•´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ íšë“
        """
        try:
            documents = self.index_manager.get_documents(self.domain)
            return documents if documents else []
        except Exception as e:
            logger.error(f"âŒ {self.domain} ë¬¸ì„œ íšë“ ì‹¤íŒ¨: {e}")
            return []
    
    def _hybrid_search(self, query: str, k: int = 5) -> List[Tuple[TextChunk, float]]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (FAISS + BM25 + RRF)
        """
        try:
            vectorstore = self._get_vectorstore()
            bm25 = self._get_bm25()
            documents = self._get_documents()
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ê²€ìƒ‰ ë°©ë²• í™•ì¸
            faiss_available = (vectorstore is not None and getattr(vectorstore, "embedding_function", None) is not none)
            bm25_available = bm25 is not None and len(documents) > 0
            
            if not faiss_available and not bm25_available:
                logger.warning(f"âš ï¸ {self.domain}: ê²€ìƒ‰ ì¸ë±ìŠ¤ ì—†ìŒ")
                return []
            
            # FAISS ê²€ìƒ‰
            faiss_results = []
            if faiss_available:
                try:
                    faiss_docs = vectorstore.similarity_search_with_score(query, k=k*2)
                    for doc, score in faiss_docs:
                        text_chunk = TextChunk(
                            text=doc.page_content,
                            source_id=doc.metadata.get('source_id', 'unknown'),
                            chunk_index=doc.metadata.get('chunk_index', 0),
                            metadata=doc.metadata
                        )
                        faiss_results.append((text_chunk, 1.0 - score))
                except Exception as e:
                    logger.warning(f"âš ï¸ FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # BM25 ê²€ìƒ‰
            bm25_results = []
            if bm25_available:
                try:
                    query_tokens = query.lower().split()
                    bm25_scores = bm25.get_scores(query_tokens)
                    
                    scored_docs = list(zip(documents, bm25_scores))
                    scored_docs.sort(key=lambda x: x[1], reverse=True)
                    
                    top_k = min(k*2, len(scored_docs))
                    bm25_results = scored_docs[:top_k]
                except Exception as e:
                    logger.warning(f"âš ï¸ BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # RRF ê²°í•©
            combined_results = self._rrf_combine(faiss_results, bm25_results, k=k)
            
            logger.debug(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(combined_results)}ê°œ ê²°ê³¼")
            return combined_results
            
        except Exception as e:
            logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _rrf_combine(self, faiss_results: List[Tuple[TextChunk, float]], 
                     bm25_results: List[Tuple[TextChunk, float]], 
                     k: int = 60) -> List[Tuple[TextChunk, float]]:
        """
        RRF (Reciprocal Rank Fusion)ë¡œ ê²€ìƒ‰ ê²°ê³¼ ê²°í•©
        """
        try:
            doc_scores = {}
            
            # FAISS ê²°ê³¼ ì²˜ë¦¬
            for rank, (doc, score) in enumerate(faiss_results, 1):
                doc_id = f"{doc.source_id}_{getattr(doc, 'chunk_index', 0)}"
                rrf_score = 1.0 / (k + rank)
                
                if doc_id not in doc_scores:
                    doc_scores[doc_id] = {'doc': doc, 'score': 0}
                doc_scores[doc_id]['score'] += rrf_score * 0.6
            
            # BM25 ê²°ê³¼ ì²˜ë¦¬
            for rank, (doc, score) in enumerate(bm25_results, 1):
                doc_id = f"{doc.source_id}_{getattr(doc, 'chunk_index', 0)}"
                rrf_score = 1.0 / (k + rank)
                
                if doc_id not in doc_scores:
                    doc_scores[doc_id] = {'doc': doc, 'score': 0}
                doc_scores[doc_id]['score'] += rrf_score * 0.4
            
            # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
            if doc_scores:
                combined = [(info['doc'], info['score']) for info in doc_scores.values()]
                combined.sort(key=lambda x: x[1], reverse=True)
                return combined[:min(k, len(combined))]
            
            return []
            
        except Exception as e:
            logger.error(f"âŒ RRF ê²°í•© ì‹¤íŒ¨: {e}")
            return faiss_results[:k] if faiss_results else bm25_results[:k]
    
    def _calculate_confidence(self, query: str, retrieved_docs: List[Tuple[TextChunk, float]], 
                            response: str) -> float:
        """
        ì»¨í”¼ë˜ìŠ¤ ì ìˆ˜ ê³„ì‚°
        """
        try:
            if not retrieved_docs:
                return 0.0
            
            # ê¸°ë³¸ ì ìˆ˜: ê²€ìƒ‰ëœ ë¬¸ì„œì˜ í‰ê·  ìœ ì‚¬ë„
            avg_similarity = sum(score for _, score in retrieved_docs) / len(retrieved_docs)
            
            # ì¿¼ë¦¬ ê¸¸ì´ ë³´ì •
            query_length_factor = min(1.0, len(query.split()) / 10.0)
            
            # ì‘ë‹µ ê¸¸ì´ ë³´ì •
            response_length_factor = min(1.0, len(response.split()) / 20.0)
            
            confidence = avg_similarity * query_length_factor * response_length_factor
            
            return min(1.0, max(0.0, confidence))
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì»¨í”¼ë˜ìŠ¤ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _extract_citations(self, retrieved_docs: List[Tuple[TextChunk, float]]) -> List[Citation]:
        """
        Citation ìë™ ì¶”ì¶œ
        """
        citations = []
        
        try:
            for doc, score in retrieved_docs:
                snippet = doc.text[:200] + "..." if len(doc.text) > 200 else doc.text
                
                citation = Citation(
                    source_id=doc.source_id,
                    text=snippet,
                    relevance_score=score,
                    snippet=snippet,
                    metadata=doc.metadata
                )
                citations.append(citation)
                
        except Exception as e:
            logger.warning(f"âš ï¸ Citation ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return citations
    
    def _stream_response(self, prompt: str) -> Iterator[str]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        """
        try:
            if not self.llm:
                yield "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                return
            
            response_stream = self.llm.stream(prompt)
            buffer = ""
            
            for chunk in response_stream:
                if hasattr(chunk, 'content'):
                    buffer += chunk.content
                    
                    # 50í† í° ë‹¨ìœ„ë¡œ ë°©ì¶œ
                    if len(buffer.split()) >= 50:
                        yield buffer
                        buffer = ""
            
            # ë‚¨ì€ ë‚´ìš© ë°©ì¶œ
            if buffer.strip():
                yield buffer
                
        except Exception as e:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹¤íŒ¨: {e}")
            yield f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    @abstractmethod
    def _generate_prompt(self, query: str, retrieved_docs: List[Tuple[TextChunk, float]]) -> str:
        """
        ë„ë©”ì¸ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„± (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)
        """
        pass
    
    def handle(self, request: QueryRequest) -> HandlerResponse:
        """
        í‘œì¤€ í•¸ë“¤ëŸ¬ ì²˜ë¦¬ ë¡œì§
        """
        start_time = time.time()
        
        try:
            # QueryRequestì—ì„œ ì¿¼ë¦¬ ì¶”ì¶œ
            query = getattr(request, 'query', None) or getattr(request, 'text', '')
            
            # 1. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            retrieved_docs = self._hybrid_search(query, k=5)
            
            # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._generate_prompt(query, retrieved_docs)
            
            # 3. LLM ì‘ë‹µ ìƒì„±
            if not self.llm:
                return self._fallback_response(query, "LLM ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            response_chunks = list(self._stream_response(prompt))
            answer = "".join(response_chunks)
            
            # 4. ì»¨í”¼ë˜ìŠ¤ ê³„ì‚°
            confidence = self._calculate_confidence(query, retrieved_docs, answer)
            
            # 5. Citation ì¶”ì¶œ
            citations = self._extract_citations(retrieved_docs)
            
            # 6. ì‘ë‹µ ìƒì„±
            elapsed_ms = (time.time() - start_time) * 1000
            
            return HandlerResponse(
                answer=answer,
                confidence=confidence,
                domain=self.domain,
                citations=citations,
                elapsed_ms=elapsed_ms,
                success=confidence >= self.confidence_threshold,
                diagnostics={
                    "handler": self.domain,
                    "search_results": len(retrieved_docs),
                    "confidence_threshold": self.confidence_threshold,
                    "timestamp": datetime.now().isoformat()
                }
            )
            
        except Exception as e:
            logger.error(f"âŒ {self.domain} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._fallback_response(query, str(e))
    
    def _fallback_response(self, query: str, error_msg: str = "") -> HandlerResponse:
        """
        í´ë°± ì‘ë‹µ ìƒì„±
        """
        fallback_text = (
            f"ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
            f"ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› ê´€ë ¨ ì§ˆë¬¸ì´ì‹œë¼ë©´ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
        )
        
        if error_msg:
            fallback_text += f"\n\n(ê¸°ìˆ ì  ì˜¤ë¥˜: {error_msg})"
        
        return HandlerResponse(
            answer=fallback_text,
            confidence=0.1,
            domain=self.domain,
            citations=[],
            elapsed_ms=0,
            success=False,
            diagnostics={
                "handler": self.domain,
                "fallback": True,
                "error": error_msg,
                "timestamp": datetime.now().isoformat()
            }
        )
