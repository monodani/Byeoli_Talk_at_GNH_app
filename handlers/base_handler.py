#!/usr/bin/env python3
"""
ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› RAG ì±—ë´‡ - base_handler ê¸°ë°˜ í´ë˜ìŠ¤

ëª¨ë“  í•¸ë“¤ëŸ¬ì˜ ê³µí†µ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤:
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (FAISS + BM25 + RRF)
- ì»¨í”¼ë˜ìŠ¤ ê³„ì‚° (ë‹¨ìˆœí™”ëœ ë²„ì „)
- Citation ì¶”ì¶œ (200ì snippet)
- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (50í† í° ë‹¨ìœ„)
- í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ (QueryRequest â†’ HandlerResponse)
"""

import logging
import time
import hashlib
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Dict, Any, Optional, Iterator, Tuple
from datetime import datetime

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from utils.config import config
from utils.contracts import QueryRequest, HandlerResponse, Citation
from utils.textifier import TextChunk

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from rank_bm25 import BM25Okapi

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class base_handler(ABC):
    """
    ëª¨ë“  í•¸ë“¤ëŸ¬ì˜ ê¸°ë°˜ í´ë˜ìŠ¤
    
    ì£¼ìš” ê¸°ëŠ¥:
    - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (FAISS + BM25 + RRF)
    - ì»¨í”¼ë˜ìŠ¤ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸
    - Citation ìë™ ì¶”ì¶œ
    - ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›
    """
    
    def __init__(self, domain: str, index_name: str, confidence_threshold: float):
        """
        base_Handler ì´ˆê¸°í™”
        
        Args:
            domain: ë„ë©”ì¸ ì´ë¦„ (ì˜ˆ: "satisfaction")
            index_name: ë²¡í„°ìŠ¤í† ì–´ ì¸ë±ìŠ¤ ì´ë¦„
            confidence_threshold: ì»¨í”¼ë˜ìŠ¤ ì„ê³„ê°’ (Î¸)
        """
        self.domain = domain
        self.index_name = index_name
        self.confidence_threshold = confidence_threshold
        
        # ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ ì„¤ì •
        self.vectorstore_dir = config.ROOT_DIR / "vectorstores" / f"vectorstore_{domain}" if domain != "satisfaction" else config.ROOT_DIR / "vectorstores" / "vectorstore_unified_satisfaction"
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embeddings = OpenAIEmbeddings()
        
        # LLM ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            temperature=0.1,
            model="gpt-4o-mini",
            streaming=True
        )
        
        # ë²¡í„°ìŠ¤í† ì–´ ë° BM25 (ì§€ì—° ë¡œë”©)
        self.vectorstore = None
        self.bm25 = None
        self.documents = None
        
        logger.info(f"âœ¨ {domain.upper()} Handler ì´ˆê¸°í™” ì™„ë£Œ (Î¸={confidence_threshold})")
    
    def _load_vectorstore(self) -> bool:
        """ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ (ì§€ì—° ë¡œë”©)"""
        if self.vectorstore is not None:
            return True
            
        try:
            logger.info(f"ğŸ“š {self.domain} ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
            
            self.vectorstore = FAISS.load_local(
                folder_path=str(self.vectorstore_dir),
                embeddings=self.embeddings,
                allow_dangerous_deserialization=True,
                index_name=self.index_name
            )
            
            # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (BM25ìš©)
            self.documents = []
            for i in range(len(self.vectorstore.docstore._dict)):
                doc = self.vectorstore.docstore._dict.get(str(i))
                if doc:
                    self.documents.append(doc.page_content)
            
            # BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
            if self.documents:
                tokenized_docs = [doc.split() for doc in self.documents]
                self.bm25 = BM25Okapi(tokenized_docs)
                logger.info(f"âœ… BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
            
            logger.info(f"âœ… {self.domain} ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ {self.domain} ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def hybrid_search(self, query: str, k: int = 10) -> List[Tuple[str, float, Dict[str, Any]]]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (FAISS + BM25 + RRF)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            List of (text, score, metadata) tuples
        """
        if not self._load_vectorstore():
            return []
        
        try:
            # 1. FAISS ê²€ìƒ‰
            faiss_results = self.vectorstore.similarity_search_with_score(query, k=k)
            faiss_docs = [(doc.page_content, score, doc.metadata) for doc, score in faiss_results]
            
            # 2. BM25 ê²€ìƒ‰
            bm25_docs = []
            if self.bm25:
                tokenized_query = query.split()
                bm25_scores = self.bm25.get_scores(tokenized_query)
                
                # ìƒìœ„ kê°œ BM25 ê²°ê³¼ ì„ íƒ
                top_indices = np.argsort(bm25_scores)[-k:][::-1]
                for idx in top_indices:
                    if idx < len(self.documents):
                        # ë©”íƒ€ë°ì´í„° ì°¾ê¸°
                        doc_id = str(idx)
                        metadata = {}
                        if doc_id in self.vectorstore.docstore._dict:
                            metadata = self.vectorstore.docstore._dict[doc_id].metadata
                        
                        bm25_docs.append((self.documents[idx], bm25_scores[idx], metadata))
            
            # 3. RRF ì¬ë­í‚¹ (ë‹¨ìˆœí™”)
            # FAISS 0.6 + BM25 0.4 ê°€ì¤‘ì¹˜
            combined_results = {}
            
            # FAISS ê²°ê³¼ ì²˜ë¦¬ (ì ìˆ˜ ì •ê·œí™”)
            max_faiss_score = max([score for _, score, _ in faiss_docs]) if faiss_docs else 1.0
            for text, score, metadata in faiss_docs:
                normalized_score = score / max_faiss_score
                text_hash = hashlib.md5(text.encode()).hexdigest()
                combined_results[text_hash] = {
                    'text': text,
                    'metadata': metadata,
                    'faiss_score': normalized_score,
                    'bm25_score': 0.0
                }
            
            # BM25 ê²°ê³¼ ì²˜ë¦¬ (ì ìˆ˜ ì •ê·œí™”)
            max_bm25_score = max([score for _, score, _ in bm25_docs]) if bm25_docs else 1.0
            for text, score, metadata in bm25_docs:
                normalized_score = score / max_bm25_score if max_bm25_score > 0 else 0
                text_hash = hashlib.md5(text.encode()).hexdigest()
                
                if text_hash in combined_results:
                    combined_results[text_hash]['bm25_score'] = normalized_score
                else:
                    combined_results[text_hash] = {
                        'text': text,
                        'metadata': metadata,
                        'faiss_score': 0.0,
                        'bm25_score': normalized_score
                    }
            
            # ìµœì¢… ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
            final_results = []
            for item in combined_results.values():
                final_score = (item['faiss_score'] * 0.6) + (item['bm25_score'] * 0.4)
                final_results.append((item['text'], final_score, item['metadata']))
            
            # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            final_results.sort(key=lambda x: x[1], reverse=True)
            
            logger.info(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ê²°ê³¼")
            return final_results[:k]
            
        except Exception as e:
            logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def calculate_confidence(self, search_results: List[Tuple[str, float, Dict[str, Any]]]) -> float:
        """
        ì»¨í”¼ë˜ìŠ¤ ê³„ì‚° (ë‹¨ìˆœí™”ëœ ë²„ì „)
        
        Args:
            search_results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì»¨í”¼ë˜ìŠ¤ ì ìˆ˜ (0.0 ~ 1.0)
        """
        if not search_results:
            return 0.0
        
        # ìƒìœ„ 3ê°œ ê²°ê³¼ì˜ í‰ê·  ì ìˆ˜ ì‚¬ìš©
        top_scores = [score for _, score, _ in search_results[:3]]
        avg_score = sum(top_scores) / len(top_scores)
        
        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        confidence = min(max(avg_score, 0.0), 1.0)
        
        logger.debug(f"ğŸ“Š ì»¨í”¼ë˜ìŠ¤ ê³„ì‚°: {confidence:.3f} (ìƒìœ„ {len(top_scores)}ê°œ í‰ê· )")
        return confidence
    
    def extract_citations(self, search_results: List[Tuple[str, float, Dict[str, Any]]], max_citations: int = 3) -> List[Citation]:
        """
        Citation ì¶”ì¶œ (200ì snippet)
        
        Args:
            search_results: ê²€ìƒ‰ ê²°ê³¼
            max_citations: ìµœëŒ€ Citation ìˆ˜
            
        Returns:
            Citation ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        citations = []
        
        for i, (text, score, metadata) in enumerate(search_results[:max_citations]):
            # source_id ìƒì„±
            source_file = metadata.get('source_file', 'unknown')
            if 'page_number' in metadata:
                source_id = f"{self.domain}/{source_file}#page_{metadata['page_number']}"
            elif 'row_number' in metadata:
                source_id = f"{self.domain}/{source_file}#row_{metadata['row_number']}"
            else:
                source_id = f"{self.domain}/{source_file}#chunk_{i}"
            
            # snippet ìƒì„± (200ì + ë¬¸ì¥ ë‹¨ìœ„ ì ˆë‹¨)
            snippet = text[:200]
            if len(text) > 200:
                # ë§ˆì§€ë§‰ ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸°
                last_period = snippet.rfind('.')
                last_question = snippet.rfind('?')
                last_exclamation = snippet.rfind('!')
                
                cut_point = max(last_period, last_question, last_exclamation)
                if cut_point > 100:  # ë„ˆë¬´ ì§§ì§€ ì•Šë„ë¡
                    snippet = snippet[:cut_point + 1]
                else:
                    snippet += "..."
            
            citation = Citation(
                source_id=source_id,
                snippet=snippet
            )
            citations.append(citation)
        
        logger.info(f"ğŸ“ Citation ì¶”ì¶œ ì™„ë£Œ: {len(citations)}ê±´")
        return citations
    
    def streaming_response(self, messages: List[Dict[str, str]]) -> Iterator[str]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (50í† í° ë‹¨ìœ„)
        
        Args:
            messages: LLM ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            
        Yields:
            ì‘ë‹µ í† í°ë“¤
        """
        try:
            token_buffer = ""
            token_count = 0
            
            for chunk in self.llm.stream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    token_buffer += chunk.content
                    token_count += 1
                    
                    # 50í† í°ë§ˆë‹¤ ë˜ëŠ” ë¬¸ì¥ ëì—ì„œ flush
                    if token_count >= 50 or chunk.content in '.!?':
                        if token_buffer.strip():
                            yield token_buffer
                            token_buffer = ""
                            token_count = 0
            
            # ë‚¨ì€ ë²„í¼ ì¶œë ¥
            if token_buffer.strip():
                yield token_buffer
                
        except Exception as e:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            yield f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    @abstractmethod
    def get_system_prompt(self) -> str:
        """ë„ë©”ì¸ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜ (ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass
    
    @abstractmethod
    def format_context(self, search_results: List[Tuple[str, float, Dict[str, Any]]]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§· (ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass
    
    def handle(self, request: QueryRequest) -> HandlerResponse:
        """
        í‘œì¤€ í•¸ë“¤ëŸ¬ ì¸í„°í˜ì´ìŠ¤
        
        Args:
            request: QueryRequest ê°ì²´
            
        Returns:
            HandlerResponse ê°ì²´
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ¯ {self.domain} í•¸ë“¤ëŸ¬ ì²˜ë¦¬ ì‹œì‘: {request.text[:50]}...")
            
            # 1. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            search_results = self.hybrid_search(request.text, k=10)
            
            # 2. ì»¨í”¼ë˜ìŠ¤ ê³„ì‚°
            confidence = self.calculate_confidence(search_results)
            
            # 3. ì»¨í”¼ë˜ìŠ¤ ì²´í¬
            if confidence < self.confidence_threshold:
                logger.warning(f"âš ï¸ ë‚®ì€ ì»¨í”¼ë˜ìŠ¤: {confidence:.3f} < {self.confidence_threshold}")
                # TODO: k í™•ì¥ ê²€ìƒ‰ ë˜ëŠ” ì¬ì§ˆë¬¸ ë¡œì§ êµ¬í˜„
            
            # 4. Citation ì¶”ì¶œ
            citations = self.extract_citations(search_results)
            
            # 5. ì‘ë‹µ ìƒì„±
            system_prompt = self.get_system_prompt()
            context = self.format_context(search_results)
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ì»¨í…ìŠ¤íŠ¸:\n{context}\n\nì§ˆë¬¸: {request.text}"}
            ]
            
            # ìŠ¤íŠ¸ë¦¬ë°ì´ ì•„ë‹Œ ì¼ë°˜ ì‘ë‹µìœ¼ë¡œ êµ¬í˜„ (ë‹¨ìˆœí™”)
            response = self.llm.invoke(messages)
            answer = response.content if hasattr(response, 'content') else str(response)
            
            # 6. HandlerResponse ìƒì„±
            elapsed_ms = int((time.time() - start_time) * 1000)
            
            handler_response = HandlerResponse(
                answer=answer,
                citations=citations,
                confidence=confidence,
                handler_id=self.domain,
                elapsed_ms=elapsed_ms
            )
            
            logger.info(f"âœ… {self.domain} í•¸ë“¤ëŸ¬ ì²˜ë¦¬ ì™„ë£Œ ({elapsed_ms}ms, confidence={confidence:.3f})")
            return handler_response
            
        except Exception as e:
            logger.error(f"âŒ {self.domain} í•¸ë“¤ëŸ¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            elapsed_ms = int((time.time() - start_time) * 1000)
            
            # ì—ëŸ¬ ì‘ë‹µ
            return HandlerResponse(
                answer=f"ì£„ì†¡í•©ë‹ˆë‹¤. {self.domain} ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                citations=[],
                confidence=0.0,
                handler_id=self.domain,
                elapsed_ms=elapsed_ms
            )
