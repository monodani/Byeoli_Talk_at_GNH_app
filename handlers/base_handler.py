#!/usr/bin/env python3
"""
ê²½ìƒë‚¨ë„ì¸ì¬ê°œë°œì› RAG ì±—ë´‡ - base_handler ê¸°ë°˜ í´ë˜ìŠ¤ (IndexManager í†µí•© + BM25 ì˜¤ë¥˜ ìˆ˜ì •)

ëª¨ë“  í•¸ë“¤ëŸ¬ì˜ ê³µí†µ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤:
- IndexManager ì‹±ê¸€í†¤ì„ í™œìš©í•œ ì¤‘ì•™ì§‘ì¤‘ì‹ ë²¡í„°ìŠ¤í† ì–´ ê´€ë¦¬
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (FAISS + BM25 + RRF)
- ì»¨í”¼ë˜ìŠ¤ ê³„ì‚° (ë‹¨ìˆœí™”ëœ ë²„ì „)
- Citation ì¶”ì¶œ (200ì snippet)
- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (50í† í° ë‹¨ìœ„)
- í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ (QueryRequest â†’ HandlerResponse)

ì£¼ìš” ìˆ˜ì •ì‚¬í•­:
âœ… BM25 ê²€ìƒ‰ ìŠ¬ë¼ì´ì‹± ì˜¤ë¥˜ ìˆ˜ì •
âœ… Citation snippet ê¸¸ì´ ì œí•œ ê°œì„ 
âœ… í† í°í™” ë¡œì§ ê°œì„ 
âœ… ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
"""

import logging
import time
import hashlib
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Dict, Any, Optional, Iterator, Tuple
from datetime import datetime

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from utils.config import config
from utils.contracts import QueryRequest, HandlerResponse, Citation
from utils.textifier import TextChunk
from utils.index_manager import get_index_manager

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from rank_bm25 import BM25Okapi

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class base_handler(ABC):
    """
    ëª¨ë“  í•¸ë“¤ëŸ¬ì˜ ê¸°ë°˜ í´ë˜ìŠ¤ (IndexManager í†µí•© + ì˜¤ë¥˜ ìˆ˜ì •)
    
    ì£¼ìš” ê¸°ëŠ¥:
    - IndexManagerë¥¼ í†µí•œ ì¤‘ì•™ì§‘ì¤‘ì‹ ë²¡í„°ìŠ¤í† ì–´ ê´€ë¦¬
    - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (FAISS + BM25 + RRF)
    - ì»¨í”¼ë˜ìŠ¤ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸
    - Citation ìë™ ì¶”ì¶œ
    - ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›
    """
    
    def __init__(self, domain: str, index_name: str, confidence_threshold: float):
        """
        base_Handler ì´ˆê¸°í™” (IndexManager í†µí•©)
        
        Args:
            domain: ë„ë©”ì¸ ì´ë¦„ (ì˜ˆ: "satisfaction")
            index_name: ë²¡í„°ìŠ¤í† ì–´ ì¸ë±ìŠ¤ ì´ë¦„
            confidence_threshold: ì»¨í”¼ë˜ìŠ¤ ì„ê³„ê°’ (Î¸)
        """
        self.domain = domain
        self.index_name = index_name
        self.confidence_threshold = confidence_threshold
        
        # IndexManager ì‹±ê¸€í†¤ ì°¸ì¡°
        self.index_manager = get_index_manager()
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embeddings = OpenAIEmbeddings()
        
        # LLM ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            temperature=0.1,
            model="gpt-4o-mini",
            streaming=True
        )
        
        logger.info(f"âœ¨ {domain.upper()} Handler ì´ˆê¸°í™” ì™„ë£Œ (Î¸={confidence_threshold}, IndexManager í†µí•©)")
    
    def _get_vectorstore(self) -> Optional[FAISS]:
        """
        IndexManagerë¥¼ í†µí•´ ë²¡í„°ìŠ¤í† ì–´ íšë“ (ì¤‘ì•™ì§‘ì¤‘ì‹)
        
        Returns:
            Optional[FAISS]: ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None
        """
        try:
            vectorstore = self.index_manager.get_vectorstore(self.domain)
            if vectorstore is None:
                logger.warning(f"âš ï¸ {self.domain} ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
            return vectorstore
        except Exception as e:
            logger.error(f"âŒ {self.domain} ë²¡í„°ìŠ¤í† ì–´ íšë“ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_bm25(self) -> Optional[BM25Okapi]:
        """
        IndexManagerë¥¼ í†µí•´ BM25 ì¸ë±ìŠ¤ íšë“
        
        Returns:
            Optional[BM25Okapi]: BM25 ì¸ë±ìŠ¤ ë˜ëŠ” None
        """
        try:
            bm25 = self.index_manager.get_bm25(self.domain)
            if bm25 is None:
                logger.warning(f"âš ï¸ {self.domain} BM25 ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
            return bm25
        except Exception as e:
            logger.error(f"âŒ {self.domain} BM25 ì¸ë±ìŠ¤ íšë“ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_documents(self) -> List[TextChunk]:
        """
        âœ… ìˆ˜ì •: TextChunk íƒ€ì…ìœ¼ë¡œ ë°˜í™˜
        IndexManagerë¥¼ í†µí•´ ë¬¸ì„œ ëª©ë¡ íšë“
        
        Returns:
            List[TextChunk]: ë¬¸ì„œ ì²­í¬ ëª©ë¡
        """
        try:
            documents = self.index_manager.get_documents(self.domain)
            return documents if documents else []
        except Exception as e:
            logger.error(f"âŒ {self.domain} ë¬¸ì„œ ëª©ë¡ íšë“ ì‹¤íŒ¨: {e}")
            return []
    
    def _tokenize_text(self, text: str) -> List[str]:
        """
        âœ… ì¶”ê°€: ê°œì„ ëœ í† í°í™” í•¨ìˆ˜
        
        Args:
            text: í† í°í™”í•  í…ìŠ¤íŠ¸
            
        Returns:
            List[str]: í† í° ë¦¬ìŠ¤íŠ¸
        """
        # í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ê³ ë ¤í•œ í† í°í™”
        # ê³µë°±, êµ¬ë‘ì  ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
        import re
        tokens = re.findall(r'\b\w+\b', text.lower())
        return tokens
    
    def hybrid_search(self, query: str, k: int = 10) -> List[Tuple[str, float, Dict[str, Any]]]:
        """
        âœ… ìˆ˜ì •: BM25 ê²€ìƒ‰ ì˜¤ë¥˜ ìˆ˜ì •
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (FAISS + BM25 + RRF) - IndexManager í™œìš©
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            List of (text, score, metadata) tuples
        """
        # IndexManagerì—ì„œ ë²¡í„°ìŠ¤í† ì–´ ë° BM25 íšë“
        vectorstore = self._get_vectorstore()
        bm25 = self._get_bm25()
        documents = self._get_documents()  # TextChunk ë¦¬ìŠ¤íŠ¸
        
        if not vectorstore:
            logger.warning(f"âš ï¸ {self.domain} ë²¡í„°ìŠ¤í† ì–´ ì—†ìŒ, ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            return []
        
        try:
            # 1. FAISS ê²€ìƒ‰
            faiss_results = vectorstore.similarity_search_with_score(query, k=k)
            faiss_docs = [(doc.page_content, score, doc.metadata) for doc, score in faiss_results]
            
            # 2. BM25 ê²€ìƒ‰ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            bm25_docs = []
            if bm25 and documents:
                # âœ… ê°œì„ ëœ í† í°í™”
                tokenized_query = self._tokenize_text(query)
                
                # âœ… ìˆ˜ì •: documentsê°€ TextChunk ë¦¬ìŠ¤íŠ¸ì„ì„ ê³ ë ¤
                doc_texts = []
                for doc in documents:
                    if isinstance(doc, TextChunk):
                        doc_texts.append(doc.text)
                    elif isinstance(doc, str):
                        doc_texts.append(doc)
                    else:
                        doc_texts.append(str(doc))
                
                # BM25 ìŠ¤ì½”ì–´ ê³„ì‚°
                if doc_texts:
                    try:
                        bm25_scores = bm25.get_scores(tokenized_query)
                        
                        # ìƒìœ„ kê°œ ì„ íƒ
                        if len(bm25_scores) > 0:
                            # âœ… ìˆ˜ì •: numpy ë°°ì—´ ì¸ë±ì‹± ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                            top_k = min(k, len(bm25_scores))
                            top_indices = np.argsort(bm25_scores)[-top_k:][::-1]
                            
                            for idx in top_indices:
                                if 0 <= idx < len(documents):
                                    doc = documents[idx]
                                    text = doc.text if isinstance(doc, TextChunk) else str(doc)
                                    metadata = doc.metadata if isinstance(doc, TextChunk) else {}
                                    score = float(bm25_scores[idx])
                                    
                                    bm25_docs.append((text, score, metadata))
                    except Exception as e:
                        logger.error(f"âŒ {self.domain} BM25 ìŠ¤ì½”ì–´ ê³„ì‚° ì‹¤íŒ¨: {e}")
            
            # 3. RRF (Reciprocal Rank Fusion) ì ìš©
            combined_results = self._apply_rrf(faiss_docs, bm25_docs, k=k)
            
            logger.debug(f"ğŸ” {self.domain} í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: FAISS {len(faiss_docs)}, BM25 {len(bm25_docs)}, ê²°í•© {len(combined_results)}")
            return combined_results
            
        except Exception as e:
            logger.error(f"âŒ {self.domain} í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            
            # ì—ëŸ¬ ì‹œ FAISS ê²°ê³¼ë§Œì´ë¼ë„ ë°˜í™˜
            try:
                if vectorstore:
                    faiss_results = vectorstore.similarity_search_with_score(query, k=k)
                    return [(doc.page_content, score, doc.metadata) for doc, score in faiss_results]
            except:
                pass
            
            return []
    
    def _apply_rrf(self, faiss_docs: List[Tuple[str, float, Dict]], 
                   bm25_docs: List[Tuple[str, float, Dict]], k: int = 60) -> List[Tuple[str, float, Dict]]:
        """
        RRF (Reciprocal Rank Fusion) ì ìš©
        
        Args:
            faiss_docs: FAISS ê²€ìƒ‰ ê²°ê³¼
            bm25_docs: BM25 ê²€ìƒ‰ ê²°ê³¼
            k: RRF íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’: 60)
            
        Returns:
            List: RRF ì ìˆ˜ë¡œ ì •ë ¬ëœ ê²°í•© ê²°ê³¼
        """
        doc_scores = {}
        
        # FAISS ì ìˆ˜ (ê±°ë¦¬ ê¸°ë°˜ì´ë¯€ë¡œ ì—­ìˆœìœ¼ë¡œ ë­í‚¹)
        for rank, (text, score, metadata) in enumerate(faiss_docs):
            doc_key = text[:100]  # í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ì„ í‚¤ë¡œ ì‚¬ìš©
            rrf_score = 1.0 / (k + rank + 1)
            doc_scores[doc_key] = {
                'text': text,
                'metadata': metadata,
                'rrf_score': rrf_score,
                'faiss_score': score
            }
        
        # BM25 ì ìˆ˜ ì¶”ê°€
        for rank, (text, score, metadata) in enumerate(bm25_docs):
            doc_key = text[:100]
            rrf_score = 1.0 / (k + rank + 1)
            
            if doc_key in doc_scores:
                # ê¸°ì¡´ ë¬¸ì„œë©´ RRF ì ìˆ˜ í•©ì‚°
                doc_scores[doc_key]['rrf_score'] += rrf_score
                doc_scores[doc_key]['bm25_score'] = score
            else:
                # ìƒˆ ë¬¸ì„œë©´ ì¶”ê°€
                doc_scores[doc_key] = {
                    'text': text,
                    'metadata': metadata,
                    'rrf_score': rrf_score,
                    'bm25_score': score
                }
        
        # RRF ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ í›„ ìƒìœ„ ê²°ê³¼ ë°˜í™˜
        sorted_docs = sorted(doc_scores.values(), key=lambda x: x['rrf_score'], reverse=True)
        
        result = []
        for doc in sorted_docs[:len(faiss_docs)]:  # ì›ë˜ ìš”ì²­í•œ kê°œë§Œí¼ ë°˜í™˜
            result.append((doc['text'], doc['rrf_score'], doc['metadata']))
        
        return result
    
    def calculate_confidence(self, search_results: List[Tuple[str, float, Dict[str, Any]]], 
                           query: str) -> float:
        """
        âœ… ê°œì„ : ë¬¸ì„œê°€ ì ì„ ë•Œë„ ì ì ˆí•œ ì»¨í”¼ë˜ìŠ¤ ê³„ì‚°
        
        Args:
            search_results: ê²€ìƒ‰ ê²°ê³¼
            query: ì›ë³¸ ì¿¼ë¦¬
            
        Returns:
            float: ì»¨í”¼ë˜ìŠ¤ ì ìˆ˜ (0.0 - 1.0)
        """
        if not search_results:
            return 0.0
        
        # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì— ë”°ë¥¸ ê¸°ë³¸ ì ìˆ˜ ì¡°ì •
        result_count = len(search_results)
        if result_count == 1:
            base_confidence = 0.3  # ê²°ê³¼ê°€ 1ê°œë©´ ê¸°ë³¸ 0.3
        elif result_count == 2:
            base_confidence = 0.4  # ê²°ê³¼ê°€ 2ê°œë©´ ê¸°ë³¸ 0.4
        else:
            base_confidence = 0.5  # 3ê°œ ì´ìƒì´ë©´ ê¸°ë³¸ 0.5
        
        # ìƒìœ„ ê²°ê³¼ì˜ í‰ê·  ì ìˆ˜ (ì •ê·œí™”)
        top_scores = [score for _, score, _ in search_results[:min(3, result_count)]]
        if top_scores:
            # ì ìˆ˜ ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ)
            max_score = max(top_scores)
            if max_score > 0:
                normalized_scores = [s / max_score for s in top_scores]
                avg_score = sum(normalized_scores) / len(normalized_scores)
                score_boost = avg_score * 0.3  # ìµœëŒ€ 0.3 ì¶”ê°€
            else:
                score_boost = 0.0
        else:
            score_boost = 0.0
        
        # ë¬¸ì„œ ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
        sources = set()
        for _, _, metadata in search_results[:min(3, result_count)]:
            source = metadata.get('source', 'unknown')
            sources.add(source)
        diversity_bonus = min(len(sources) * 0.05, 0.15)  # ìµœëŒ€ 0.15 ë³´ë„ˆìŠ¤
        
        # ì¿¼ë¦¬ ìœ í˜• ë§¤ì¹­ ë³´ë„ˆìŠ¤
        query_lower = query.lower()
        domain_keywords = self._get_domain_keywords()
        keyword_matches = sum(1 for kw in domain_keywords if kw in query_lower)
        keyword_bonus = min(keyword_matches * 0.05, 0.15)  # ìµœëŒ€ 0.15 ë³´ë„ˆìŠ¤
        
        # ìµœì¢… ì»¨í”¼ë˜ìŠ¤ ê³„ì‚°
        confidence = min(base_confidence + score_boost + diversity_bonus + keyword_bonus, 1.0)
        
        logger.debug(f"ğŸ¯ {self.domain} ì»¨í”¼ë˜ìŠ¤: {confidence:.3f} (ê¸°ë³¸: {base_confidence:.3f}, ì ìˆ˜: {score_boost:.3f}, ë‹¤ì–‘ì„±: {diversity_bonus:.3f}, í‚¤ì›Œë“œ: {keyword_bonus:.3f})")
        return confidence
    
    def _get_domain_keywords(self) -> List[str]:
        """ë„ë©”ì¸ë³„ í‚¤ì›Œë“œ ë°˜í™˜ (ì»¨í”¼ë˜ìŠ¤ ê³„ì‚°ìš©)"""
        domain_keywords = {
            'satisfaction': ['ë§Œì¡±ë„', 'í‰ê°€', 'ì„¤ë¬¸', 'ì¡°ì‚¬', 'ì ìˆ˜', 'ìˆœìœ„'],
            'general': ['í•™ì¹™', 'ê·œì •', 'ì „ê²°', 'ì—°ë½ì²˜', 'ë‹´ë‹¹ì', 'ë¶€ì„œ'],
            'menu': ['ì‹ë‹¨', 'ë©”ë‰´', 'êµ¬ë‚´ì‹ë‹¹', 'ê¸‰ì‹', 'ì‹ì‚¬'],
            'cyber': ['ì‚¬ì´ë²„êµìœ¡', 'ì˜¨ë¼ì¸êµìœ¡', 'ë‚˜ë¼ë°°ì›€í„°', 'ë¯¼ê°„ìœ„íƒ'],
            'publish': ['êµìœ¡ê³„íš', 'í›ˆë ¨ê³„íš', 'í‰ê°€ì„œ', 'ê³„íšì„œ'],
            'notice': ['ê³µì§€', 'ì•ˆë‚´', 'ì•Œë¦¼', 'ê³µì§€ì‚¬í•­', 'ìƒˆì†Œì‹']
        }
        return domain_keywords.get(self.domain, [])
    
    def extract_citations(self, search_results: List[Tuple[str, float, Dict[str, Any]]], 
                         max_citations: int = 3) -> List[Citation]:
        """
        âœ… ê°œì„ : Citation snippet ê¸¸ì´ ì œí•œ ì²˜ë¦¬
        
        Args:
            search_results: ê²€ìƒ‰ ê²°ê³¼
            max_citations: ìµœëŒ€ Citation ìˆ˜
            
        Returns:
            List[Citation]: Citation ëª©ë¡
        """
        citations = []
        
        for i, (text, score, metadata) in enumerate(search_results[:max_citations]):
            try:
                # ì†ŒìŠ¤ ID ìƒì„±
                source = metadata.get('source', 'unknown')
                page = metadata.get('page', '')
                source_id = f"{source}#{page}" if page else source
                
                # âœ… ê°œì„ ëœ ìŠ¤ë‹ˆí« ìƒì„± (200ì ì œí•œ, ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìë¥´ê¸°)
                if len(text) <= 200:
                    snippet = text
                else:
                    # 200ì ê·¼ì²˜ì—ì„œ ë‹¨ì–´ ê²½ê³„ ì°¾ê¸°
                    cutoff = 197  # "..." 3ì ê³ ë ¤
                    # ê³µë°±ì´ë‚˜ êµ¬ë‘ì ì—ì„œ ìë¥´ê¸°
                    while cutoff > 0 and cutoff < len(text):
                        if text[cutoff] in ' \n\t.,;!?':
                            break
                        cutoff -= 1
                    
                    if cutoff == 0:
                        snippet = text[:197] + "..."
                    else:
                        snippet = text[:cutoff].strip() + "..."
                
                citation = Citation(
                    source_id=source_id,
                    snippet=snippet
                )
                citations.append(citation)
                
            except Exception as e:
                logger.error(f"Citation ìƒì„± ì‹¤íŒ¨: {e}")
                # Citation ì—†ì´ ê³„ì† ì§„í–‰
                continue
        
        # Citationì´ ì—†ìœ¼ë©´ ê²½ê³ 
        if not citations:
            logger.warning("Citationì´ ì—†ìŠµë‹ˆë‹¤. ì†ŒìŠ¤ ì¸ìš©ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        return citations
    
    def handle(self, request: QueryRequest) -> HandlerResponse:
        """
        ë©”ì¸ í•¸ë“¤ë§ í•¨ìˆ˜ (IndexManager í†µí•©)
        
        Args:
            request: ì‚¬ìš©ì ìš”ì²­
            
        Returns:
            HandlerResponse: ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = time.time()
        
        try:
            # 1. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
            search_results = self.hybrid_search(request.text, k=5)
            
            if not search_results:
                logger.warning(f"âš ï¸ {self.domain} ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                return self._create_no_results_response(request, start_time)
            
            # 2. ì»¨í”¼ë˜ìŠ¤ ê³„ì‚°
            confidence = self.calculate_confidence(search_results, request.text)
            
            # 3. ì„ê³„ê°’ í™•ì¸ ë° ì¬ì§ˆë¬¸ ì²˜ë¦¬
            reask = None
            if confidence < self.confidence_threshold:
                logger.info(f"ğŸ“‰ {self.domain} ì»¨í”¼ë˜ìŠ¤ ë¶€ì¡±: {confidence:.3f} < {self.confidence_threshold}")
                
                # k í™•ì¥ ì¬ê²€ìƒ‰ ì‹œë„
                extended_results = self.hybrid_search(request.text, k=12)
                if extended_results:
                    extended_confidence = self.calculate_confidence(extended_results, request.text)
                    if extended_confidence >= self.confidence_threshold:
                        search_results = extended_results
                        confidence = extended_confidence
                        logger.info(f"ğŸ“ˆ {self.domain} í™•ì¥ ê²€ìƒ‰ìœ¼ë¡œ ì»¨í”¼ë˜ìŠ¤ íšŒë³µ: {confidence:.3f}")
                    else:
                        # ì—¬ì „íˆ ë‚®ìœ¼ë©´ ì¬ì§ˆë¬¸ ìƒì„±
                        reask = f"ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì‹œë©´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ê³¼ì •ëª…ì´ë‚˜ ê¸°ê°„ì„ ì•Œë ¤ì£¼ì„¸ìš”."
                        logger.warning(f"âš ï¸ ë‚®ì€ ì»¨í”¼ë˜ìŠ¤ë¡œ ì¬ì§ˆë¬¸ ìœ ë„ (confidence={confidence:.3f})")
            
            # 4. ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
            context = self.format_context(search_results)
            
            # 5. LLM ì‘ë‹µ ìƒì„±
            system_prompt = self.get_system_prompt()
            user_prompt = f"""ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {request.text}

ìœ„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""
            
            response = self.llm.invoke([
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ])
            
            answer = response.content if hasattr(response, 'content') else str(response)
            
            # 6. Citation ì¶”ì¶œ
            citations = self.extract_citations(search_results)
            
            # 7. ì‘ë‹µ ìƒì„±
            elapsed_ms = int((time.time() - start_time) * 1000)
            
            logger.info(f"âœ… {self.domain} í•¸ë“¤ëŸ¬ ì™„ë£Œ: confidence={confidence:.3f}")
            
            return HandlerResponse(
                answer=answer,
                citations=citations,
                confidence=confidence,
                handler_id=self.domain,
                elapsed_ms=elapsed_ms,
                reask=reask,
                diagnostics={
                    'search_results_count': len(search_results),
                    'threshold_met': confidence >= self.confidence_threshold,
                    'indexmanager_integration': True
                }
            )
            
        except Exception as e:
            logger.error(f"âŒ {self.domain} í•¸ë“¤ë§ ì‹¤íŒ¨: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            
            elapsed_ms = int((time.time() - start_time) * 1000)
            
            return HandlerResponse(
                answer=f"ì£„ì†¡í•©ë‹ˆë‹¤. {self.domain} ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                citations=[],
                confidence=0.0,
                handler_id=self.domain,
                elapsed_ms=elapsed_ms,
                diagnostics={'error': str(e)}
            )
    
    def _create_no_results_response(self, request: QueryRequest, start_time: float) -> HandlerResponse:
        """ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ì‘ë‹µ ìƒì„±"""
        elapsed_ms = int((time.time() - start_time) * 1000)
        
        # Citationì´ ì—†ì„ ë•Œ ê¸°ë³¸ Citation ìƒì„±
        empty_citation = Citation(
            source_id="no_results",
            snippet="ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        )
        
        return HandlerResponse(
            answer=f"ì£„ì†¡í•©ë‹ˆë‹¤. '{request.text}' ê´€ë ¨ {self.domain} ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ ë³´ì‹œê±°ë‚˜, ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.",
            citations=[empty_citation],  # ë¹ˆ Citation ëŒ€ì‹  ê¸°ë³¸ Citation ì œê³µ
            confidence=0.0,
            handler_id=self.domain,
            elapsed_ms=elapsed_ms,
            diagnostics={'no_search_results': True}
        )
    
    @abstractmethod
    def get_system_prompt(self) -> str:
        """ë„ë©”ì¸ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜ (ì¶”ìƒ ë©”ì„œë“œ)"""
        pass
    
    @abstractmethod
    def format_context(self, search_results: List[Tuple[str, float, Dict[str, Any]]]) -> str:
        """ë„ë©”ì¸ë³„ ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… (ì¶”ìƒ ë©”ì„œë“œ)"""
        pass


# ================================================================
# ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì› (ë¯¸ë˜ í™•ì¥ìš©)
# ================================================================

class StreamingHandlerMixin:
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•œ ë¯¹ìŠ¤ì¸ í´ë˜ìŠ¤"""
    
    def stream_response(self, prompt: str, max_tokens: int = 1000) -> Iterator[str]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        
        Args:
            prompt: LLM í”„ë¡¬í”„íŠ¸
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            
        Yields:
            str: í† í° ë‹¨ìœ„ ì‘ë‹µ
        """
        try:
            stream = self.llm.stream(prompt, max_tokens=max_tokens)
            for chunk in stream:
                if hasattr(chunk, 'content') and chunk.content:
                    yield chunk.content
        except Exception as e:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹¤íŒ¨: {e}")
            yield f"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


# ================================================================
# ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë¡œê·¸
# ================================================================

logger.info("âœ… base_handler.py ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - BM25 ì˜¤ë¥˜ ìˆ˜ì • ë²„ì „")